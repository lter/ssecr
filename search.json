[
  {
    "objectID": "mod_wrangle.html",
    "href": "mod_wrangle.html",
    "title": "Data Harmonization & Wrangling",
    "section": "",
    "text": "Now that we have covered how to find data and use data visualization methods to explore it, we can move on to combining separate data files and preparing that combined data file for analysis. For the purposes of this module we’re adopting a very narrow view of harmonization and a very broad view of wrangling but this distinction aligns well with two discrete philosophical/practical arenas. To make those definitions explicit:\n\n“Harmonization” = process of combining separate primary data objects into one object. This includes things like synonymizing columns, or changing data format to support combination. This excludes quality control steps–even those that are undertaken before harmonization begins.\n“Wrangling” = all modifications to data meant to create an analysis-ready ‘tidy’ data object. This includes quality control, unit conversions, and data ‘shape’ changes to name a few. Note that attaching ancillary data to your primary data object (e.g., attaching temperature data to a dataset on plant species composition) also falls into this category!",
    "crumbs": [
      "Phase III -- Execute",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "mod_wrangle.html#overview",
    "href": "mod_wrangle.html#overview",
    "title": "Data Harmonization & Wrangling",
    "section": "",
    "text": "Now that we have covered how to find data and use data visualization methods to explore it, we can move on to combining separate data files and preparing that combined data file for analysis. For the purposes of this module we’re adopting a very narrow view of harmonization and a very broad view of wrangling but this distinction aligns well with two discrete philosophical/practical arenas. To make those definitions explicit:\n\n“Harmonization” = process of combining separate primary data objects into one object. This includes things like synonymizing columns, or changing data format to support combination. This excludes quality control steps–even those that are undertaken before harmonization begins.\n“Wrangling” = all modifications to data meant to create an analysis-ready ‘tidy’ data object. This includes quality control, unit conversions, and data ‘shape’ changes to name a few. Note that attaching ancillary data to your primary data object (e.g., attaching temperature data to a dataset on plant species composition) also falls into this category!",
    "crumbs": [
      "Phase III -- Execute",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "mod_wrangle.html#learning-objectives",
    "href": "mod_wrangle.html#learning-objectives",
    "title": "Data Harmonization & Wrangling",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nIdentify typical steps in data harmonization and wrangling workflows\nCreate a harmonization workflow\nDefine quality control\nSummarize typical operations in a quality control workflow\nUse regular expressions to perform flexible text operations\nWrite custom functions to reduce code duplication\nIdentify value of and typical obstacles to data ‘joining’\nExplain benefits and drawbacks of using data shape to streamline code\nDesign a complete data wrangling workflow",
    "crumbs": [
      "Phase III -- Execute",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "mod_wrangle.html#needed-packages",
    "href": "mod_wrangle.html#needed-packages",
    "title": "Data Harmonization & Wrangling",
    "section": "Needed Packages",
    "text": "Needed Packages\nIf you’d like to follow along with the code chunks included throughout this module, you’ll need to install the following packages:\n\n# Note that these lines only need to be run once per computer\n## So you can skip this step if you've installed these before\ninstall.packages(\"ltertools\")\ninstall.packages(\"lterdatasampler\")\ninstall.packages(\"psych\")\ninstall.packages(\"supportR\")\ninstall.packages(\"tidyverse\")\n\nWe’ll load the Tidyverse meta-package here to have access to many of its useful tools when we need them later.\n\n# Load tidyverse\nlibrary(tidyverse)",
    "crumbs": [
      "Phase III -- Execute",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "mod_wrangle.html#harmonizing-data",
    "href": "mod_wrangle.html#harmonizing-data",
    "title": "Data Harmonization & Wrangling",
    "section": "Harmonizing Data",
    "text": "Harmonizing Data\nData harmonization is an interesting topic in that it is vital for synthesis projects but only very rarely relevant for primary research. Synthesis projects must reckon with the data choices made by each team of original data collectors. These collectors may or may not have recorded their judgement calls (or indeed, any metadata) but before synthesis work can be meaningfully done these independent datasets must be made comparable to one another and combined.\nFor tabular data, we recommend using the ltertools R package to perform any needed harmonization. This package relies on a “column key” to translate the original column names into equivalents that apply across all datasets. Users can generate this column key however they would like but Google Sheets is a strong option as it allows multiple synthesis team members to simultaneously work on filling in the needed bits of the key. If you already have a set of files locally, ltertools does offer a begin_key function that creates the first two required columns in the column key.\nThe column key requires three columns:\n\n“source” – Name of the raw file\n“raw_name” – Name of all raw columns in that file to be synonymized\n“tidy_name” – New name for each raw column that should be carried to the harmonized data\n\nNote that any raw names either not included in the column key or that lack a tidy name equivalent will be excluded from the final data object. For more information, consult the ltertools package vignette. For convenience, we’re attaching the visual diagram of this method of harmonization from the package vignette.",
    "crumbs": [
      "Phase III -- Execute",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "mod_wrangle.html#wrangling-data",
    "href": "mod_wrangle.html#wrangling-data",
    "title": "Data Harmonization & Wrangling",
    "section": "Wrangling Data",
    "text": "Wrangling Data\nData wrangling is a huge subject that covers a wide range of topics. In this part of the module, we’ll attempt to touch on a wide range of tools that may prove valuable to your data wrangling efforts. This is certainly non-exhaustive and you’ll likely find new tools that fit your coding style and professional intuition better. However, hopefully the topics covered below provide a nice ‘jumping off’ point to reproducibly prepare your data for analysis and visualization work later in the lifecycle of the project.\nTo begin, we’ll load the Plum Island Ecosystems fiddler crab dataset we’ve used in other modules.\n\n# Load the lterdatasampler package\nlibrary(lterdatasampler)\n\n# Load the fiddler crab dataset\ndata(pie_crab)\n\n\nExploring the Data\nBefore beginning any code operations, it’s important to get a sense for the data. Characteristics like the dimensions of the dataset, the column names, and the type of information stored in each column are all crucial pre-requisites to knowing what tools can or should be used on the data.\nChecking the data structure is one way of getting a lot of this high-level information.\n\n# Check dataset structure\nstr(pie_crab)\n\ntibble [392 × 9] (S3: tbl_df/tbl/data.frame)\n $ date         : Date[1:392], format: \"2016-07-24\" \"2016-07-24\" ...\n $ latitude     : num [1:392] 30 30 30 30 30 30 30 30 30 30 ...\n $ site         : chr [1:392] \"GTM\" \"GTM\" \"GTM\" \"GTM\" ...\n $ size         : num [1:392] 12.4 14.2 14.5 12.9 12.4 ...\n $ air_temp     : num [1:392] 21.8 21.8 21.8 21.8 21.8 ...\n $ air_temp_sd  : num [1:392] 6.39 6.39 6.39 6.39 6.39 ...\n $ water_temp   : num [1:392] 24.5 24.5 24.5 24.5 24.5 ...\n $ water_temp_sd: num [1:392] 6.12 6.12 6.12 6.12 6.12 ...\n $ name         : chr [1:392] \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" ...\n\n\nFor data that are primarily numeric, you may find data summary functions to be valuable. Note that most functions of this type do not provide useful information on text columns so you’ll need to find that information elsewhere.\n\n# Get a simple summary of the data\nsummary(pie_crab)\n\n      date               latitude         site                size      \n Min.   :2016-07-24   Min.   :30.00   Length:392         Min.   : 6.64  \n 1st Qu.:2016-07-28   1st Qu.:34.00   Class :character   1st Qu.:12.02  \n Median :2016-08-01   Median :39.10   Mode  :character   Median :14.44  \n Mean   :2016-08-02   Mean   :37.69                      Mean   :14.66  \n 3rd Qu.:2016-08-09   3rd Qu.:41.60                      3rd Qu.:17.34  \n Max.   :2016-08-13   Max.   :42.70                      Max.   :23.43  \n    air_temp      air_temp_sd      water_temp    water_temp_sd  \n Min.   :10.29   Min.   :6.391   Min.   :13.98   Min.   :4.838  \n 1st Qu.:12.05   1st Qu.:8.110   1st Qu.:14.33   1st Qu.:6.567  \n Median :13.93   Median :8.410   Median :17.50   Median :6.998  \n Mean   :15.20   Mean   :8.654   Mean   :17.65   Mean   :7.252  \n 3rd Qu.:18.63   3rd Qu.:9.483   3rd Qu.:20.54   3rd Qu.:7.865  \n Max.   :21.79   Max.   :9.965   Max.   :24.50   Max.   :9.121  \n     name          \n Length:392        \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n\nFor text columns it can sometimes be useful to simply look at the unique entries in a given column and sort them alphabetically for ease of parsing.\n\n# Look at the sites included in the data\nsort(unique(pie_crab$site))\n\n [1] \"BC\"  \"CC\"  \"CT\"  \"DB\"  \"GTM\" \"JC\"  \"NB\"  \"NIB\" \"PIE\" \"RC\"  \"SI\"  \"VCR\"\n[13] \"ZI\" \n\n\nFor those of you who think more visually, a histogram can be a nice way of examining numeric data. There are simple histogram functions in the ‘base’ packages of most programming languages but it can sometimes be worth it to use those from special libraries because they can often convey additional detail.\n\n# Load the psych library\nlibrary(psych)\n\n# Get the histogram of crab \"size\" (carapace width in mm)\npsych::multi.hist(pie_crab$size)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussion: Data Exploration Tools\n\n\n\nWith a group of 4-5 others discuss the following questions:\n\nWhat tools do you use when exploring new data?\nDo you already use any of these tools to explore your data?\n\nIf you do, why do you use them?\nIf not, where do you think they might be valuable to include?\n\nWhat value–if any–do you see in including these exploratory efforts in your code workflow?\n\n\n\n\n\nQuality Control\nYou may have encountered the phrase “QA/QC” (Quality Assurance / Quality Control) in relation to data cleaning. Technically, quality assurance only encapsulates preventative measures for reducing errors. One example of QA would be using a template for field datasheets because using standard fields reduces the risk that data are recorded inconsistently and/or incompletely. Quality control on the other hand refers to all steps taken to resolve errors after data are collected. Any code that you write to fix typos or remove outliers from a dataset falls under the umbrella of QC.\nIn synthesis work, QA is only very rarely an option. You’ll be working with datasets that have already been collected and attempting to handle any issues post hoc which means the vast majority of data wrangling operations will be quality control methods. These QC efforts can be incredibly time-consuming so using a programming language (like  R or  Python) is a dramatic improvement over manually looking through the data using Microsoft Excel or other programs like it.\n\nNumber Checking\nWhen you read in a dataset and a column that should be numeric is instead read in as a character, it can be a sign that there are malformed numbers lurking in the background. Checking for and resolving these non-numbers is preferable to simply coercing the column into being numeric because the latter method typically changes those values to ‘NA’ where a human might be able to deduce the true number each value ‘should be.’\n\n# Load the supportR package\nlibrary(supportR)\n\n# Create an example dataset with non-numbers in ideally numeric columns\nfish_ct &lt;- data.frame(\"species\" = c(\"salmon\", \"bass\", \"halibut\", \"moray eel\"),\n                      \"count\" = c(12, \"14x\", \"_23\", 1))\n\n# Check for malformed numbers in column(s) that should be numeric\nbad_nums &lt;- supportR::num_check(data = fish_ct, col = \"count\")\n\nFor 'count', 2 non-numbers identified: '14x' | '_23'\n\n\nIn the above example, “14x” would be coerced to NA if you simply force the column without checking but you could drop the “x” with text replacing methods once you use tools like this one to flag it for your attention.\n\n\nText Replacement\nOne of the simpler ways of handling text issues is just to replace a string with another string. Most programming languages support this functionality.\n\n# Use pattern match/replace to simplify problem entries\nfish_ct$count &lt;- gsub(pattern = \"x|_\", replacement = \"\", x = fish_ct$count)\n\n# Check that they are fixed\nbad_nums &lt;- supportR::num_check(data = fish_ct, col = \"count\")\n\nFor 'count', no non-numeric values identified.\n\n\nThe vertical line in the gsub example above lets us search for (and replace) multiple patterns. Note however that while you can search for many patterns at once, only a single replacement value can be provided with this function.\n\n\nRegular Expressions\nYou may sometimes want to perform more generic string matching where you don’t necessarily know–or want to list–all possible strings to find and replace. For instance, you may want remove any letter in a numeric column or find and replace numbers with some sort of text note. “Regular expressions” are how programmers specify these generic matches and using them can be a nice way of streamlining code.\n\n# Make a test vector\nregex_vec &lt;- c(\"hello\", \"123\", \"goodbye\", \"456\")\n\n# Find all numbers and replace with the letter X\ngsub(pattern = \"[[:digit:]]\", replacement = \"x\", x = regex_vec)\n\n[1] \"hello\"   \"xxx\"     \"goodbye\" \"xxx\"    \n\n# Replace any number of letters with only a single 0\ngsub(pattern = \"[[:alpha:]]+\", replacement = \"0\", x = regex_vec)\n\n[1] \"0\"   \"123\" \"0\"   \"456\"\n\n\nThe stringr package cheatsheet has a really nice list of regular expression options that you may find valuable if you want to delve deeper on this topic. Scroll to the second page of the PDF to see the most relevant parts.\n\n\n\nConditionals\nRather than finding and replacing content, you may want to create a new column based on the contents of a different column. In plain language you might phrase this as ‘if column X has [some values] then column Y should have [other values]’. These operations are called conditionals and are an important part of data wrangling.\nIf you only want your conditional to support two outcomes (as in an either/or statement) there are useful functions that support this. Let’s return to our Plum Island Ecosystems crab dataset for an example.\n\n# Make a new colum with an either/or conditional\npie_crab_v2 &lt;- pie_crab %&gt;% \n1  dplyr::mutate(size_category = ifelse(test = (size &gt;= 15),\n                                       yes = \"big\",\n                                       no = \"small\"),\n                .after = size) \n\n# Count the number of crabs in each category\npie_crab_v2 %&gt;% \n  dplyr::group_by(size_category) %&gt;% \n  dplyr::summarize(crab_ct = dplyr::n())\n\n\n1\n\nmutate makes a new column, ifelse is actually doing the conditional\n\n\n\n\n# A tibble: 2 × 2\n  size_category crab_ct\n  &lt;chr&gt;           &lt;int&gt;\n1 big               179\n2 small             213\n\n\nIf you have multiple different conditions you can just stack these either/or conditionals together but this gets cumbersome quickly. It is preferable to instead use a function that supports as many alternates as you want!\n\n# Make a new column with several conditionals\npie_crab_v2 &lt;- pie_crab %&gt;% \n  dplyr::mutate(size_category = dplyr::case_when( \n1    size &lt;= 10 ~ \"tiny\",\n    size &gt; 10 & size &lt;= 15 ~ \"small\",\n    size &gt; 15 & size &lt;= 20 ~ \"big\",\n    size &gt; 20 ~ \"huge\",\n2    TRUE ~ \"uncategorized\"),\n                .after = size)\n\n# Count the number of crabs in each category\npie_crab_v2 %&gt;% \n  dplyr::group_by(size_category) %&gt;% \n  dplyr::summarize(crab_ct = dplyr::n())\n\n\n1\n\nSyntax is ‘test ~ what to do when true’\n\n2\n\nThis line is a catch-all for any rows that don’t meet previous conditions\n\n\n\n\n# A tibble: 4 × 2\n  size_category crab_ct\n  &lt;chr&gt;           &lt;int&gt;\n1 big               150\n2 huge               28\n3 small             178\n4 tiny               36\n\n\nNote that you can use functions like this one when you do have an either/or conditional if you prefer this format.\n\n\n\n\n\n\nActivity: Conditionals\n\n\n\nIn a script, attempt the following with the PIE crab data:\n\nCreate a column indicating when air temperature is above or below 13° Fahrenheit\nCreate a column indicating whether water temperature is lower than the first quartile, between the first quartile and the median water temp, between the median and the third quartile or greater than the third quartile\n\nHint: consult the summary function output!\n\n\n\n\n\n\nUniting / Separating Columns\nSometimes one column has multiple pieces of information that you’d like to consider separately. A date column is a common example of this because particular months are always in a given season regardless of the specific day or year. So, it can be useful to break a complete date (i.e., year/month/day) into its component bits to be better able to access those pieces of information.\n\n# Split date into each piece of temporal info\npie_crab_v3 &lt;- pie_crab_v2 %&gt;% \n  tidyr::separate_wider_delim(cols = date, \n1                              delim = \"-\",\n                              names = c(\"year\", \"month\", \"day\"),\n2                              cols_remove = TRUE)\n\n# Check that out\nstr(pie_crab_v3)\n\n\n1\n\n‘delim’ is short for “delimiter” which we covered in the Reproducibility module\n\n2\n\nThis argument specifies whether to remove the original column when making the new columns\n\n\n\n\ntibble [392 × 12] (S3: tbl_df/tbl/data.frame)\n $ year         : chr [1:392] \"2016\" \"2016\" \"2016\" \"2016\" ...\n $ month        : chr [1:392] \"07\" \"07\" \"07\" \"07\" ...\n $ day          : chr [1:392] \"24\" \"24\" \"24\" \"24\" ...\n $ latitude     : num [1:392] 30 30 30 30 30 30 30 30 30 30 ...\n $ site         : chr [1:392] \"GTM\" \"GTM\" \"GTM\" \"GTM\" ...\n $ size         : num [1:392] 12.4 14.2 14.5 12.9 12.4 ...\n $ size_category: chr [1:392] \"small\" \"small\" \"small\" \"small\" ...\n $ air_temp     : num [1:392] 21.8 21.8 21.8 21.8 21.8 ...\n $ air_temp_sd  : num [1:392] 6.39 6.39 6.39 6.39 6.39 ...\n $ water_temp   : num [1:392] 24.5 24.5 24.5 24.5 24.5 ...\n $ water_temp_sd: num [1:392] 6.12 6.12 6.12 6.12 6.12 ...\n $ name         : chr [1:392] \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" ...\n\n\nWhile breaking apart a column’s contents can be useful, it can also be helpful to combine the contents of several columns!\n\n# Re-combine data information back into date\npie_crab_v4 &lt;- pie_crab_v3 %&gt;% \n  tidyr::unite(col = \"date\",\n1               sep = \"/\",\n               year:day, \n2               remove = FALSE)\n\n# Structure check\nstr(pie_crab_v4)\n\n\n1\n\nThis is equivalent to the ‘delim’ argument in the previous function\n\n2\n\nComparable to the ‘cols_remove’ argument in the previous function\n\n\n\n\ntibble [392 × 13] (S3: tbl_df/tbl/data.frame)\n $ date         : chr [1:392] \"2016/07/24\" \"2016/07/24\" \"2016/07/24\" \"2016/07/24\" ...\n $ year         : chr [1:392] \"2016\" \"2016\" \"2016\" \"2016\" ...\n $ month        : chr [1:392] \"07\" \"07\" \"07\" \"07\" ...\n $ day          : chr [1:392] \"24\" \"24\" \"24\" \"24\" ...\n $ latitude     : num [1:392] 30 30 30 30 30 30 30 30 30 30 ...\n $ site         : chr [1:392] \"GTM\" \"GTM\" \"GTM\" \"GTM\" ...\n $ size         : num [1:392] 12.4 14.2 14.5 12.9 12.4 ...\n $ size_category: chr [1:392] \"small\" \"small\" \"small\" \"small\" ...\n $ air_temp     : num [1:392] 21.8 21.8 21.8 21.8 21.8 ...\n $ air_temp_sd  : num [1:392] 6.39 6.39 6.39 6.39 6.39 ...\n $ water_temp   : num [1:392] 24.5 24.5 24.5 24.5 24.5 ...\n $ water_temp_sd: num [1:392] 6.12 6.12 6.12 6.12 6.12 ...\n $ name         : chr [1:392] \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" ...\n\n\nNote in this output how despite re-combining data information the column is listed as a character column! Simply combining or separating data is not always enough so you need to really lean into frequent data structure checks to be sure that your data are structured in the way that you want.\n\n\nJoining Data\nOften the early steps of a synthesis project involve combining the data tables horizontally. You might imagine that you have two groups’ data on sea star abundance and–once you’ve synonymized the column names–you can simply ‘stack’ the tables on top of one another. Slightly trickier but no less common is combining tables by the contents of a shared column (or columns). Cases like this include wanting to combine your sea star table with ocean temperature data from the region of each group’s research. You can’t simply attach the columns because that assumes that the row order is identical between the two data tables (and indeed, that there are the same number of rows in both to begin with!). In this case, if both data tables shared some columns (perhaps “site” and coordinate columns) you can use joins to let your computer match these key columns and make sure that only appropriate rows are combined.\nBecause joins are completely dependent upon the value in both columns being an exact match, it is a good idea to carefully check the contents of those columns before attempting a join to make sure that the join will be successful.\n\n# Create a fish taxonomy dataframe that corresponds with the earlier fish dataframe\nfish_tax &lt;- data.frame(\"species\" = c(\"salmon\", \"bass\", \"halibut\", \"eel\"),\n                       \"family\" = c(\"Salmonidae\", \"Serranidae\", \"Pleuronectidae\", \"Muraenidae\"))\n\n# Check to make sure that the 'species' column matches between both tables\nsupportR::diff_check(old = fish_ct$species, new = fish_tax$species) \n\nFollowing element(s) found in old object but not new: \n\n\n[1] \"moray eel\"\n\n\nFollowing element(s) found in new object but not old: \n\n\n[1] \"eel\"\n\n\n\n# Use text replacement methods to fix that mistake in one table\nfish_tax_v2 &lt;- fish_tax %&gt;% \n1  dplyr::mutate(species = gsub(pattern = \"^eel$\",\n                               replacement = \"moray eel\", \n                               x = species))\n\n# Re-check to make sure that fixed it\nsupportR::diff_check(old = fish_ct$species, new = fish_tax_v2$species)\n\n\n1\n\nThe symbols around “eel” mean that we’re only finding/replacing exact matches. It doesn’t matter in this context but often replacing a partial match would result in more problems. For example, replacing “eel” with “moray eel” could make “electric eel” into “electric moray eel”.\n\n\n\n\nAll elements of old object found in new\n\n\nAll elements of new object found in old\n\n\nNow that the shared column matches between the two two dataframes we can use a join to combine them! There are four types of join:\n\nleft/right join\nfull join (a.k.a. outer join)\ninner join\nanti join\n\nYou can learn more about the types of join here or here but the quick explanation is that the name of the join indicates whether the rows of the “left” and/or the “right” table are retained in the combined table. In synthesis work a left join or full join is most common (where you have your primary data in the left position and some ancillary/supplementary dataset in the right position).\n\n# Let's combine the fish count and fish taxonomy information\nfish_df &lt;- fish_ct %&gt;% \n  # Actual join step\n1  dplyr::left_join(y = fish_tax_v2, by = \"species\") %&gt;%\n  # Move 'family' column to the left of all other columns\n  dplyr::relocate(family, .before = dplyr::everything())\n\n# Look at the result of that\nfish_df\n\n\n1\n\nThe ‘by’ argument accepts a vector of column names found in both data tables\n\n\n\n\n          family   species count\n1     Salmonidae    salmon    12\n2     Serranidae      bass    14\n3 Pleuronectidae   halibut    23\n4     Muraenidae moray eel     1\n\n\n\n\n\n\n\n\nActivity: Separating Columns & Joining Data\n\n\n\nIn a script, attempt the following with the PIE crab data:\n\nCreate a data frame where you bin months into seasons (i.e., winter, spring, summer, fall)\n\nUse your judgement on which month(s) should fall into each given PIE’s latitude/location\n\nJoin your season table to the PIE crab data based on month\n\nHint: you may need to modify the PIE dataset to ensure both data tables share at least one column upon which they can be joined\n\nCalculate the average size of crabs in each season in order to identify which season correlates with the largest crabs\n\n\n\n\n\nLeveraging Data Shape\nYou may already be familiar with data shape but fewer people recognize how playing with the shape of data can make certain operations dramatically more efficient. If you haven’t encountered it before, any data table can be said to have one of two ‘shapes’: either long or wide. Wide data have all measured variables from a single observation in one row (typically resulting in more columns than rows or “wider” data tables). Long data usually have one observation split into many rows (typically resulting in more rows than columns or “longer” data tables).\nData shape is often important for statistical analysis or visualization but it has an under-appreciated role to play in quality control efforts as well. If many columns have the shared criteria for what constitutes “tidy”, you can reshape the data to get all of those values into a single column (i.e., reshape longer), perform any needed wrangling, then–when you’re finished–reshape back into the original data shape (i.e., reshape wider). As opposed to applying the same operations repeatedly to each column individually.\nLet’s consider an example to help clarify this. We’ll simulate a butterfly dataset where both the number of different species and their sex were recorded in the same column. This makes the column not technically numeric and therefore unusable in analysis or visualization.\n\n# Generate a butterfly dataframe\nbfly_v1 &lt;- data.frame(\"pasture\" = c(\"PNW\", \"PNW\", \"RCS\", \"RCS\"),\n                      \"monarch\" = c(\"14m\", \"10f\", \"7m\", \"16f\"),\n                      \"melissa_blue\" = c(\"32m\", \"2f\", \"6m\", \"0f\"),\n                      \"swallowtail\" = c(\"1m\", \"3f\", \"0m\", \"5f\"))\n\n# First we'll reshape this into long format\nbfly_v2 &lt;- bfly_v1 %&gt;% \n  tidyr::pivot_longer(cols = -pasture, \n                      names_to = \"butterfly_sp\", \n                      values_to = \"count_sex\")\n\n# Check what that leaves us with\nhead(bfly_v2, n = 4)\n\n# A tibble: 4 × 3\n  pasture butterfly_sp count_sex\n  &lt;chr&gt;   &lt;chr&gt;        &lt;chr&gt;    \n1 PNW     monarch      14m      \n2 PNW     melissa_blue 32m      \n3 PNW     swallowtail  1m       \n4 PNW     monarch      10f      \n\n# Let's separate count from sex to be more usable later\nbfly_v3 &lt;- bfly_v2 %&gt;% \n  tidyr::separate_wider_regex(cols = count_sex, \n                              c(count = \"[[:digit:]]+\", sex = \"[[:alpha:]]\")) %&gt;% \n  # Make the 'count' column a real number now\n  dplyr::mutate(count = as.numeric(count))\n\n# Re-check output\nhead(bfly_v3, n = 4)\n\n# A tibble: 4 × 4\n  pasture butterfly_sp count sex  \n  &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;\n1 PNW     monarch         14 m    \n2 PNW     melissa_blue    32 m    \n3 PNW     swallowtail      1 m    \n4 PNW     monarch         10 f    \n\n# Reshape back into wide-ish format\nbfly_v4 &lt;- bfly_v3 %&gt;% \n  tidyr::pivot_wider(names_from = \"butterfly_sp\", values_from = count)\n\n# Re-re-check output\nhead(bfly_v4)\n\n# A tibble: 4 × 5\n  pasture sex   monarch melissa_blue swallowtail\n  &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n1 PNW     m          14           32           1\n2 PNW     f          10            2           3\n3 RCS     m           7            6           0\n4 RCS     f          16            0           5\n\n\nWhile we absolutely could have used the same function to break apart count and butterfly sex data it would have involved copy/pasting the same information repeatedly. By pivoting to long format first, we can greatly streamline our code. This can also be advantageous for unit conversions, applying data transformations, or checking text column contents among many other possible applications.\n\n\nLoops\nAnother way of simplfying repetitive operations is to use a “for loop” (often called simply “loops”). Loops allow you to iterate across a piece of code for a set number of times. Loops require you to define an “index” object that will change itself at the end of each iteration of the loop before beginning the next iteration. This index object’s identity will be determined by whatever set of values you define at the top of the loop.\nHere’s a very bare bones example to demonstrate the fundamentals.\n\n# Loop across each number between 2 and 4\n1for(k in 2:4){\n  \n  # Square the number\n  result &lt;- k^2\n  \n  # Message that outside of the loop\n  message(k, \" squared is \", result)\n2}\n\n\n1\n\n‘k’ is our index object in this loop\n\n2\n\nNote that the operations to iterate across are wrapped in curly braces ({...})\n\n\n\n\n2 squared is 4\n\n\n3 squared is 9\n\n\n4 squared is 16\n\n\nOnce you get the hang of loops, they can be a nice way of simplifying your code in a relatively human-readable way! Let’s return to our Plum Island Ecosystems crab dataset for a more nuanced example.\n\n# Create an empty list\ncrab_list &lt;- list()\n\n# Let's loop across size categories of crab\n1for(focal_size in unique(pie_crab_v4$size_category)){\n  \n  # Subset the data to just this size category\n  focal_df &lt;- pie_crab_v4 %&gt;% \n    dplyr::filter(size_category == focal_size)\n  \n  # Calculate average and standard deviation of size within this category\n  size_avg &lt;- mean(focal_df$size, na.rm = T) \n  size_dev &lt;- sd(focal_df$size, na.rm = T) \n  \n  # Assemble this into a data table and add to our list\n  crab_list[[focal_size]] &lt;- data.frame(\"size_category\" = focal_size,\n                                        \"size_mean\" = size_avg,\n                                        \"size_sd\" = size_dev)\n} # Close loop\n\n# Unlist the outputs into a dataframe\n2crab_df &lt;- purrr::list_rbind(x = crab_list)\n\n# Check out the resulting data table\ncrab_df\n\n\n1\n\nNote that this is not the most efficient way of doing group-wise summarization but is–hopefully–a nice demonstration of loops!\n\n2\n\nWhen all elements of your list have the same column names, list_rbind efficiently stacks those elements into one longer data table.\n\n\n\n\n  size_category size_mean   size_sd\n1         small 12.624270 1.3827471\n2          tiny  8.876944 0.9112686\n3           big 17.238267 1.3650173\n4          huge 21.196786 0.8276744\n\n\n\n\nCustom Functions\nFinally, writing your own, customized functions can be really useful particularly when doing synthesis work. Custom functions are generally useful for reducing duplication and increasing ease of maintenance (see the note on custom functions in the SSECR Reproducibility module) and also can be useful end products of synthesis work in and of themselves.\nIf one of your group’s outputs is a new standard data format or analytical workflow, the functions that you develop to aid yourself become valuable to anyone who adopts your synthesis project’s findings into their own workflows. If you get enough functions you can even release a package that others can install and use on their own computers. Such packages are a valuable product of synthesis efforts and can be a nice addition to a robust scientific resume/CV.\n\n# Define custom function\ncrab_hist &lt;- function(df, size_cat){\n  \n  # Subset data to the desired category\n  data_sub &lt;- dplyr::filter(.data = df, size_category == size_cat)\n  \n  # Create a histogram\n  p &lt;- psych::multi.hist(x = data_sub$size)\n}\n\n# Invoke function\ncrab_hist(df = pie_crab_v4, size_cat = \"tiny\")\n\n\n\n\n\n\n\n\nWhen writing your own functions it can also be useful to program defensively. This involves anticipating likely errors and writing your own error messages that are more informative to the user than whatever machine-generated error would otherwise get generated\n\n# Define custom function\n1crab_hist &lt;- function(df, size_cat = \"small\"){\n  \n  # Error out if 'df' isn't the right format\n2  if(is.data.frame(df) != TRUE)\n    stop(\"'df' must be provided as a data frame\")\n  \n  # Error out if the data doesn't have the right columns\n3  if(all(c(\"size_category\", \"size\") %in% names(df)) != TRUE)\n    stop(\"'df' must include a 'size' and 'size_category' column\")\n  \n  # Error out for unsupported size category values\n  if(size_cat %in% unique(df$size_category) != TRUE)\n    stop(\"Specified 'size_cat' not found in provided data\")\n  \n  # Subset data to the desired category\n  data_sub &lt;- dplyr::filter(.data = df, size_category == size_cat)\n  \n  # Create a histogram\n  p &lt;- psych::multi.hist(x = data_sub$size)\n}\n\n# Invoke new-and-improved function\n4crab_hist(df = pie_crab_v4)\n\n\n1\n\nThe default category is now set to “small”\n\n2\n\nWe recommend phrasing your error checks with this format (i.e., ’if &lt;some condition&gt; is not true, then &lt;informative error/warning message&gt;)\n\n3\n\nThe %in% operator lets you check whether one value matches any element of a set of accepted values. Very useful in contexts like this because the alternative would be a lot of separate “or” conditionals\n\n4\n\nWe don’t need to specify the ‘size_cat’ argument because we can rely on the default\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nActivity: Custom Functions\n\n\n\nIn a script, attempt the following on the PIE crab data:\n\nWrite a function that:\n\n\ncalculates the median of the user-supplied column\n\n\ndetermines whether each value is above, equal to, or below the median\n\n\nmakes a column indicating the results of step B\n\n\nUse the function on the standard deviation of water temperature\nUse it again on the standard deviation of air temperature\nRevisit your function and identify 2-3 likely errors\nWrite custom checks (and error messages) for the set of likely issues you just identified",
    "crumbs": [
      "Phase III -- Execute",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "mod_wrangle.html#additional-resources",
    "href": "mod_wrangle.html#additional-resources",
    "title": "Data Harmonization & Wrangling",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\nReviews and Syntheses: The Promise of Big Diverse Soil Data, Moving Current Practices Towards Future Potential. Todd-Brown, K.E.O. et al., 2022. Biogeosciences\nThe Ultimate Guide to Data Cleaning. Elgarby, O. 2019. Medium\nSome Simple Guidelines for Effective Data Management. Borer, E. et al., 2009. Ecological Society of America Bulletin\n\n\n\nWorkshops & Courses\n\nData Analysis and Visualization in R for Ecologists, Episode 4: Manipulating, Analyzing, and Exporting Data with tidyverse. The Carpentries\nCoding in the Tidyverse. NCEAS Scientific Computing Team, 2023.\nNCEAS Learning Hub’s coreR Course, Chapter 8: Cleaning & Wrangling Data. NCEAS Learning Hub, 2023.\nNCEAS Learning Hub’s coreR Course, Chapter 16: Writing Functions & Packages. NCEAS Learning Hub, 2023.\nOpen Science Synthesis for the Delta Science Program’s Data Munging / QA / QC / Cleaning\n\n\n\nWebsites\n\nTen Commandments for Good Data Management",
    "crumbs": [
      "Phase III -- Execute",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "mod_project-mgmt.html",
    "href": "mod_project-mgmt.html",
    "title": "Project Management & Logic Models",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase II -- Plan",
      "Project Management"
    ]
  },
  {
    "objectID": "mod_project-mgmt.html#overview",
    "href": "mod_project-mgmt.html#overview",
    "title": "Project Management & Logic Models",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase II -- Plan",
      "Project Management"
    ]
  },
  {
    "objectID": "mod_project-mgmt.html#learning-objectives",
    "href": "mod_project-mgmt.html#learning-objectives",
    "title": "Project Management & Logic Models",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nArticulate key principles of project management\nDevelop (or refine) the project management framework for your team project\nDefine common approaches for logic models\nIdentify and make explicit internal logical leaps",
    "crumbs": [
      "Phase II -- Plan",
      "Project Management"
    ]
  },
  {
    "objectID": "mod_project-mgmt.html#module-content",
    "href": "mod_project-mgmt.html#module-content",
    "title": "Project Management & Logic Models",
    "section": "Module Content",
    "text": "Module Content",
    "crumbs": [
      "Phase II -- Plan",
      "Project Management"
    ]
  },
  {
    "objectID": "mod_project-mgmt.html#additional-resources",
    "href": "mod_project-mgmt.html#additional-resources",
    "title": "Project Management & Logic Models",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\n\n\n\n\nWorkshops & Courses\n\nNCEAS Scientific Computing team’s Collaborative Coding with GitHub workshop project management modules\n\nGitHub Issues\nGitHub Projects\n\nOpen Science Synthesis for the Delta Science Program’s Logic Models and Synthesis Development\n\n\n\nWebsites",
    "crumbs": [
      "Phase II -- Plan",
      "Project Management"
    ]
  },
  {
    "objectID": "mod_credit.html",
    "href": "mod_credit.html",
    "title": "Authorship & Intellectual Credit",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase III -- Execute",
      "Intellectual Credit"
    ]
  },
  {
    "objectID": "mod_credit.html#overview",
    "href": "mod_credit.html#overview",
    "title": "Authorship & Intellectual Credit",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase III -- Execute",
      "Intellectual Credit"
    ]
  },
  {
    "objectID": "mod_credit.html#learning-objectives",
    "href": "mod_credit.html#learning-objectives",
    "title": "Authorship & Intellectual Credit",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nDefine types of intellectual contributions to a synthesis project\nDescribe some common frameworks for equitable authorship decisions\nExplain benefits (or avoided costs) of making authorship decisions both collaboratively and transparently\nCreate a draft intellectual credit plan for your team",
    "crumbs": [
      "Phase III -- Execute",
      "Intellectual Credit"
    ]
  },
  {
    "objectID": "mod_credit.html#module-content",
    "href": "mod_credit.html#module-content",
    "title": "Authorship & Intellectual Credit",
    "section": "Module Content",
    "text": "Module Content",
    "crumbs": [
      "Phase III -- Execute",
      "Intellectual Credit"
    ]
  },
  {
    "objectID": "mod_credit.html#additional-resources",
    "href": "mod_credit.html#additional-resources",
    "title": "Authorship & Intellectual Credit",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\nDahlin et al., Hear Every Voice, Working Groups that Work. 2019. Frontiers in Ecology and the Environment\nAllen et al., How Can We Ensure Visibility and Diversity in Research Contributions? How the Contributor Role Taxonomy (CReDiT) is Helping the Shift from Authorship to Contributorship. 2018. Learned Publishing\n\n\n\nWorkshops & Courses\n\n\n\n\n\nWebsites\n\nNutrient Network (NutNet) authorship policy\nHerbivory Variability Network (HerbVar) participation guidelines",
    "crumbs": [
      "Phase III -- Execute",
      "Intellectual Credit"
    ]
  },
  {
    "objectID": "mod_findings.html",
    "href": "mod_findings.html",
    "title": "Communicating Findings",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase V -- Share",
      "Communicating Findings"
    ]
  },
  {
    "objectID": "mod_findings.html#overview",
    "href": "mod_findings.html#overview",
    "title": "Communicating Findings",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase V -- Share",
      "Communicating Findings"
    ]
  },
  {
    "objectID": "mod_findings.html#learning-objectives",
    "href": "mod_findings.html#learning-objectives",
    "title": "Communicating Findings",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nDefine elements of (in)effective communication\nIdentify relevant audiences for a particular work\nDetermine audience motivations and interest\nTranslate communication into various formats based on efficacy with target group",
    "crumbs": [
      "Phase V -- Share",
      "Communicating Findings"
    ]
  },
  {
    "objectID": "mod_findings.html#effective-communication",
    "href": "mod_findings.html#effective-communication",
    "title": "Communicating Findings",
    "section": "Effective Communication",
    "text": "Effective Communication",
    "crumbs": [
      "Phase V -- Share",
      "Communicating Findings"
    ]
  },
  {
    "objectID": "mod_findings.html#publishing-code-data-and-results",
    "href": "mod_findings.html#publishing-code-data-and-results",
    "title": "Communicating Findings",
    "section": "Publishing Code, Data, and Results",
    "text": "Publishing Code, Data, and Results",
    "crumbs": [
      "Phase V -- Share",
      "Communicating Findings"
    ]
  },
  {
    "objectID": "mod_findings.html#data-management-plans",
    "href": "mod_findings.html#data-management-plans",
    "title": "Communicating Findings",
    "section": "Data Management Plans",
    "text": "Data Management Plans",
    "crumbs": [
      "Phase V -- Share",
      "Communicating Findings"
    ]
  },
  {
    "objectID": "mod_findings.html#additional-resources",
    "href": "mod_findings.html#additional-resources",
    "title": "Communicating Findings",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\n\n\n\n\nWorkshops & Courses\n\nOpen Science Synthesis for the Delta Science Program’s Communicating Your Science\n\n\n\nWebsites\n\nCompass’ The Message Box",
    "crumbs": [
      "Phase V -- Share",
      "Communicating Findings"
    ]
  },
  {
    "objectID": "mod_reports.html",
    "href": "mod_reports.html",
    "title": "Reproducible Reports",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase V -- Share",
      "Reproducible Reports"
    ]
  },
  {
    "objectID": "mod_reports.html#overview",
    "href": "mod_reports.html#overview",
    "title": "Reproducible Reports",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase V -- Share",
      "Reproducible Reports"
    ]
  },
  {
    "objectID": "mod_reports.html#learning-objectives",
    "href": "mod_reports.html#learning-objectives",
    "title": "Reproducible Reports",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nIdentify the three fundamental elements of R Markdown / Quarto documents\nUse Markdown syntax to accomplish text styling\nCreate reports that use a blend of plain text and embedded code to effectively communicate rationale, methodologies, and primary findings\nMake a (small) Quarto website\nDeploy a website and/or report through GitHub Pages",
    "crumbs": [
      "Phase V -- Share",
      "Reproducible Reports"
    ]
  },
  {
    "objectID": "mod_reports.html#module-content",
    "href": "mod_reports.html#module-content",
    "title": "Reproducible Reports",
    "section": "Module Content",
    "text": "Module Content",
    "crumbs": [
      "Phase V -- Share",
      "Reproducible Reports"
    ]
  },
  {
    "objectID": "mod_reports.html#additional-resources",
    "href": "mod_reports.html#additional-resources",
    "title": "Reproducible Reports",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\n\n\n\n\nWorkshops & Courses\n\nNCEAS coreR Literature Analysis with Quarto session\nOSS Reproducible Papers with R Markdown\nUCSB’s Master of Environmental Data Science (MEDS) Creating your Personal Website using Quarto lesson\n\n\n\nWebsites\n\nPosit’s Welcome to Quarto",
    "crumbs": [
      "Phase V -- Share",
      "Reproducible Reports"
    ]
  },
  {
    "objectID": "policy_attendance.html",
    "href": "policy_attendance.html",
    "title": "Synthesis Skills for Early Career Researchers",
    "section": "",
    "text": "If you get sick, observe a religious holiday unaccounted for by the SSECR schedule, have to miss class for an interview, or simply don’t think you can handle class on a given day, please email the course instructors as early as possible to let us know that you won’t be in class with a (brief) explanation. This will help us to share resources we’ll cover in class with you and plan for a smaller in-class community while you are out. Our hope is that this course will be somewhere you want to attend, but we totally understand that you have many demands on your time and sometimes life happens!\nPlease keep in mind that your presence in and contributions to class are important both to your understanding of the material and the creation and maintenance of an in-class community."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Course Overview",
    "section": "",
    "text": "Synthesis Skills for Early Career Researchers (SSECR; [SEE-ker]) is a newly-designed course organized by the Long Term Ecological Research (LTER) Network. This course aims to address the need for more comprehensive interpersonal and data skills in ecology. You can find more information on SSECR at the LTER Network page for the course.\nIf you’re interested in joining the course as a student you can see the application here. If instead you’re interested in joining as a team project mentor you can find more information–and apply–here.",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#course-priorities",
    "href": "index.html#course-priorities",
    "title": "Course Overview",
    "section": "Course Priorities",
    "text": "Course Priorities\n\nSurface and test new synthesis ideas for feasibility\nPrepare more graduate students to be effective participants in/leaders of the synthesis projects\nConnect LTER graduate students across sites\nDevelop intergenerational linkages around synthesis research",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Course Overview",
    "section": "Course Description",
    "text": "Course Description\nThe course is structured around small group synthesis projects, so that lessons are immediately applied to a synthesis problem that is relevant to learners and will likely result in a publication. The course starts with an in-person launch to establish cohort cohesion and ensure that any setup issues are resolved. Participants will pitch projects to the group and assemble a team of collaborators.The ideal configuration would be 6 project teams of 4-5 students each. Prior to the start of the course, the Network Office will recruit a corps of potential project mentors, who will be matched with participant projects and who agree to meet with students approximately 4-5 times per year.\nApplicants will propose modest or exploratory synthesis projects as part of the application process. In addition to ideas stemming from participants’ own work, course mentors will make available a small library of synthesis ideas in search of execution. The in-person kickoff week will focus on cohort-building, pitching projects and assembling project teams, identifying relevant data, and getting set up on servers and collaboration tools.",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#course-scheduling",
    "href": "index.html#course-scheduling",
    "title": "Course Overview",
    "section": "Course Scheduling",
    "text": "Course Scheduling\nCourse participants meet for three hours, weekly, at the same time each week. Sessions alternate between hands-on instruction in technical and soft skills that are relevant for inclusive synthesis and team work on the chosen group projects.\nEach three-hour session will include 2 components:\n\nInspiration (~60 minutes): Presentation by an experienced synthesis scientist, describing why and how they conduct synthesis. This diverse group of researchers will be recruited from across the field and course participants will have ample time for discussion with each presenter.\nInstruction (~120 minutes): Each session will focus on a specific instructional topic, with technical skills, team-science skills, and communication topics interspersed throughout the year. The discussion will be limited to official course participants, but instructional materials for each topic will be available online, allowing individuals or site- or topic-based groups to follow along independently.\n\nTechnical skills will build on earlier lessons and are not intended to be stand-alone modules. The course will include social and leadership skills required to bring a synthesis project from idea to completion (or, for larger projects to completed proposal) and will include techniques for ensuring that multiple thinking and learning styles are respected and valued.\n\n\nProject groups will also meet at a time of their own choosing to work on projects. Project mentors are encouraged to participate in work sessions at least 4 times throughout the year.",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#course-policies",
    "href": "index.html#course-policies",
    "title": "Course Overview",
    "section": "Course Policies",
    "text": "Course Policies\n\nAttendance\nIf you get sick, observe a religious holiday unaccounted for by the SSECR schedule, have to miss class for an interview, or simply don’t think you can handle class on a given day, please email the course instructors as early as possible to let us know that you won’t be in class with a (brief) explanation. This will help us to share resources we’ll cover in class with you and plan for a smaller in-class community while you are out. Our hope is that this course will be somewhere you want to attend, but we totally understand that you have many demands on your time and sometimes life happens!\nPlease keep in mind that your presence in and contributions to class are important both to your understanding of the material and the creation and maintenance of an in-class community.\n\n\nUsability, Accessibility, and Design\nWe are committed to creating a course that is inclusive in its design. If you encounter barriers, please let the instructors know immediately so that we can determine if there is a design adjustment that can be made or if an accommodation might be needed to overcome the limitations of the design. We are always happy to consider creative solutions as long as they do not compromise the intent of the learning activity. We welcome feedback that will assist us in improving the usability and experience for all students.\n\n\nArtificial Intelligence Tools\nArtificial intelligence (AI) tools are increasingly well-known and widely discussed in the context of data science. AI products can increase the efficiency of code writing and are becoming a common part of the data science landscape. For the purposes of this course, we strongly recommend that you do not use AI tools to write code. There is an under-discussed ethical consideration to the use and training of these tools in addition to their known practical limitations. However, the main reason we suggest you not use them for this class though is that leaning too heavily upon AI tools is likely to negatively impact your learning and skill acquisition.\nYou may have prior experience with some of the quantitative skills this course aims to teach but others are likely new to you. During the first steps of learning any new skill, it can be really helpful to struggle a bit in solving problems. Your efforts now will help refine your troubleshooting skills and will likely make it easier to remember how you solved a given problem the next time it arises. Over-use of AI tools can short circuit this pathway to mastery. Once you have become a proficient coder, you will be better able to identify and avoid any distortions or assumptions introduced by relying on AI.\nAI Resources\n\nPratim Ray, P. ChatGPT: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope. 2023. Internet of Things and Cyber-Physical Systems\nTrust, T. ChatGPT & Education Slide Deck. 2023. National Teaching Repository\nCsik, S. Teach Me How to Google. University of California, Santa Barbara (UCSB) Master of Environmental Data Science (MEDS) Program.\n\n\n\nName, Gender Identity, and/or Gender Expression\nYou provided a name when you first applied to be a part of the course but we will gladly honor your request to be addressed by an alternate name. We will also use whichever pronouns you identify with. Please advise us of your pronouns and/or chosen name early in the course so that we can ensure that we treat you respectfully throughout the course.\n\n\nCode of Conduct\nGroup work is a significant part of this course explicitly in the synthesis project facet as well as implicitly by the collaborative nature of many of the modules. We expect that you will be mutually respectful with one another both in and outside of class time. We will ask you questions during the course and during class is also an ideal time for you all to ask us questions that you have on course topics or policies. We don’t believe that “dumb questions” exist, and expect that you treat your peers’ questions with the respect you’d like your questions to be with. We will learn more together in an environment where we build one another up than we would in one where we fail to support one another.",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "mod_spatial.html",
    "href": "mod_spatial.html",
    "title": "Working with Spatial Data",
    "section": "",
    "text": "Under Construction",
    "crumbs": [
      "Bonus Modules",
      "Spatial Data"
    ]
  },
  {
    "objectID": "mod_spatial.html#overview",
    "href": "mod_spatial.html#overview",
    "title": "Working with Spatial Data",
    "section": "",
    "text": "Under Construction",
    "crumbs": [
      "Bonus Modules",
      "Spatial Data"
    ]
  },
  {
    "objectID": "mod_spatial.html#learning-objectives",
    "href": "mod_spatial.html#learning-objectives",
    "title": "Working with Spatial Data",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this topic you will be able to:\n\nDefine characteristics of common types of spatial data\nManipulate spatial data with R\nIntegrate spatial data with tabular data",
    "crumbs": [
      "Bonus Modules",
      "Spatial Data"
    ]
  },
  {
    "objectID": "mod_spatial.html#needed-packages",
    "href": "mod_spatial.html#needed-packages",
    "title": "Working with Spatial Data",
    "section": "Needed Packages",
    "text": "Needed Packages\nIf you’d like to follow along with the code chunks included throughout this module, you’ll need to install the following packages:\n\n# Note that these lines only need to be run once per computer\n## So you can skip this step if you've installed these before\ninstall.packages(\"tidyverse\")",
    "crumbs": [
      "Bonus Modules",
      "Spatial Data"
    ]
  },
  {
    "objectID": "mod_spatial.html#raster-versus-vector-data",
    "href": "mod_spatial.html#raster-versus-vector-data",
    "title": "Working with Spatial Data",
    "section": "Raster versus Vector Data",
    "text": "Raster versus Vector Data",
    "crumbs": [
      "Bonus Modules",
      "Spatial Data"
    ]
  },
  {
    "objectID": "mod_spatial.html#coordinate-reference-systems",
    "href": "mod_spatial.html#coordinate-reference-systems",
    "title": "Working with Spatial Data",
    "section": "Coordinate Reference Systems",
    "text": "Coordinate Reference Systems",
    "crumbs": [
      "Bonus Modules",
      "Spatial Data"
    ]
  },
  {
    "objectID": "mod_spatial.html#making-maps",
    "href": "mod_spatial.html#making-maps",
    "title": "Working with Spatial Data",
    "section": "Making Maps",
    "text": "Making Maps",
    "crumbs": [
      "Bonus Modules",
      "Spatial Data"
    ]
  },
  {
    "objectID": "mod_spatial.html#extracting-spatial-data",
    "href": "mod_spatial.html#extracting-spatial-data",
    "title": "Working with Spatial Data",
    "section": "Extracting Spatial Data",
    "text": "Extracting Spatial Data",
    "crumbs": [
      "Bonus Modules",
      "Spatial Data"
    ]
  },
  {
    "objectID": "mod_spatial.html#additional-spatial-resources",
    "href": "mod_spatial.html#additional-spatial-resources",
    "title": "Working with Spatial Data",
    "section": "Additional Spatial Resources",
    "text": "Additional Spatial Resources\n\nPapers & Documents\n\n\n\n\n\nWorkshops & Courses\n\nThe Carpentries’ Introduction to Geospatial Raster & Vector Data with R\nThe Carpentries’ Introduction to R for Geospatial Data\nArctic Data Center’s Spatial and Image Data Using GeoPandas chapter of their Scalable Computing course\nJason Flower’s (UC Santa Barbara) Introduction to rasters with terra\nKing, R. Spatial Data Visualization workshop\n\n\n\nWebsites\n\nNASA’s Application for Extracting and Exploring Analysis Ready Samples (AppEEARS) Portal",
    "crumbs": [
      "Bonus Modules",
      "Spatial Data"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "SSECR Contributing Guidelines",
    "section": "",
    "text": "We are using a GitHub Team (see here for more information on GitHub Teams) to simplify access protocols to SSECR GitHub materials.\nContact Marty Downs and/or Nick Lyon to be added to the SSECR GitHub Team. This will give you write-level access to (A) the SSECR GitHub repository and (B) the SSECR GitHub Project that we’re using for task management.\n\n\n\nContact Marty Downs and/or Nick Lyon for access to the Google Drive. The Shared Drive is named “LTER-Grad-Course”.\n\n\n\nCommunicating via email is fine though we also have a channel (#lter-grad-course) in NCEAS’ Slack organization if that is preferable.\n\n\n\n\n\nIndividual tasks should be tracked as GitHub Issues\n\nBe sure that each task is SMART (i.e., specific, measurable, achievable, relevant, and time-bound)\n\nPlease use the issue template\n\nWhen you select “New Issue” you will be prompted to use this template automatically\n\nTry to document task progress within the dedicated issue for that task (for posterity)\nStrategic planning (i.e., project management across tasks) should use the SSECR GitHub Project\n\nTask lifecycle can be tracked by dragging an issue’s “card” among columns that correspond to major steps in task completion\n\n\n\n\n\nAs much as possible, use snake case (i.e., all_lowercase_separated_by_underscores). When in doubt, try to maintain consistency with the naming convention and internal structure of other files in the same directory/repository."
  },
  {
    "objectID": "CONTRIBUTING.html#accessing-course-materials",
    "href": "CONTRIBUTING.html#accessing-course-materials",
    "title": "SSECR Contributing Guidelines",
    "section": "",
    "text": "We are using a GitHub Team (see here for more information on GitHub Teams) to simplify access protocols to SSECR GitHub materials.\nContact Marty Downs and/or Nick Lyon to be added to the SSECR GitHub Team. This will give you write-level access to (A) the SSECR GitHub repository and (B) the SSECR GitHub Project that we’re using for task management.\n\n\n\nContact Marty Downs and/or Nick Lyon for access to the Google Drive. The Shared Drive is named “LTER-Grad-Course”.\n\n\n\nCommunicating via email is fine though we also have a channel (#lter-grad-course) in NCEAS’ Slack organization if that is preferable."
  },
  {
    "objectID": "CONTRIBUTING.html#project-management",
    "href": "CONTRIBUTING.html#project-management",
    "title": "SSECR Contributing Guidelines",
    "section": "",
    "text": "Individual tasks should be tracked as GitHub Issues\n\nBe sure that each task is SMART (i.e., specific, measurable, achievable, relevant, and time-bound)\n\nPlease use the issue template\n\nWhen you select “New Issue” you will be prompted to use this template automatically\n\nTry to document task progress within the dedicated issue for that task (for posterity)\nStrategic planning (i.e., project management across tasks) should use the SSECR GitHub Project\n\nTask lifecycle can be tracked by dragging an issue’s “card” among columns that correspond to major steps in task completion"
  },
  {
    "objectID": "CONTRIBUTING.html#style-guide",
    "href": "CONTRIBUTING.html#style-guide",
    "title": "SSECR Contributing Guidelines",
    "section": "",
    "text": "As much as possible, use snake case (i.e., all_lowercase_separated_by_underscores). When in doubt, try to maintain consistency with the naming convention and internal structure of other files in the same directory/repository."
  },
  {
    "objectID": "mod_reproducibility.html",
    "href": "mod_reproducibility.html",
    "title": "Reproducibility Best Practices",
    "section": "",
    "text": "As we set out to engage with the synthesis skills this course aims to offer, it will be helpful to begin with a careful consideration of “reproducibility.” Because synthesis projects draw data from many sources and typically involve many researchers working in concert, reproducibility is particularly important. In individual projects, adhering to reproducibility best practices is certainly a good goal but failing to do so for synthesis projects can severely limit the work in a more significant way than for those individual projects. “Reproducibility” is a wide sphere encompassing many different–albeit related–topics so it can be challenging to feel well-equipped to evaluate how well we are following these guidelines in our own work. In this module, we will cover a few fundamental facets of reproducibility and point to some considerations that may encourage you to push yourself to elevate your practices to the next level.",
    "crumbs": [
      "Phase I -- Prepare",
      "Reproducibility"
    ]
  },
  {
    "objectID": "mod_reproducibility.html#overview",
    "href": "mod_reproducibility.html#overview",
    "title": "Reproducibility Best Practices",
    "section": "",
    "text": "As we set out to engage with the synthesis skills this course aims to offer, it will be helpful to begin with a careful consideration of “reproducibility.” Because synthesis projects draw data from many sources and typically involve many researchers working in concert, reproducibility is particularly important. In individual projects, adhering to reproducibility best practices is certainly a good goal but failing to do so for synthesis projects can severely limit the work in a more significant way than for those individual projects. “Reproducibility” is a wide sphere encompassing many different–albeit related–topics so it can be challenging to feel well-equipped to evaluate how well we are following these guidelines in our own work. In this module, we will cover a few fundamental facets of reproducibility and point to some considerations that may encourage you to push yourself to elevate your practices to the next level.",
    "crumbs": [
      "Phase I -- Prepare",
      "Reproducibility"
    ]
  },
  {
    "objectID": "mod_reproducibility.html#learning-objectives",
    "href": "mod_reproducibility.html#learning-objectives",
    "title": "Reproducibility Best Practices",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nIdentify core tenets of reproducibility best practices\nCreate robust workflow documentation\nImplement reproducible project organization strategies\nDiscuss methods for improving the reproducibility of your code products\nSummarize FAIR and CARE data principles\nEvaluate the FAIR/CAREness of your work",
    "crumbs": [
      "Phase I -- Prepare",
      "Reproducibility"
    ]
  },
  {
    "objectID": "mod_reproducibility.html#lego-activity",
    "href": "mod_reproducibility.html#lego-activity",
    "title": "Reproducibility Best Practices",
    "section": "Lego Activity",
    "text": "Lego Activity\nBefore we dive into the world of reproducibility for synthesis projects, we thought it would be fun (and informative!) to begin with an activity that is a useful analogy for the importance of some of the concepts we’ll cover today. The LEGO activity was designed by Mary Donaldson and Matt Mahon at the University of Glasgow. The full materials can be accessed here.",
    "crumbs": [
      "Phase I -- Prepare",
      "Reproducibility"
    ]
  },
  {
    "objectID": "mod_reproducibility.html#project-documentation-organization",
    "href": "mod_reproducibility.html#project-documentation-organization",
    "title": "Reproducibility Best Practices",
    "section": "Project Documentation & Organization",
    "text": "Project Documentation & Organization\nMuch of the popular conversation around reproducibility centers on reproducibility as it pertains to code. That is definitely an important facet but before we write even a single line it is vital to consider project-wide reproducibility. “Perfect” code in a project that isn’t structured thoughtfully can still result in a project that isn’t reproducible. On the other hand, “bad” code can be made more intelligible when it is placed in a well-documented/organized project!\n\nDocumentation\nDocumenting a project can feel daunting but it is often not as hard as one might imagine and always well worth the effort! One simple practice you can adopt to dramatically improve the reproducibility of your project is to create a “README” file in the top-level of your project’s folder system. This file can be formatted however you’d like but generally READMEs should include:\n\nProject overview written in plain language\nBasic table of contents for the primary folders in your project folder\nBrief description of the file naming scheme you’ve adopted for this project.\n\nYour project’s README becomes the ‘landing page’ for those navigating your repository and makes it easy for team members to know where documentation should go (in the README!). You may also choose to create a README file for some of the sub-folders of your project. This can be particularly valuable for your “data” folder(s) as it is an easy place to store data source/provenance information that might be overwhelming to include in the project-level README file.\nFinally, you should choose a place to keep track of ideas, conversations, and decisions about the project. While you can take notes on these topics on a piece of paper, adopting a digital equivalent is often helpful because you can much more easily search a lengthy document when it is machine readable. We will discuss GitHub during the Version Control module but GitHub offers something called Issues that can be a really effective place to record some of this information.\n\n\n\n\n\n\nActivity: Create a README\n\n\n\nCreate a draft README for one of your research projects. If all of your projects already have READMEs (very impressive!) revisit the one with the least detail.\n\nInclude a 2-4 sentence description of the project objectives / hypotheses\nIdentify and describe (in 1 sentence) the primary sub-folders in the project\nIf your chosen project includes scripts, summarize each and indicate which script(s) they depend on and which depend on them\n\nFeel free to put your personal flair on the README! If there is other information you feel would be relevant to an outsider looking at your project, you can definitely add that.\n\n\n\n\nFundamental Structure\n\nThe simplest way of beginning a reproducible project is adopting a good file organization system. There is no single “best” way of organizing your projects’ files as long as you are consistent. Consistency will make your system–whatever that consists of–understandable to others.\nHere are some rules to keep in mind as you decide how to organize your project:\n\nUse one folder per project\n\nKeeping all inputs, outputs, and documentation in a single folder makes it easier to collaborate and share all project materials. Also, most programming applications (RStudio, VS Code, etc.) work best when all needed files are in the same folder.\nNote that how you define “projct” may affect the number of folders you need! Some synthesis projects may separate data harmonization into its own project while for others that same effort might not warrant being considered as a separate project. Similarly, you may want to make a separate folder for each manuscript your group plans on writing so that the code for each paper is kept separate.\n\nOrganize content with sub-folders\n\nPutting files that share a purpose or source into logical sub-folders is a great idea! This makes it easy to figure out where to put new content and reduces the effort of documenting project organization. Don’t feel like you need to use an intricate web of sub-folders either! Just one level of sub-folders is enough for many projects.\n\nCraft informative file names\n\nAn ideal file name should give some information about the file’s contents, purpose, and relation to other project files. Some of that may be reinforced by folder names, but the file name itself should be inherently meaningful. This lets you change folder names without fear that files would also need to be re-named.\n\n\n\n\n\n\nDiscussion: Project Structure\n\n\n\nWith a partner discuss (some of) the following questions:\n\nHow do you typically organize your projects’ files?\nWhat benefits do you see of your current approach?\nWhat–if any–limitations to your system have you experienced?\nDo you think your structure would work well in a team environment?\n\nIf not, what changes might you make to better fit that context?\n\n\n\n\n\nNaming Tips\nWe’ve brought up the importance of naming several times already but haven’t actually discussed the specifics of what makes a “good” name for a file or folder. Consider the adopting some (or all!) of the file name tips we outline below.\n\nNames should be sorted by a computer and human in the same way\n\nComputers sort files/folders alphabetically and numerically. Sorting alphabetically rarely matches the order scripts in a workflow should be run. If you add step numbers to the start of each file name the computer will sort the files in an order that makes sense for the project. You may also want to “zero pad” numbers so that all numbers have the same number of digits (e.g., “01” and “10” vs. “1” and “10”).\n\nNames should avoid spaces and special characters\n\nSpaces and special characters (e.g., é, ü, etc.) cause errors in some computers (particularly Windows operating systems). You can replace spaces with underscores or hyphens to increase machine readability. Avoid using special characters as much as possible. You should also be consistent about casing (i.e., lower vs. uppercase).\n\nNames should use consistent delimiters\n\nDelimiters are characters used to separate pieces of information in otherwise plain text. Underscores are a commonly used example of this. If a file/folder name has multiple pieces of information, you can separate these with a delimiter to make them more readable to people and machines. For example, you could name a folder “coral_reef_data” which would be more readable than “coralreefdata”.\nYou may also want to use multiple delimiters to indicate different things. For instance, you could use underscores to differentiate categories and then use hyphens instead of spaces between words.\n\nNames should use “slugs” to connect inputs and outputs\n\nSlugs are human-readable, unique pieces of file names that are shared between files and the outputs that they create. Maybe a script is named “02_tidy.R” and all of the data files it creates are named “02_…”. Weird or unlikely outputs are easily traced to the scripts that created them because of their shared slug.\n\n\n\nOrganizing Example\nThese tips are all worthwhile but they can feel a little abstract without a set of files firmly in mind. Let’s consider an example synthesis project where we incrementally change the project structure to follow increasing more of the guidelines we suggest above.\n\nVersion 1Version 2Version 3Version 4\n\n\n\n\n\n\n\nPositives\n\nAll project files are in one folder\n\n\n\nAreas for Improvement\n\nNo use of sub-folders to divide logically-linked content\nFile names lack key context (e.g., workflow order, inputs vs. outputs, etc.)\nInconsistent use of delimiters\n\n\n\n\n\n\n\n\n\n\n\nPositives\n\nSub-folders used to divide content\nProject documentation included in top level (README and license files)\n\n\n\nAreas for Improvement\n\nFile names still inconsistent\n\nFile names contain different information in different order\nMixed use of delimiters\nMany file names include spaces\n\n\n\n\n\n\n\n\n\n\n\n\nPositives\n\nMost file names contain context\nStandardized use of casing and–within sub-folder–consistent delimiters used\n\n\n\nAreas for Improvement\n\nWorkflow order “guessable” but not explicit\nUnclear which files are inputs / outputs (and of which scripts)\n\n\n\n\n\n\n\n\n\n\n\nPositives\n\nScripts include zero-padded numbers indicating order of operations\nInputs / outputs share zero padded slug with source script\nReport file names machine sorted from least to most recent (top to bottom)\n\n\n\nAreas for Improvement\n\nDepending on sub-folder complexity, could add sub-folder specific README files\nGraph file names still include spaces\n\n\n\n\n\n\n\n\n\nOrganization Recommendations\nIf you integrate any of the concepts we’ve covered above you will find the reproducibility and transparency of your project will greatly increase. However, if you’d like additional recommendations we’ve assembled a non-exhaustive set of additional “best practices” that you may find helpful.\n\nNever Edit Raw Data\nFirst and foremost, it is critical that you never edit the raw data directly. If you do need to edit the raw data, use a script to make all needed edits and save the output of that script as a separate file. Editing the raw data directly without a script or using a script but overwriting the raw data are both incredibly risky operations because your create a file that “looks” like the raw data (and is likely documented as such) but differs from what others would have if they downloaded the ‘real’ raw data personally.\n\n\nSeparate Raw and Processed Data\nIn the same vein as the previous best practice, we recommend that you separate the raw and processed data into separate folders. This will make it easier to avoid accidental edits to the raw data and will make it clear what data are created by your project’s scripts; even if you choose not to adopt a file naming convention that would make this clear.\n\n\nQuarantine External Outputs\nThis can sound harsh, but it is often a good idea to “quarantine” outputs received from others until they can be carefully vetted. This is not at all to suggest that such contributions might be malicious! As you embrace more of the project organization recommendations we’ve described above outputs from others have more and more opportunities to diverge from the framework you establish. Quarantining inputs from others gives you a chance to rename files to be consistent with the rest of your project as well as make sure that the style and content of the code also match (e.g., use or exclusion of particular packages, comment frequency and content, etc.)",
    "crumbs": [
      "Phase I -- Prepare",
      "Reproducibility"
    ]
  },
  {
    "objectID": "mod_reproducibility.html#reproducible-coding",
    "href": "mod_reproducibility.html#reproducible-coding",
    "title": "Reproducibility Best Practices",
    "section": "Reproducible Coding",
    "text": "Reproducible Coding\nNow that you’ve organized your project in a reasonable way and documented those choices, we can move on to principles of reproducible coding! Doing your data operations with scripts is more reproducible than doing those operations without a programming language (i.e., with Microsoft Excel, Google Sheets, etc.). However, scripts are often written in a way that is not reproducible. A recent study aiming to run 2,000 project’s worth of R code found that 74% of the associated R files failed to complete without error (Trisovic et al. 2022). Many of those errors involve coding practices that hinder reproducibility but are easily preventable by the original code authors.\n\nWhen your scripts are clear and reproducibly-written you will reap the following benefits:\n\nReturning to your code after having set it down for weeks/months is much simpler\nCollaborating with team members requires less verbal explanation\nSharing methods for external result validation is more straightforward\nIn cases where you’re developing a novel method or workflow, structuring your code in this way will increase the odds that someone outside of your team will adopt your strategy\n\n\nPackages, Namespacing, and Software Versions\nAn under-appreciated facet of reproducible coding is a record of what code packages are used in a particular script and the version number of those packages. Packages evolve over time and code that worked when using one version of a given package may not work for future versions of that same package. Perpetually updating your code to work with the latest package versions is not sustainable but recording key information can help users set up the code environment that does work for your project.\n\nLoad Libraries Explicitly\nIt is important to load libraries at the start of every script. In some languages (like Python) this step is required but in others (like R) this step is technically “optional” but disastrous to skip. It is safe to skip including the installation step in your code because the library step should tell code-literate users which packages they need to install.\nFor instance you might begin each script with something like:\n# Load needed libraries\nlibrary(dplyr); library(magrittr); library(ggplot2)\n\n# Get to actual work\n. . .\nIn R the semicolon allows you to put multiple code operations in the same line of the script. Listing the needed libraries in this way cuts down on the number of lines while still being precise about which packages are needed in the script.\nIf you are feeling generous you could use the librarian R package to install packages that are not yet installed and simultaneously load all needed libraries. Note that users would still need to install librarian itself but this at least limits possible errors to one location. This is done like so:\n# Load `librarian` package\nlibrary(librarian)\n\n# Install missing packages and load needed libraries\nshelf(dplyr, magrittr, ggplot2)\n\n# Get to actual work\n. . .\n\n\nFunction Namespacing\nIt is also strongly recommended to “namespace” functions everywhere you use them. In R this is technically optional but it is a really good practice to adopt, particularly for functions that may appear in multiple packages with the same name but do very different operations depending on their source. In R the ‘namespacing operator’ is two colons.\n# Use the `mutate` function from the `dplyr` package\ndplyr::mutate(. . .)\nAn ancillary benefit of namespacing is that namespaced functions don’t need to have their respective libraries loaded. Still good practice to load the library though!\n\n\nPackage Versions\nWhile working on a project you should use the latest version of every needed package. However, as you prepare to publish or otherwise publicize your code, you’ll need to record package versions. R provides the sessionInfo function (from the utils package included in “base” R) which neatly summarizes some high level facets of your code environment. Note that for this method to work you’ll need to actually run the library-loading steps of your scripts.\nFor more in-depth records of package versions and environment preservation–in R–you might also consider the renv package or the packrat package.\n\n\n\nScript Organization\nEvery change to the data between the initial raw data and the finished product should be scripted. The ideal would be that you could hand someone your code and the starting data and have them be able to perfectly retrace your steps. This is not possible if you make unscripted modifications to the data at any point!\nYou may wish to break your scripted workflow into separate, modular files for ease of maintenance and/or revision. This is a good practice so long as each file fits clearly into a logical/thematic group (e.g., data cleaning versus analysis).\n\n\nFile Paths\nWhen importing inputs or exporting outputs we need to specify “file paths”. These are the set of folders between where your project is ‘looking’ and where the input/output should come from/go. The figure from Trisovic et al. (2022) shows that file path and working directory errors are a substantial barrier to code that can be re-run in clean coding environments. Consider the following ways of specifying file paths from least to most reproducible.\n\nWorstBadBetterBest!\n\n\n\nAbsolute Paths\nThe worst way of specifying a file path is to use the “absolute” file path. This is the path from the root of your computer to a given file. There are many issues here but the primary one is that absolute paths only work for one computer! Given that only one person can even run lines of code that use absolute paths, it’s not really worth specifying the other issues.\n\n\nExample\n# Read in bee community data\nmy_df &lt;- read.csv(file = \"~/Users/lyon/Documents/Grad School/Thesis (Chapter 1)/Data/bees.csv\")\n\n\n\n\nManually Setting the Working Directory\nMarginally better than using the absolute path is to set the working directory to some location. This may look neater than the absolute path option but it actually has the same point of failure: Both methods only work for one computer!\n\n\nExample\n# Set working directory\nsetwd(dir = \"~/Users/lyon/Documents/Grad School/Thesis (Chapter 1)\")\n\n# Read in bee community data\nmy_df &lt;- read.csv(file = \"Data/bees.csv\")\n\n\n\n\nRelative Paths\nInstead of using absolute paths or manually setting the working directory you can use “relative” file paths! Relative paths assume all project content lives in the same folder.\nThis is a safe assumption because it is the most fundamental tenet of reproducible project organization! The strength of relative paths is actually a serious contributing factor for why it is good practice to use a single folder.\n\n\nExample\n# Read in bee community data\n1my_df &lt;- read.csv(file = \"Data/bees.csv\")\n\n1\n\nParts of file path specific to each user are automatically recognized by the computer\n\n\n\n\n\n\nOperating System-Flexible Relative Paths\nThe “better” example is nice but has a serious limitation: it hard coded the type of slash between file path elements. This means that only computers of the same operating system as the code author could run that line.\nWe can use functions to automatically detect and insert the correct slashes though!\n\n\nExample\n# Read in bee community data\nmy_df &lt;- read.csv(file = file.path(\"Data\", \"bees.csv\"))\n\n\n\n\n\n\nCode Style\nWhen it comes to code style, the same ‘rule of thumb’ applies here that applied to project organization: virtually any system will work so long as you (and your team) are consistent! That said, there are a few principles worth adopting if you have not already done so.\n\nUse concise and descriptive object names\n\nIt can be difficult to balance these two imperatives but short object names are easier to re-type and visually track through a script. Descriptive object names on the other hand are useful because they help orient people reading the script to what the object contains.\n\nDon’t be afraid of empty space!\n\nScripts are free to write regardless of the number of lines so do not feel as though there is a strict character limit you need to keep in mind. Cramped code is difficult to read and thus can be challenging to share with others or debug on your own. Inserting an empty line between coding lines can help break up sections of code and putting spaces before and after operators can make reading single lines much simpler.\n\n\n\nCode Comments\nA “comment” in a script is a human readable, non-coding line that helps give context for the code. In R (and Python), comment lines start with a hashtag (#). Including comments is a low effort way of both (A) creating internal documentation for the script and (B) increasing the reproducibility of the script. It is difficult to include “too many” comments, so when in doubt: add more comments!\nThere are two major strategies for comments and either or both might make sense for your project.\n\n“What” Comments\nComments describe what the code is doing.\n\nBenefits: allows team members to understand workflow without code literacy\nRisks: rationale for code not explicit\n\n# Remove all pine trees from dataset\nno_pine_df &lt;- dplyr::filter(full_df, genus != \"Pinus\")\n\n\n“Why” Comments\nComments describe rationale and/or context for code.\n\nBenefits: built-in documentation for team decisions\nRisks: assumes everyone can read code\n\n# Cone-bearing plants are not comparable with other plants in dataset\nno_pine_df &lt;- dplyr::filter(full_df, genus != \"Pinus\")\n\n\n\n\n\n\nDiscussion: Comment on Comments\n\n\n\nWith a partner discuss the following questions:\n\nWhen you write comments, do you focus more on the “what” or the “why”?\nWhat would you estimate is the ratio of code to comment lines in your code?\n\n1:1 being every code line has one comment line\n\nIf you have revisited old code, were your comments helpful?\n\nHow could you make them more helpful?\n\nIn what ways do you think you would need to change your commenting style for a team project?\n\n\n\n\n\n\n\n\n\nActivity: Make Comments\n\n\n\nRevisit a script from an old project (ideally one you haven’t worked on recently). Once you’ve opened the script:\n\nScan through the script\n\nCan you identify the main purpose(s) of the code?\n\nIdentify any areas where you’re not sure either (A) what the code is doing or (B) why that section of code exists\n\nAdd comments to these areas to document what they’re up to\n\nShare the commented version of one of these trouble areas with a partner\n\nDo they understand the what and/or why of your code?\nIf not, revise the comments and repeat\n\n\n\n\n\n\n\nConsider Custom Functions\nIn most cases, duplicating code is not good practice. Such duplication risks introducing a typo in one copy but not the others. Additionally, if a decision is made later on that requires updating this section of code, you must remember to update each copy separately.\nInstead of taking this copy/paste approach, you could consider writing a “custom” function that fits your purposes. All instances where you would have copied the code now invoke this same function. Any error is easily tracked to the single copy of the function and changes to that step of the workflow can be accomplished in a centralized location.\n\nFunction Recommendations\nWe have the following ‘rules of thumb’ for custom function use:\n- If a given operation is duplicated 3 or more times within a project, write a custom function\nFunctions written in this case can be extremely specific and–though documentation is always a good idea–can be a little lighter on documentation. Note that the reason you can reduce the emphasis on documentation is only because of the assumption that you won’t be sharing the function widely. If you do decide the function could be widely valuable you would need to add the needed documentation post hoc.\n- Write functions defensively\nWhen you write custom functions, it is really valuable to take the time to write them defensively. In this context, “defensively” means that you anticipate likely errors and write your own informative/human readable error messages. Let’s consider a simplified version of a function from the ltertools R package for calculating the coefficient of variation (CV).\nThe coefficient of variation is equal to the standard deviation divided by the mean. Fortunately, R provides functions for calculating both of these already and both expect numeric vectors. If either of those functions is given a non-number you get the following warning message: “In mean.default(x =”…“) : argument is not numeric or logical: returning NA”.\nSomeone with experience in R may be able to interpret this error but for many users this error message is completely opaque. In the function included below however we can see that there is a simpler, more human readable version of the error message and the function is stopped before it can ever reach the part of the code that would throw the warning message included above.\ncv &lt;- function(x){\n  \n  # Error out if x is not numeric\n  if(is.numeric(x) != TRUE)\n    stop(\"`x` must be numeric\")\n  \n  # Calculate CV\n  sd(x = x) / mean(x = x)\nThe key to defensive programming is to try to get functions to fail fast and fail informatively as soon as a problem is detected! This is easier to debug and understand for coders with a range of coding expertise and–for complex functions–can save a ton of useless processing time when failure is guaranteed at a later step.\n- If a given operation is duplicated 3 or more times across projects, consider creating an R package\nCreating an R package can definitely seem like a daunting task but duplication across projects carries the same weaknesses of excessive duplication within a project. However, when duplication is across projects, not even writing a custom function saves you because you need to duplicate that function’s script for each project that needs the tool.\nHadley Wickham and Jenny Bryan have written a free digital book on this subject that demystifies a lot of this process and may make you feel more confident to create your own R package if/when one is needed.\nIf you do take this path, you can simply install your package as you would any other in order to have access to the operations rather than creating duplicates by hand.",
    "crumbs": [
      "Phase I -- Prepare",
      "Reproducibility"
    ]
  },
  {
    "objectID": "mod_reproducibility.html#fair-care-data-principles",
    "href": "mod_reproducibility.html#fair-care-data-principles",
    "title": "Reproducibility Best Practices",
    "section": "FAIR & CARE Data Principles",
    "text": "FAIR & CARE Data Principles\nData availability, data size, and demand for transparency by government and funding agencies are all steadily increasing. While ensuring that your project and code practices are reproducible is important, it is also important to consider how open and reproducible your data are as well. Synthesis projects are in a unique position here because synthesis projects use data that may have been previously published on and/or be added to a public data repository by the original data collectors. However, synthesis projects take data from these different sources and wrangle it such that the different data sources are comparable to one another. These ‘synthesis data products’ can be valuable to consider archiving in a public repository to save other researchers from needing to re-run your entire wrangling workflow in order to get the data product. In either primary or synthesis research contexts there are several valuable frameworks to consider as data structure and metadata are being decided. Among these are the FAIR and CARE data principles.\n\nFAIR\nFAIR is an acronym for Findable Accessible Interpoerable and Reusable. Each element of the FAIR principles can be broken into a set of concrete actions that you can take throughout the lifecycle of your project to ensure that your data are open and transparent. Perhaps most importantly, FAIR data are most easily used by other research teams in the future so the future impact of your work is–in some ways–dependent upon how thoroughly you consider these actions.\nConsider the following list of actions you might take to make your data FAIR. Note that not all actions may be appropriate for all types of data (see our discussion of the CARE principles below), but these guidelines are still important to consider–even if you ultimately choose to reject some of them. Virtually all of the guidelines considered below apply to metadata (i.e., the formal documentation describing your data) and the ‘actual’ data but for ease of reference we will call both of these resources “data.”\n\nFindability\n\nEnsure that data have a globally unique and persistent identifier\nCompletely fill out all metadata details\nRegister/index data in a searchable resource\n\nAccessibility\n\nStore data in a file format that is open, free, and universally implementable (e.g., CSV rather than MS Excel, etc.)\nEnsure that metadata will be available even if the data they describe are not\n\nInteroperability\n\nUse formal, shared, and broadly applicable language for knowledge representation\n\nThis can mean using full species names rather than codes or shorthand that may not be widely known\n\nUse vocabularies that are broadly used and that themselves follow FAIR principles\nInclude explicit references to other FAIR data\n\nReusability\n\nDescribe your data with rich detail that covers a plurality of relevant attributes\nAttach a clear data usage license so that secondary data users know how they are allowed to use your data\nInclude detailed provenance information about your data\nEnsure that your data meet discipline-specific community standards\n\n\n\n\n\n\n\nDiscussion: Consider Data FAIRness\n\n\n\nConsider the first data chapter of your thesis or dissertation. On a scale of 1-5, how FAIR do you think your data and metadata are? What actions could you take to make your data more FAIR compliant? If it helps, feel free to rate your (meta)data based on each FAIR criterion separately!\nFeel free to use these questions to guide your thinking\n\nHow are the data for that project stored?\nWhat metadata exists to document those data?\nHow easy would it be for someone in your lab group to pick up and use your data?\nHow easy would it be for someone not in your lab group?\n\n\n\n\n\nCARE\nWhile making data and code more FAIR is often a good ideal the philosophy behind those four criteria come from a perspective that emphasizes data sharing as a good in and of itself. This approach can ignore historical context and contemporary power differentials and thus be insufficient as the sole tool to use in evaluating how data/code are shared and stored. The Global Indigenous Data Alliance (GIDA) created the CARE principles with these ethical considerations explicitly built into their tenets. Before making your data widely available and transparent (ideally before even beginning your research), it is crucial to consider this ethical dimension.\n\nCARE stands for Collective Benefit, Authority to Control, Responsibility, and Ethics. Ensuring that your data meet these criteria helps to advance Indigenous data sovereignty and respects those who have been–and continue to be–collecting knowledge about the world around us for millennia. The following actions are adapted from Jennings et al. 2023 (linked at the bottom of this page).\nCollective Benefit\n\nDemonstrate how your research and its potential results are relevant and of value to the interests of the community at large and its individual members\nInclude and value local community experts in the research team\nUse classifications and categories in ways defined by Indigenous communities and individuals\nDisaggregate large geographic scale data to increase relevance for place-specific Indigenous priorities\nCompensate community experts throughout the research process (proposal development through to community review of pre-publication manuscripts)\n\nAuthority to Control\n\nEstablish institutional principles or protocols that explicitly recognize Indigenous Peoples’ rights to and interests in their knowledges/data\nEnsure data collection and distribution are consistent with individual and community consent provisions and that consent is ongoing (including the right to withdraw or refuse)\nEnsure Indigenous communities have access to the (meta)data in usable format\n\nResponsibility\n\nCreate and expand opportunities for community capacity\nRecord the Traditional Knowledge and biocultural labels of the Local Contexts Hub in metadata\nEnsure review of draft publications before publication\nUse the languages of Indigenous Peoples in the (meta)data\n\nEthics\n\nAccess research using Indigenous ethical frameworks\nUse community-defined review processes with appropriate reviewers for activities delineated in data management plans\nWork to maximize benefits from the perspectives of Indigenous Peoples by clear and transparent dialogue with communities and individuals\nEngage with community guidelines for the use and reuse of data (including facilitating data removal and/or disposal requests from aggregated datasets)",
    "crumbs": [
      "Phase I -- Prepare",
      "Reproducibility"
    ]
  },
  {
    "objectID": "mod_reproducibility.html#reproducibility-best-practices-summary",
    "href": "mod_reproducibility.html#reproducibility-best-practices-summary",
    "title": "Reproducibility Best Practices",
    "section": "Reproducibility Best Practices Summary",
    "text": "Reproducibility Best Practices Summary\nMaking sure that your project is reproducible requires a handful of steps before you begin, some actions during the life of the project, and then a few finishing touches when the project nears its conclusion. The following diagram may prove helpful as a coarse roadmap for how these steps might be followed in a general project setting.",
    "crumbs": [
      "Phase I -- Prepare",
      "Reproducibility"
    ]
  },
  {
    "objectID": "mod_reproducibility.html#additional-resources",
    "href": "mod_reproducibility.html#additional-resources",
    "title": "Reproducibility Best Practices",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\nA Large-Scale Study on Research Code Quality and Execution. Trisovic et al., 2022. Scientific Data\nApplying the ‘CARE Principles for Indigenous Data Governance’ to Ecology and Biodiversity Research. Jennings et al., 2023. Nature Ecology & Evolution\nGuides to Better Science - Reproducible Code. The British Ecological Society, 2024.\nFAIR Teaching Handbook. Englehardt et al., 2024.\nR Packages (2nd ed.). Wickham & Bryan.\n\n\n\nWorkshops & Courses\n\nData Analysis and Visualization in R for Ecologists, Episode 1: Before We Start. The Carpentries\nIntroduction to R for Geospatial Data, Episode 2: Project Management with RStudio. The Carpentries\ncoreR Course, Chapter 5: FAIR and CARE Principles. National Center for Ecological Analysis and Synthesis (NCEAS) Learning Hub, 2023.\ncoreR Course, Chapter 18: Reproducibility & Provenance. NCEAS Learning Hub, 2023.\n\n\n\nWebsites\n\nCoding Tips. NCEAS Scientific Computing Team, 2024.\nDocumenting Things: Openly for Future Us. Lowndes et al., 2023.\nTeam Coding: 5 Essentials. NCEAS Scientific Computing Team, 2024.\nAdvanced R (1st ed.) Style Guide. Wickham\nPEP 8 Style Guide for Python Code. van Rossum et al. 2013.\nGoogle Style Guides",
    "crumbs": [
      "Phase I -- Prepare",
      "Reproducibility"
    ]
  },
  {
    "objectID": "mod_facilitation.html",
    "href": "mod_facilitation.html",
    "title": "Inclusive Facilitation",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase II -- Plan",
      "Inclusive Facilitation"
    ]
  },
  {
    "objectID": "mod_facilitation.html#overview",
    "href": "mod_facilitation.html#overview",
    "title": "Inclusive Facilitation",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase II -- Plan",
      "Inclusive Facilitation"
    ]
  },
  {
    "objectID": "mod_facilitation.html#learning-objectives",
    "href": "mod_facilitation.html#learning-objectives",
    "title": "Inclusive Facilitation",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nDescribe benefits of encouraging full, thoughtful, engaged participation\nIdentify methods for ensuring equitable access to participation in a team setting\nIdentify one activity that privileges each thinking style",
    "crumbs": [
      "Phase II -- Plan",
      "Inclusive Facilitation"
    ]
  },
  {
    "objectID": "mod_facilitation.html#module-content",
    "href": "mod_facilitation.html#module-content",
    "title": "Inclusive Facilitation",
    "section": "Module Content",
    "text": "Module Content",
    "crumbs": [
      "Phase II -- Plan",
      "Inclusive Facilitation"
    ]
  },
  {
    "objectID": "mod_facilitation.html#additional-resources",
    "href": "mod_facilitation.html#additional-resources",
    "title": "Inclusive Facilitation",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\nFacilitator’s Guide to Participatory Decision-Making by Sam Kaner\n\n\n\nWorkshops & Courses\n\n\n\n\n\nWebsites\n\nLiberating Structures website",
    "crumbs": [
      "Phase II -- Plan",
      "Inclusive Facilitation"
    ]
  },
  {
    "objectID": "mod_version-control.html",
    "href": "mod_version-control.html",
    "title": "Version Control",
    "section": "",
    "text": "Synthesis projects require an intensely collaborative team spirit complemented by an emphasis on quantitative rigor. While other modules focus on elements of each of these, version control is a useful tool that lives at their intersection. We will shortly embark on a deep dive into version control but broadly it is a method of tracking changes to files (especially code “scripts”) that lends itself remarkably well to coding in groups. The National Center for Ecological Analysis and Synthesis (NCEAS) has a Scientific Computing Team that developed a robust workshop-style set of materials aimed at teaching version control to LTER-funded synthesis working groups. Rather than reinvent these materials for the purposes of SSECR, we will work through parts of these workshop materials.",
    "crumbs": [
      "Phase I -- Prepare",
      "Version Control"
    ]
  },
  {
    "objectID": "mod_version-control.html#overview",
    "href": "mod_version-control.html#overview",
    "title": "Version Control",
    "section": "",
    "text": "Synthesis projects require an intensely collaborative team spirit complemented by an emphasis on quantitative rigor. While other modules focus on elements of each of these, version control is a useful tool that lives at their intersection. We will shortly embark on a deep dive into version control but broadly it is a method of tracking changes to files (especially code “scripts”) that lends itself remarkably well to coding in groups. The National Center for Ecological Analysis and Synthesis (NCEAS) has a Scientific Computing Team that developed a robust workshop-style set of materials aimed at teaching version control to LTER-funded synthesis working groups. Rather than reinvent these materials for the purposes of SSECR, we will work through parts of these workshop materials.",
    "crumbs": [
      "Phase I -- Prepare",
      "Version Control"
    ]
  },
  {
    "objectID": "mod_version-control.html#learning-objectives",
    "href": "mod_version-control.html#learning-objectives",
    "title": "Version Control",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nDescribe how version control facilitates code collaboration\nNavigate GitHub via a web browser\nCreate and edit a repository through GitHub\nDefine fundamental git vocabulary\nSketch the RStudio-to-GitHub order of operations\nUse RStudio, Git, and GitHub to collaborate with version control",
    "crumbs": [
      "Phase I -- Prepare",
      "Version Control"
    ]
  },
  {
    "objectID": "mod_version-control.html#nceas-scicomp-workshop-materials",
    "href": "mod_version-control.html#nceas-scicomp-workshop-materials",
    "title": "Version Control",
    "section": "NCEAS SciComp Workshop Materials",
    "text": "NCEAS SciComp Workshop Materials\nThe workshop materials we will be working through live here but for convenience we have also embedded the workshop directly into the SSECR course website (see below).",
    "crumbs": [
      "Phase I -- Prepare",
      "Version Control"
    ]
  },
  {
    "objectID": "mod_version-control.html#collaborating-with-git",
    "href": "mod_version-control.html#collaborating-with-git",
    "title": "Version Control",
    "section": "Collaborating with Git",
    "text": "Collaborating with Git\nIt is important to remember that while Git is a phenomenal tool for collaboration, it is not Google Docs! You can work together but you cannot work simultaneously in the same files. Working at the same time is how merge conflicts happen which can be a huge pain to untangle after the fact. Fortunately, avoiding merge conflicts is relatively simple! Here are a few strategies for avoiding conflicts.\n\nSeparate ScriptsWork in ShiftsWork in ForksSingle Code Author\n\n\nAt it’s simplest, you can make a separate script for each group member and have each of you work exclusively in your own script. If no one ever works in your script you will never have a merge conflict even if you are working in your script at the same time as someone else is working in theirs.\nYou can do this by all working on separate scripts that are trying to do the same thing or you can delegate a particular script in the workflow to a single person (e.g., one person is the only one allowed to edit the ‘data wrangling’ script, another is the only one allowed to edit the ‘analysis’ script, etc.)\nRecommendation: Worth Discussing!\n\n\nYou might also decide to work together on the same scripts and just stagger the time that you are doing stuff so that all of your changes are made, committed, and pushed before the next person begins work. This is a particularly nice option if you have people in different time zones because someone in Maine can work on code likely before another team member living in Oregon has even woken up much less started working on code.\nFor this to work you will need to communicate extensively with the rest of your team so that you are absolutely sure that you won’t start working before someone else has finished their edits.\nRecommendation: Worth Discussing!\n\n\nGitHub does offer a “fork” feature where people can make a copy of a given repository that they then ‘own’. Forks are connected to the source repository and you can open a pull request to get the edits from one fork into the source repository.\nThis may sound like a perfect fit for collaboration but in reality it introduces significant hurdles! Consider the following:\n\nIt is difficult to know where the “best” version of the code lives\n\nIt is equally likely for the primary code version to be in any group member’s fork (or the original fork). So if you want to re-run a set of analyses you’ll need to hunt down which fork the current script lives in rather than consulting a single repository in which you all work together.\n\nYou essentially gaurantee significant merge conflicts\n\nIf everyone is working independently and submitting pull requests to merge back into the main repository you all but ensure that people will make different edits that GitHub then doesn’t know how to resolve. The pull request will tell you that there are merge conflicts but you still need to fix them yourself–and now that fixing effort must be done in someone else’s fork of the repository.\n\nIt’s not the intended use of GitHub forks\n\nForks are intended for when you want to take a set of code and then “go your own way” with that code base. While there is a mechanism for contributing those edits back to the main repository it’s really better used when you never intend to do a pull request and thus don’t have to worry about eventual merge conflicts. A good example here is you might attend a workshop and decide to offer a similar workshop yourself. You could then fork the original workshop’s repository to serve as a starting point for your version and save yourself from unnecessary labor. It would be bizarre for you to suggest that your workshop should replace the original one even if did begin with that content.\nRecommendation: Don’t Do This\n\n\nYou may be tempted to just delegate all code editing to a single person in the group. While this strategy does guarantee that there will never be a merge conflict it is also deeply inequitable as it places an unfair share of the labor of the project on one person.\nPractically-speaking this also encourages an atmosphere where only one person can even read your group’s code. This makes it difficult for other group members to contribute and ultimately may cause your group to ‘miss out on’ novel insights.\nRecommendation: Don’t Do This",
    "crumbs": [
      "Phase I -- Prepare",
      "Version Control"
    ]
  },
  {
    "objectID": "mod_version-control.html#additional-resources",
    "href": "mod_version-control.html#additional-resources",
    "title": "Version Control",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\nPereira Braga et al., Not Just for Programmers: How GitHub can Accelerate Collaborative and Reproducible Research in Ecology and Evolution. 2023. Methods in Ecology and Evolution\nGitHub, Git Cheat Sheet. 2023.\n\n\n\nWorkshops & Courses\n\nHappy Git and GitHub for the useR. Bryan et al., 2024.\ncoreR Course, Chapter 6: Intro to Git and GitHub. NCEAS Learning Hub, 2023.",
    "crumbs": [
      "Phase I -- Prepare",
      "Version Control"
    ]
  },
  {
    "objectID": "mod_stats.html",
    "href": "mod_stats.html",
    "title": "Analysis & Modeling",
    "section": "",
    "text": "Given the wide range in statistical training in graduate curricula (and corresponding breadth of experience among early career researchers), we’ll be approaching this module by splitting it into two halves.\n\nFirst half: a “flipped approach” where project teams will share their proposed analyses with one another\nSecond half: typical instructional module dedicated to analyses that are more common in–or exclusive to–synthesis research.\n\nContent produced by project teams during the flipped half may be linked in the ‘Additional Resources’ section at the bottom of this module at the discretion of each team. Otherwise the content of this module will focus only on the non-flipped content.",
    "crumbs": [
      "Phase III -- Execute",
      "Analysis & Modeling"
    ]
  },
  {
    "objectID": "mod_stats.html#overview",
    "href": "mod_stats.html#overview",
    "title": "Analysis & Modeling",
    "section": "",
    "text": "Given the wide range in statistical training in graduate curricula (and corresponding breadth of experience among early career researchers), we’ll be approaching this module by splitting it into two halves.\n\nFirst half: a “flipped approach” where project teams will share their proposed analyses with one another\nSecond half: typical instructional module dedicated to analyses that are more common in–or exclusive to–synthesis research.\n\nContent produced by project teams during the flipped half may be linked in the ‘Additional Resources’ section at the bottom of this module at the discretion of each team. Otherwise the content of this module will focus only on the non-flipped content.",
    "crumbs": [
      "Phase III -- Execute",
      "Analysis & Modeling"
    ]
  },
  {
    "objectID": "mod_stats.html#learning-objectives",
    "href": "mod_stats.html#learning-objectives",
    "title": "Analysis & Modeling",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nDescribe proposed analytical methods to an interested audience of mixed prior experience\nExplain nuance in interpretation of results of proposed analyses\nIdentify some statistical tests common in synthesis research\nPerform some synthesis-specific analyses",
    "crumbs": [
      "Phase III -- Execute",
      "Analysis & Modeling"
    ]
  },
  {
    "objectID": "mod_stats.html#needed-packages",
    "href": "mod_stats.html#needed-packages",
    "title": "Analysis & Modeling",
    "section": "Needed Packages",
    "text": "Needed Packages\nIf you’d like to follow along with the code chunks included throughout this module, you’ll need to install the following packages:\n\n# Note that these lines only need to be run once per computer\n## So you can skip this step if you've installed these before\ninstall.packages(\"tidyverse\")\ninstall.packages(\"lmerTest\")\ninstall.packages(\"palmerpenguins\")\n\nWe’ll go ahead and load some of these libraries as well to be able to better demonstrate these concepts.\n\n# Load needed libraries\nlibrary(tidyverse)\nlibrary(lmerTest)\nlibrary(palmerpenguins)",
    "crumbs": [
      "Phase III -- Execute",
      "Analysis & Modeling"
    ]
  },
  {
    "objectID": "mod_stats.html#hypothesis-framing-aside",
    "href": "mod_stats.html#hypothesis-framing-aside",
    "title": "Analysis & Modeling",
    "section": "Hypothesis Framing Aside",
    "text": "Hypothesis Framing Aside\nBefore we dive in, we should discuss two of the ways in which you can frame your hypothesis and the differences in interpretation and appropriate statistical tool(s) that follow from that choice. We’ll restrict our conversation here to two alternate modes of thinking about your hypothesis: frequentist statistics versus multi-model inference.\nNote that this is something of a false dichotomy as tools from both worlds can be/are frequently used to complement one another. However, many graduate students are trained by instructors with strong feelings about one method in opposition to the other so it is worthwhile to consider these two paths separately even if you wind up using components of both in your own work.\n\nFrequentist InferenceMulti-Model Inference\n\n\nHypotheses here are a question of whether a variable has a “significant” effect on another. “Significant” has a very precise meaning in this context that has to do with p-values. Fundamentally, these methods focus on whether the observed relationship in the data is likely to be observed by chance alone or not. Strong effects are less likely–though not impossible–to be observed due to random chance.\nIf your hypothesis can be summarized as something along the lines of ‘we hypothesize that X affects Y’ then frequentist inference may be a more appropriate methodology.\nFor the purposes of SSECR, our discussion of frequentist inference will focus on mixed-effect models.\n\n\nHyoptheses here are a question of which variables explain the most variation in the data. Methods in this framing are unconcerned–or at least less concerned than in frequentist inference–with the probability associated with a particular variable. Intead, these methods focus on which of a set of user-defined candidate models explains most of the noise in the data even when that best model does not necessarily explain much of that variation in absolute terms.\nIf your hypothesis can be summarized as something along the lines of ‘we hypothesize that models including X explain more of the variation in Y than those that do not’ then multi-model inference may be a more appropriate methodology.\nFor the purposes of SSECR, our discussion of multi-model inference will focus on comparing model strengths with AIC.",
    "crumbs": [
      "Phase III -- Execute",
      "Analysis & Modeling"
    ]
  },
  {
    "objectID": "mod_stats.html#mixed-effects-models",
    "href": "mod_stats.html#mixed-effects-models",
    "title": "Analysis & Modeling",
    "section": "Mixed-Effects Models",
    "text": "Mixed-Effects Models\nIn any statistical test there is at least one response variable (a.k.a. “dependent” variable) and some number of explanatory variables (a.k.a. “independent” variables). However, in biology our experiments often involve repeated sampling over time or at the same locations. These variables (time or site) are neither response nor explanatory variables but we might reasonably conclude that they affect our response and/or explanatory variables.\nIn essence we want to use a statistical tool that asks ‘what is the effect of the explanatory variable(s) on the response when the variation due to these non-variable considerations is accounted for?’ Such tests are called mixed-effects models. This name derives from considering explanatory variables “fixed effects” and non-explanatory/response variables as “random effects”. Including both fixed and random effects thus creates a model with “mixed effects.”\n\nTypes of Random Effect\nThere are a few types of random effects but we can limit our conversation here to just two: random intercepts and random slopes.\n\nRandom InterceptsRandom Slopes\n\n\nRandom intercepts should be used when you expect that the average response differs among levels of that variable but not in a way that changes the relationship between each level of this variable and the other variables (either fixed or random). In statistical terms you want to allow the intercept to change with levels of this variable.\nFor example, let’s imagine that we are studying the effect of different organic farming practices on beneficial insect populations. We build relationships with several organic farmers willing to let us conduct this research on their properties and sample the insect communities at each farm over the course of a summer. However, we know that each farm is surrounded by a different habitat type that affects the composition of the local insect community. It is reasonable to expect that even farms where ‘the same’ management method is used are likely to differ because of this difference in landscape context.\nIn cases like this, we don’t want to include a term for ‘site’ as a fixed effect but we do want to account for those differences so that our assessment of the significance of our explanatory variables isn’t limited by the variation due to site.\n\n\nRandom slopes should be used when you expect that the average response differs among levels of that variable in a way that does change with other variables.\nFor example, let’s imagine that we are studying the effect of temperature on avian malaria rates in songbirds. We identify several sites–along a gradient of daytime temperature ranges–where our species of interest can be found, capture them, and measure malaria infection rates. However, unbeknownst to us at the start of our study, our study sites have varying populations of dragonflies which affects local mosquito populations and malaria transmission/infection rates. If we revisit our sites repeatedly for several years is is reasonable to expect that this difference among sites likely affects the relationship between daytime temperatures and songbird malaria rates.\nBy including site as a random slope in this context, we can account for this effect and still analyze our explanatory variables of interest. Note that random slopes are very “data hungry” so you may not be able to use them without very high replication in your study design.\n\n\n\n\n\nNested Random Effects\nTo further complicate matters, we can use nested random effects as well. These can be either random intercepts or random slopes though they are more commonly seen with random intercepts. A nested random effect accounts for the effect of one random variable that is itself affected by another variable! A classic example of this is when a study design uses two (or more) levels of spatial nestedness in their experimentall design.\nFor instance, let’s imagine we were conducting a global study of marine plankton biodiversity. To gether these data we took several cruises (scientific not–exclusively–pleasure) at different places around the world and during each cruise we followed a set of transects. In each transect we did several plankton tows and quantified the diversity of each tow. We can reasonably assume the following:\n\nEach cruise differs from each other cruise (due to any number of climatic/ecological factors)\n\nBut cruises within the same part of the world are still likely to have similar planktonic communities\n\nWithin each cruise, each transect differs from the others (again, due to unpreventable factors)\n\nBut transects within the same cruise are still likely to be more similar to one another than to transects in different cruises (even other ones in the same region!)\n\nWithin each transect, each plankton tow differs from one another!\n\nBut again, more similar to other tows in the same transect than other tows in different transects/cruises\n\n\nIf we put these assumptions together we realize we want to account for the variation of cruise, transect, and tow while still retaining the nestedness of the similarity among samples. A nested random effect where transect is nested inside of cruise and tow is nested inside of transect would capture this effectively!\n\n\nPhilosophical Note: Random vs. Fixed\nDeciding whether a given variable should be a fixed or random effect can be tough. You’ll likely need to rely on your scientific intuition about which feels more appropriate and then be prepared to defend that decision to your committee and/or “reviewer #2”. It may prove helpful though to consider whether you ‘care’ about the effect of that variable.\nIf your hypothesis includes that variable than it should likely be a fixed effect. If the variable is just a facet of your experimental design but isn’t something you’re necessarily interested in testing, then it should likely be a random effect. And, once you’ve made your decision, it is totally okay to change your mind and tweak the structure of your model!\n\n\n\n\n\n\nDiscussion: Random versus Fixed Effects\n\n\n\nWith a small group, decide whether you think the terms in the examples below should be fixed effects or random effects:\n\nYou study small mammal populations in urban settings\n\nShould ‘proportion green space’ be a fixed effect or a random effect?\n\nYou are part of a team studying leopard seal feeding behavior\n\nWhat type of effect should ‘observer’ be?\n\nYou study the gut microbiota of a particular beetle species\n\nShould ‘beetle sex’ be a fixed or a random effect?\nWhat about beetle life stage (e.g., larva versus adult)?\nWhat about the region of the gut from which the samples were taken?\n\nYou study vascular plant chemical defenses against herbivory\n\nShould phylogeny (i.e., evolutionary relatedness) be a fixed or random effect?\nWhat about feeding guild of the herbivore?\n\n\n\n\n\n\nMixed-Effects Case Study\nLet’s imagine we are researching tarantula populations for several years in the Chihuahuan Desert. Our hypothesis is that the number of tarantulas will be greater in sites further from the nearest road. We select ten study sites of varying distances from the nearest road and intensively count our furry friends at three plots within each site for several months. We return to our sites–and their associated plots–and repeat this process each year for three years. In the second year we have help from a new member of our lab but in the third year we’re back to working alone (they had their own project to handle by then). We enter our data and perform careful quality control to get it into a tidy format ready for analyis.\n\n# Read in data\ntarantula_df &lt;- read.csv(file = file.path(\"data\", \"tarantulas.csv\"))\n\n# Check structure\nstr(tarantula_df)\n\n'data.frame':   450 obs. of  6 variables:\n $ year           : int  2022 2022 2022 2022 2022 2022 2022 2022 2022 2022 ...\n $ road.dist_km   : num  26.9 26.9 26.9 26.9 26.9 ...\n $ site           : chr  \"site_A\" \"site_A\" \"site_A\" \"site_A\" ...\n $ plot           : chr  \"plot_a\" \"plot_a\" \"plot_a\" \"plot_a\" ...\n $ site.plot      : chr  \"A_a\" \"A_a\" \"A_a\" \"A_a\" ...\n $ tarantula_count: int  12 18 18 9 18 51 45 60 45 63 ...\n\n\nWith our data in hand, we now want to run some statistical tests and–hopefully–get some endorphine-inducingly small p-values. If we choose to simply ignore our possible random effects, we could fit a linear regression.\n\n# Fit model\n1tarantula_lm &lt;- lm(tarantula_count ~ road.dist_km, data = tarantula_df)\n\n# Extract summary\nsummary(tarantula_lm)\n\n\n1\n\nR syntax for statistical tests is response ~ explanatory a.k.a. Y ~ X\n\n\n\n\n\nCall:\nlm(formula = tarantula_count ~ road.dist_km, data = tarantula_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-180.360  -67.508    3.415   60.415  273.640 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    4.3262    13.1676   0.329    0.743    \nroad.dist_km   2.6306     0.2745   9.582   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 90.55 on 448 degrees of freedom\nMultiple R-squared:  0.1701,    Adjusted R-squared:  0.1682 \nF-statistic: 91.82 on 1 and 448 DF,  p-value: &lt; 2.2e-16\n\n\nThis naive test seems to support our hypothesis. However, sampling effort differed between the three study years. Not only was there a second person in the second year but we can also reasonably expect that by the third year in this system we had greatly improved our tarantula-finding skills. So, a random effect of year is definitely justified. We are not concerned that the different study years will affect the relationship between tarantula populations and road distance though so a random intercept is fine.\nThere could be an argument for including year as a fixed effect in its own right but some preliminary investigations reveal no significant climatic differences across the region we worked in those three years. So, while we think that years may differ from one another, that difference is not something we care to analyze.\n\n# Fit the new model\ntarantula_mem1 &lt;- lmerTest::lmer(tarantula_count ~ road.dist_km + \n1                               (1|year),\n                             data = tarantula_df)\n\n# Extract summary\nsummary(tarantula_mem1)\n\n\n1\n\nThis is the syntax for specifying a random intercept (random slope variables should be before the | where 1 goes for a random intercept)\n\n\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: tarantula_count ~ road.dist_km + (1 | year)\n   Data: tarantula_df\n\nREML criterion at convergence: 5272.9\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.5660 -0.8039  0.1522  0.6220  2.7981 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n year     (Intercept) 1548     39.34   \n Residual             7163     84.64   \nNumber of obs: 450, groups:  year, 3\n\nFixed effects:\n             Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)    4.3262    25.8335   3.1485   0.167    0.877    \nroad.dist_km   2.6306     0.2566 446.0000  10.252   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nroad.dst_km -0.451\n\n\nBy including that random effect we actually get a slightly stronger effect of road distance (T value of ~12 without versus ~13 with). This is because our new random effect accounts for some of the ‘noise’ between study years. That actually gives us a better picture of the relationship between our response and explanatory variables.\nNow that we’re already using a mixed-effects model, we have little excuse not to account for the other potential random effect: plot! Remember that there were three plots within each site and from our extensive time in the field we have developed a strong intuition that there might be substantial among-plot variation at each site. We can make a quick exploratory graph to facilitate an ‘eyeball test’ of whether the data show what our intuition suggest.\n\nggplot(tarantula_df, aes(y = tarantula_count, x = plot, fill = plot)) +\n1  geom_violin(alpha = 0.5) +\n  geom_jitter(width = 0.25, size = 0.5) +\n  facet_wrap(site ~ .) +\n  theme_bw() +\n2  theme(axis.text.x = element_text(angle = 35, hjust = 1))\n\n\n1\n\nViolin plots are a nice alternative to boxplots because they allow visualizing data distributions directly rather than requiring an intutive grasp of the distribution metrics described by each bit of a boxplot\n\n2\n\nThis is allowing us to ‘tilt’ the X axis tick labels so they don’t overlap with one another\n\n\n\n\n\n\n\n\n\n\n\nThis graph clearly supports our intuition that among-plot variation is dramatic! We could account for this by including plot as a fixed effect but we’ll need to sacrifice a lot of degrees of freedom (can be thought of as “statistical power”) for a variable that we don’t actually care about. Instead, we could include plot as another random effect.\n\n# Fit the new model\ntarantula_mem2 &lt;- lmerTest::lmer(tarantula_count ~ road.dist_km + \n1                               (1|year) + (1|site.plot),\n                             data = tarantula_df)\n\n# Extract summary\nsummary(tarantula_mem2)\n\n\n1\n\nNote that we need to use this column as the random effect because plots are not uniquely named across sites (i.e., all sites have plots “a”, “b”, and “c”). Making the random effect just the ‘plot’ column would fail to reflect how plots are nested within each site\n\n\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: tarantula_count ~ road.dist_km + (1 | year) + (1 | site.plot)\n   Data: tarantula_df\n\nREML criterion at convergence: 4471.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2684 -0.5698 -0.0689  0.5857  3.3902 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n site.plot (Intercept) 6671.0   81.68   \n year      (Intercept) 1589.6   39.87   \n Residual               881.2   29.69   \nNumber of obs: 450, groups:  site.plot, 30; year, 3\n\nFixed effects:\n             Estimate Std. Error      df t value Pr(&gt;|t|)  \n(Intercept)    4.3262    51.6178 23.3405   0.084   0.9339  \nroad.dist_km   2.6306     0.9633 28.0001   2.731   0.0108 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nroad.dst_km -0.847\n\n\nThis test reveals that while there is a significant relationship between road distance and tarantula population but the effect is not nearly as strong as it was when we let plot-level variation be ignored. This is likely due to high (or low) average populations in a single plot skewing the site-level average. Still, this is a result we can be more confident in because we’ve now accounted for all known sources of variation in our data–either by including them as fixed effects or including them as a random effects.\nWe can create one more graph of our tidy data and use some aesthetic settings to make sure the nested structure of the data is clear to those looking at our work. Note that you could also use predicted values from the model itself though that choice is–arguably–a matter of personal preference.\n\nggplot(tarantula_df, aes(y = tarantula_count, x = road.dist_km)) +\n  geom_point(aes(color = plot, shape = as.factor(year)), size = 2, alpha = 0.5) +\n  geom_smooth(method = \"lm\", formula = \"y ~ x\", se = T, color = \"black\") +\n  labs(y = \"Tarantula Abundance\", x = \"Distance to Nearest Road (km)\") +\n  theme_bw()",
    "crumbs": [
      "Phase III -- Execute",
      "Analysis & Modeling"
    ]
  },
  {
    "objectID": "mod_stats.html#multi-model-inference-1",
    "href": "mod_stats.html#multi-model-inference-1",
    "title": "Analysis & Modeling",
    "section": "Multi-Model Inference",
    "text": "Multi-Model Inference\nRegardless of your choice of statistical test, multi-model inference may be an appropriate method to use to assess your hypothesis. As stated earlier, this frames your research question as a case of which variables best explain the data rather than the likelihood of the observed effect relating to any variable in particular.\nTo begin, it can be helpful to write out all possible “candidate models”. For instance, let’s say that you measured some response variable (Y) and several potential explanatory variables (X, W, and Z). We would then fit the following candidate models:\n\nX alone explains the most variation in Y\nW alone explains the most variation in Y\nZ alone explains the most variation in Y\nX, W, and Z together explain the most variation in Y\n\nWe might also fit other candidate models for pairs of X, W, and Z but for the sake of simplicity in this hypothetical we’ll skip those. Note that for this method to be appropriate you need to fit the same type of model in all cases!\nOnce we’ve fit all of our models and assigned them to objects, we can use the AIC function included in base R to compare the AIC score of each model. “AIC” stands for Akaike (AH-kuh-ee-kay) Information Criterion and is one of several related information criteria for summarizing a model’s explanatory power. Models with more parameters are penalized to make it mathematically possible for a model with fewer explanatory variables to still do a better job capturing the variation in the data.\nThe model with the lowest AIC best explains the data. Technically any difference in AIC indicates model improvement but many scientists use a rule of thumb of a difference of 2. So, if two models have AIC scores that differ by less than 2, you can safely say that they have comparable explanatory power. That is definitely a semi-arbitrary threshold but so is the 0.05 threshold for p-value “significance”.\n\nAIC Case Study\nLet’s check out an example using AIC to compare the strengths of several models. Rather than using simulated data–as we did earlier in the mixed-effect model section–we’ll use some real penguin data included in the palmerpenguins package.\nThis dataset includes annual data on three penguin species spread across several islands. The sex of the penguins was also recorded in addition to the length of their flippers, body mass, and bill length and depth.\nFor the purposes of this example, our research question is as follows: what factors best explain penguin body mass?\n\n# Load the penguins data from the `palmerpenguins` package\ndata(penguins)\n\n# Make a version where no NAs are allowed\n1peng_complete &lt;- penguins[complete.cases(penguins), ]\n\n# Check the structure of it\ndplyr::glimpse(peng_complete)\n\n\n1\n\nThis is a base R way of keeping only rows that have no NA values in any column. It is better to identify and handle NAs more carefully but for this context we just want to have the same number of observations in each model\n\n\n\n\nRows: 333\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 41.1, 38.6…\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 17.6, 21.2…\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, 193, 190, 181, 195, 182, 191, 198, 18…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3200, 3800…\n$ sex               &lt;fct&gt; male, female, female, female, male, female, male, fe…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nWith our data in hand and research question in mind, we can fit several candidate models that our scientific intuition and the published literature support as probable then compare them with AIC.\n\n# Species and sex\nmod_spp &lt;- lm(body_mass_g ~ species + sex, data = peng_complete)\n\n# Island alone\nmod_isl &lt;- lm(body_mass_g ~ island, data = peng_complete)\n\n# Combination of species and island\nmod_eco &lt;- lm(body_mass_g ~ island + species + sex, data = peng_complete)\n\n# Body characteristics alone\nmod_phys &lt;- lm(body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm,\n               data = peng_complete)\n\n# Global model\n1mod_sink &lt;- lm(body_mass_g ~ island + species + sex +\n               flipper_length_mm + bill_length_mm + bill_depth_mm,\n               data = peng_complete)\n\n\n1\n\nWe’ve named the global model “sink” because of the American idiom “everything but the kitchen sink.” It is used in cases where everything that can be included has been\n\n\n\n\nOnce we’ve fit all of these models, we can use the AIC function from base R (technically from the stats package included in base R).\n\n# Compare models\nAIC(mod_spp, mod_isl, mod_eco, mod_phys, mod_sink) %&gt;% \n1  dplyr::arrange(AIC)\n\n\n1\n\nUnfortunately, the AIC function doesn’t sort by AIC score automatically so we’re using the arrange function to make it easier for us to rank models by their AIC scores\n\n\n\n\n         df      AIC\nmod_sink 10 4727.242\nmod_spp   5 4785.594\nmod_eco   7 4789.480\nmod_phys  5 4929.554\nmod_isl   4 5244.224\n\n\nInterestingly, it looks like the best model (i.e., the one that explains most of the data) is the global model that included most of the available variables. As stated earlier, it is not always the case that the model with the most parameters has the lowest AIC so we can be confident this is a “real” result. The difference between that one and the next (incidentally the model where only species and sex are included as explanatory variables) is much larger than 2 so we can be confident that the global model is much better than the next best.\nWith this result your interpretation would be that penguin body mass is better explained by a combination of species, sex, physical characteristics of the individual penguin, and the penguin’s home island than it is by any of the other candidate models. In a publication you’d likely want to report this entire AIC table (either parenthetically or in a table) so that reviewers could evaluate your logic.",
    "crumbs": [
      "Phase III -- Execute",
      "Analysis & Modeling"
    ]
  },
  {
    "objectID": "mod_stats.html#meta-analysis",
    "href": "mod_stats.html#meta-analysis",
    "title": "Analysis & Modeling",
    "section": "Meta-Analysis",
    "text": "Meta-Analysis\nMany synthesis projects are able to find the original data of each study, harmonize that data, and then perform standard analyses on that synthesized data. However, in some cases you may find that the data used in different projects are not directly comparable. For instance, if you want to know what the effect of restoration methods are on forest recovery you might not be able to simply combine data from different studies that use widely different restoration methods, data collection methods, and have different forest community compositions. In such cases you can use meta-analysis to compare the results of different studies rather than using their data. Meta-analysis is named the way it is because it is an analysis of prior analyses.\nTo perform meta-analysis you’ll need to calculate an “effect size” for all studies you’d like to include. An effect size captures the direction and magnitude of the relationship analyzed in each original study. If you use a standard effect size calculation for each stud, you’ll make it possible to directly compare results across these studies (even if context differs among them!). Note that some people disagree with the word “effect” in “effect size” because it suggests a causal relationship; for our purposes, let’s consider ‘effect’ to be inclusive of correlative relationships and ignore the possible implication of causality.\nIn order to calculate these effect sizes you’ll need to extract the following information from each study:\n\nA measure of the ‘central tendency’ of the response\n\nOften the arithmetic mean but can also be a proportion or a correlation\nYou’ll need to do this separately for any groups within the study\n\nA measure of the variation in the response\n\nTypically standard deviation\n\nThe sample size of the response\n\n\nExtracting Study Results\n\n\nCalculating Effect Sizes\n\n\nPerforming Meta-Analysis",
    "crumbs": [
      "Phase III -- Execute",
      "Analysis & Modeling"
    ]
  },
  {
    "objectID": "mod_stats.html#additional-resources",
    "href": "mod_stats.html#additional-resources",
    "title": "Analysis & Modeling",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\nUnderstanding ‘It Depends’ in Ecology: A Guide to Hypothesising, Visualising and Interpreting Statistical Interactions. Spake et al., 2023. Biological Reviews\nImproving Quantitative Synthesis to Achieve Generality in Ecology. Spake et al., 2022.Nature Ecology and Evolution\nDoing Meta-Analysis with R: A Hands-On Guide. Harrier et al. 2023.\nMixed Effects Models and Extensions in Ecology with R. Zuur et al., 2009.\n\n\n\nWorkshops & Courses\n\nMatt Vuorre’s Bayesian Meta-Analysis with R, Stan, and brms\n\n\n\nWebsites",
    "crumbs": [
      "Phase III -- Execute",
      "Analysis & Modeling"
    ]
  },
  {
    "objectID": "mod_thinking.html",
    "href": "mod_thinking.html",
    "title": "Supporting Divergent, Emergent, and Convergent Thinking",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase III -- Execute",
      "Supporting D.E.C. Thinking"
    ]
  },
  {
    "objectID": "mod_thinking.html#overview",
    "href": "mod_thinking.html#overview",
    "title": "Supporting Divergent, Emergent, and Convergent Thinking",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase III -- Execute",
      "Supporting D.E.C. Thinking"
    ]
  },
  {
    "objectID": "mod_thinking.html#learning-objectives",
    "href": "mod_thinking.html#learning-objectives",
    "title": "Supporting Divergent, Emergent, and Convergent Thinking",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nDevelop strategies to support participation, innovative thinking, integration, and convergence / decision making in a team setting",
    "crumbs": [
      "Phase III -- Execute",
      "Supporting D.E.C. Thinking"
    ]
  },
  {
    "objectID": "mod_thinking.html#module-content",
    "href": "mod_thinking.html#module-content",
    "title": "Supporting Divergent, Emergent, and Convergent Thinking",
    "section": "Module Content",
    "text": "Module Content",
    "crumbs": [
      "Phase III -- Execute",
      "Supporting D.E.C. Thinking"
    ]
  },
  {
    "objectID": "mod_thinking.html#additional-resources",
    "href": "mod_thinking.html#additional-resources",
    "title": "Supporting Divergent, Emergent, and Convergent Thinking",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\nFacilitator’s Guide to Participatory Decision-Making by Sam Kaner\n\n\n\nWorkshops & Courses\n\n\n\n\n\nWebsites\n\nLiberating Structures website",
    "crumbs": [
      "Phase III -- Execute",
      "Supporting D.E.C. Thinking"
    ]
  },
  {
    "objectID": "policy_pronouns.html",
    "href": "policy_pronouns.html",
    "title": "Synthesis Skills for Early Career Researchers",
    "section": "",
    "text": "You provided a name when you first applied to be a part of the course but we will gladly honor your request to be addressed by an alternate name. We will also use whichever pronouns you identify with. Please advise us of your pronouns and/or chosen name early in the course so that we can ensure that we treat you respectfully throughout the course."
  },
  {
    "objectID": "mod_data-viz.html",
    "href": "mod_data-viz.html",
    "title": "Data Visualization & Exploration",
    "section": "",
    "text": "Data visualization is a fundamental part of working with data. Visualization can be only used in the final stages of a project to make figures for publication but it can also be hugely valuable for quality control and hypothesis development processes. This module focuses on the fundamentals of graph creation in an effort to empower you to apply those methods in the various contexts where you might find visualization to be helpful.",
    "crumbs": [
      "Phase II -- Plan",
      "Data Visualization"
    ]
  },
  {
    "objectID": "mod_data-viz.html#overview",
    "href": "mod_data-viz.html#overview",
    "title": "Data Visualization & Exploration",
    "section": "",
    "text": "Data visualization is a fundamental part of working with data. Visualization can be only used in the final stages of a project to make figures for publication but it can also be hugely valuable for quality control and hypothesis development processes. This module focuses on the fundamentals of graph creation in an effort to empower you to apply those methods in the various contexts where you might find visualization to be helpful.",
    "crumbs": [
      "Phase II -- Plan",
      "Data Visualization"
    ]
  },
  {
    "objectID": "mod_data-viz.html#learning-objectives",
    "href": "mod_data-viz.html#learning-objectives",
    "title": "Data Visualization & Exploration",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nDefine fundamental ggplot2 vocabulary\nIdentify appropriate graph types for given data type/distribution\nDiscuss differences between presentation- and publication-quality graphs\nExplain how your graphs can be made more accessible",
    "crumbs": [
      "Phase II -- Plan",
      "Data Visualization"
    ]
  },
  {
    "objectID": "mod_data-viz.html#needed-packages",
    "href": "mod_data-viz.html#needed-packages",
    "title": "Data Visualization & Exploration",
    "section": "Needed Packages",
    "text": "Needed Packages\nIf you’d like to follow along with the code chunks included throughout this module, you’ll need to install the following packages:\n\n# Note that these lines only need to be run once per computer\n## So you can skip this step if you've installed these before\ninstall.packages(\"tidyverse\")\ninstall.packages(\"lterdatasampler\")\ninstall.packages(\"supportR\")\ninstall.packages(\"cowplot\")\ninstall.packages(\"vegan\")\ninstall.packages(\"ape\")\n\nWe’ll go ahead and load some of these libraries as well to be able to better demonstrate these concepts.\n\n# Load needed libraries\nlibrary(tidyverse)",
    "crumbs": [
      "Phase II -- Plan",
      "Data Visualization"
    ]
  },
  {
    "objectID": "mod_data-viz.html#graphing-with-ggplot2",
    "href": "mod_data-viz.html#graphing-with-ggplot2",
    "title": "Data Visualization & Exploration",
    "section": "Graphing with ggplot2",
    "text": "Graphing with ggplot2\n\nggplot2 Fundamentals\nYou may already be familiar with the ggplot2 package in R but if you are not, it is a popular graphing library based on The Grammar of Graphics. Every ggplot is composed of four elements:\n\nA ‘core’ ggplot function call\nAesthetics\nGeometries\nTheme\n\nNote that the theme component may be implicit in some graphs because there is a suite of default theme elements that applies unless otherwise specified.\nThis module will use example data to demonstrate these tools but as we work through these topics you should feel free to substitute a dataset of your choosing! If you don’t have one in mind, you can use the example dataset shown in the code chunks throughout this module. This dataset comes from the lterdatasampler R package and the data are about fiddler crabs (Minuca pugnax) at the Plum Island Ecosystems (PIE) LTER site.\n\n# Load the lterdatasampler package\nlibrary(lterdatasampler)\n\n# Load the fiddler crab dataset\ndata(pie_crab)\n\n# Check its structure\nstr(pie_crab)\n\ntibble [392 × 9] (S3: tbl_df/tbl/data.frame)\n $ date         : Date[1:392], format: \"2016-07-24\" \"2016-07-24\" ...\n $ latitude     : num [1:392] 30 30 30 30 30 30 30 30 30 30 ...\n $ site         : chr [1:392] \"GTM\" \"GTM\" \"GTM\" \"GTM\" ...\n $ size         : num [1:392] 12.4 14.2 14.5 12.9 12.4 ...\n $ air_temp     : num [1:392] 21.8 21.8 21.8 21.8 21.8 ...\n $ air_temp_sd  : num [1:392] 6.39 6.39 6.39 6.39 6.39 ...\n $ water_temp   : num [1:392] 24.5 24.5 24.5 24.5 24.5 ...\n $ water_temp_sd: num [1:392] 6.12 6.12 6.12 6.12 6.12 ...\n $ name         : chr [1:392] \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" ...\n\n\nWith a dataset in hand, let’s make a scatterplot of crab size on the Y-axis with latitude on the X. We’ll forgo doing anything to the theme elements at this point to focus on the other three elements.\n\n1ggplot(data = pie_crab, mapping = aes(x = latitude, y = size, fill = site)) +\n2  geom_point(pch = 21, size = 2, alpha = 0.5)\n\n\n1\n\nWe’re defining both the data and the X/Y aesthetics in this top-level bit of the plot. Also, note that each line ends with a plus sign\n\n2\n\nBecause we defined the data and aesthetics in the ggplot() function call above, this geometry can assume those mappings without re-specificying\n\n\n\n\n\n\n\n\n\n\n\nWe can improve on this graph by tweaking theme elements to make it use fewer of the default settings.\n\nggplot(data = pie_crab, mapping = aes(x = latitude, y = size, fill = site)) +\n  geom_point(pch = 21, size = 2, alpha = 0.5) +\n1  theme(legend.title = element_blank(),\n        panel.background = element_blank(),\n        axis.line = element_line(color = \"black\"))\n\n\n1\n\nAll theme elements require these element_... helper functions. element_blank removes theme elements but otherwise you’ll need to use the helper function that corresponds to the type of theme element (e.g., element_text for theme elements affecting graph text)\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultiple Geometries\nWe can further modify ggplot2 graphs by adding multiple geometries if you find it valuable to do so. Note however that geometry order matters! Geometries added later will be “in front of” those added earlier. Also, adding too much data to a plot will begin to make it difficult for others to understand the central take-away of the graph so you may want to be careful about the level of information density in each graph. Let’s add boxplots behind the points to characterize the distribution of points more quantitatively.\n\nggplot(data = pie_crab, mapping = aes(x = latitude, y = size, fill = site)) +\n1  geom_boxplot(pch = 21) +\n  geom_point(pch = 21, size = 2, alpha = 0.5) +\n  theme(legend.title = element_blank(), \n        panel.background = element_blank(),\n        axis.line = element_line(color = \"black\"))\n\n\n1\n\nBy putting the boxplot geometry first we ensure that it doesn’t cover up the points that overlap with the ‘box’ part of each boxplot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nActivity: Graph Creation (P1)\n\n\n\nIn a script, attempt the following with one of either yours or your group’s datasets:\n\nMake a graph using ggplot2\n\nInclude at least one geometry\nInclude at least one aesthetic (beyond X/Y axes)\nModify at least one theme element from the default\n\n\n\n\n\n\nMultiple Data Objects\nggplot2 also supports adding more than one data object to the same graph! While this module doesn’t cover map creation, maps are a common example of a graph with more than one data object. Another common use would be to include both the full dataset and some summarized facet of it in the same plot.\nLet’s calculate some summary statistics of crab size to include that in our plot.\n\n# Load the supportR library\nlibrary(supportR)\n\n\nAttaching package: 'supportR'\n\n\nThe following object is masked from 'package:dplyr':\n\n    count\n\n# Summarize crab size within latitude groups\ncrab_summary &lt;- supportR::summary_table(data = pie_crab, groups = c(\"site\", \"latitude\"),\n                                        response = \"size\", drop_na = TRUE)\n\n# Check the structure\nstr(crab_summary)\n\n'data.frame':   13 obs. of  6 variables:\n $ site       : chr  \"BC\" \"CC\" \"CT\" \"DB\" ...\n $ latitude   : num  42.2 41.9 41.3 39.1 30 39.6 41.6 33.3 42.7 34.7 ...\n $ mean       : num  16.2 16.8 14.7 15.6 12.4 ...\n $ std_dev    : num  4.81 2.05 2.36 2.12 1.8 2.72 2.29 2.42 2.3 2.34 ...\n $ sample_size: int  37 27 33 30 28 30 29 30 28 25 ...\n $ std_error  : num  0.79 0.39 0.41 0.39 0.34 0.5 0.43 0.44 0.43 0.47 ...\n\n\nWith this data object in-hand, we can make a graph that includes both this and the original, unsummarized crab data. To better focus on the ‘multiple data objects’ bit of this example we’ll pare down on the actual graph code.\n\n1ggplot() +\n  geom_point(pie_crab, mapping = aes(x = latitude, y = size, fill = site),\n             pch = 21, size = 2, alpha = 0.2) + \n2  geom_errorbar(crab_summary, mapping = aes(x = latitude,\n                                            ymax = mean + std_error,\n                                            ymin = mean - std_error),\n                width = 0.2) +\n  geom_point(crab_summary, mapping = aes(x = latitude, y = mean, fill = site),\n             pch = 23, size = 3) + \n  theme(legend.title = element_blank(),\n        panel.background = element_blank(),\n        axis.line = element_line(color = \"black\"))\n\n\n1\n\nIf you want multiple data objects in the same ggplot2 graph you need to leave this top level ggplot() call empty! Otherwise you’ll get weird errors with aesthetics later in the graph\n\n2\n\nThis geometry adds the error bars and it’s important that we add it before the summarized data points themselves if we want the error bars to be ‘behind’ their respective points\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nActivity: Graph Creation (P2)\n\n\n\nIn a script, attempt the following:\n\nAdd a second data object to the graph you made in the preceding activity\n\nHint: If your first graph is unsummarized, add a summarized version (or vice versa)",
    "crumbs": [
      "Phase II -- Plan",
      "Data Visualization"
    ]
  },
  {
    "objectID": "mod_data-viz.html#streamlining-graph-aesthetics",
    "href": "mod_data-viz.html#streamlining-graph-aesthetics",
    "title": "Data Visualization & Exploration",
    "section": "Streamlining Graph Aesthetics",
    "text": "Streamlining Graph Aesthetics\nSynthesis projects often generate an entire network of inter-related papers. Ensuring that all graphs across papers from a given team have a similar “feel” is a nice way of implying a certain standard of robustness for all of your group’s projects. However, copy/pasting the theme elements of your graphs can (A) be cumbersome to do even once and (B) needs to be re-done every time you make a change anywhere. Fortunately, there is a better way!\nggplot2 supports adding theme elements to an object that can then be reused as needed elsewhere. This is the same theory behind wrapping repeated operations into custom functions.\n\n# Define core theme elements\ntheme_synthesis &lt;- theme(legend.position = \"none\",\n                         panel.background = element_blank(),\n                         axis.line = element_line(color = \"black\"),\n1                         axis.text = element_text(size = 13))\n\n# Create a graph\nggplot(pie_crab, aes(y = water_temp, x = air_temp, color = size, size = size)) +\n  geom_point() +\n  theme_synthesis +\n2  theme(legend.position = \"right\")\n\n\n1\n\nThis theme element controls the text on the tick marks. axis.title controls the text in the labels of the axes\n\n2\n\nAs a bonus, subsequent uses of theme() will replace defaults defined in your earlier theme object. So, you can design a set of theme elements that are usually appropriate and then easily change just some of them as needed\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nActivity: Graph Creation (P3)\n\n\n\nIn a script, attempt the following:\n\nRemove all theme edits from the graph you made in the preceding activity and assign them to a separate object\n\nThen add that object to your graph\n\nMake a second (different) graph and add your consolidated theme object to that graph as well",
    "crumbs": [
      "Phase II -- Plan",
      "Data Visualization"
    ]
  },
  {
    "objectID": "mod_data-viz.html#multi-panel-graphs",
    "href": "mod_data-viz.html#multi-panel-graphs",
    "title": "Data Visualization & Exploration",
    "section": "Multi-Panel Graphs",
    "text": "Multi-Panel Graphs\nIt is sometimes the case that you want to make a single graph file that has multiple panels. For many of us, we might default to creating the separate graphs that we want, exporting them, and then using software like Microsoft PowerPoint to stitch those panels into the single image we had in mind from the start. However, as all of us who have used this method know, this is hugely cumbersome when your advisor/committee/reviewers ask for edits and you now have to redo all of the manual work behind your multi-panel graph.\nFortunately, there are two nice entirely scripted alternatives that you might consider: Faceted graphs and Plot grids. See below for more information on both.\n\nFacetsPlot Grids\n\n\nIn a faceted graph, every panel of the graph has the same aesthetics. These are often used when you want to show the relationship between two (or more) variables but separated by some other variable. In synthesis work, you might show the relationship between your core response and explanatory variables but facet by the original study. This would leave you with one panel per study where each would show the relationship only at that particular study.\nLet’s check out an example.\n\nggplot(pie_crab, aes(x = date, y = size, color = site))+\n  geom_point(size = 2) +\n1  facet_wrap(. ~ site) +\n  theme_bw() +\n2  theme(legend.position = \"none\")\n\n\n1\n\nThis is a ggplot2 function that assumes you want panels laid out in a regular grid. There are other facet_... alternatives that let you specify row versus column arrangement. You could also facet by multiple variables by putting something to the left of the tilde\n\n2\n\nWe can remove the legend because the site names are in the facet titles in the gray boxes\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn a plot grid, each panel is completely independent of all others. These are often used in publications where you want to highlight several different relationships that have some thematic connection. In synthesis work, your hypotheses may be more complicated than in primary research and such a plot grid would then be necessary to put all visual evidence for a hypothesis in the same location. On a practical note, plot grids are also a common way of circumventing figure number limits enforced by journals.\nLet’s check out an example that relies on the cowplot library.\n\n# Load a needed library\nlibrary(cowplot)\n\n# Create the first graph\n1crab_p1 &lt;- ggplot(pie_crab, aes(x = site, y = size, fill = site)) +\n  geom_violin() +\n2  coord_flip() +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n# Create the second\ncrab_p2 &lt;- ggplot(pie_crab, aes(x = air_temp, y = water_temp)) +\n  geom_errorbar(aes(ymax = water_temp + water_temp_sd, ymin = water_temp - water_temp_sd),\n                 width = 0.1) +\n3  geom_errorbarh(aes(xmax = air_temp + air_temp_sd, xmin = air_temp - air_temp_sd),\n                 width = 0.1) +\n  geom_point(aes(fill = site), pch = 23, size = 3) +\n  theme_bw()\n\n# Assemble into a plot grid\n4cowplot::plot_grid(crab_p1, crab_p2, labels = \"AUTO\", nrow = 1)\n\n\n1\n\nNote that we’re assigning these graphs to objects!\n\n2\n\nThis is a handy function for flipping X and Y axes without re-mapping the aesthetics\n\n3\n\nThis geometry is responsible for horizontal error bars (note the “h” at the end of the function name)\n\n4\n\nThe labels = \"AUTO\" argument means that each panel of the plot grid gets the next sequential capital letter. You could also substitute that for a vector with labels of your choosing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nActivity: Graph Creation (P4)\n\n\n\nIn a script, attempt the following:\n\nAssemble the two graphs you made in the preceding two activities into the appropriate type of multi-panel graph",
    "crumbs": [
      "Phase II -- Plan",
      "Data Visualization"
    ]
  },
  {
    "objectID": "mod_data-viz.html#accessibility-considerations",
    "href": "mod_data-viz.html#accessibility-considerations",
    "title": "Data Visualization & Exploration",
    "section": "Accessibility Considerations",
    "text": "Accessibility Considerations\nAfter you’ve made the graphs you need, it is good practice to revisit them with to ensure that they are as accessible as possible. You can of course also do this during the graph construction process but it is sometimes less onerous to tackle as a penultimate step in the figure creation process. There are many facets to accessibility and we’ve tried to cover just a few of them below.\n\nColor Choice\nOne of the more well-known facets of accessibility in data visualization is choosing colors that are “colorblind safe”. Such palettes still create distinctive colors for those with various forms of color blindness (e.g., deuteranomoly, protanomaly, etc.). The classic red-green heatmap for instance is very colorblind unsafe in that people with some forms of colorblindness cannot distinguish between those colors (hence the rise of the yellow-blue heatmap in recent years). Unforunately, the ggplot2 default rainbow palette–while nice for exploratory purposes–is not colorlbind sfae.\nSome websites (such as colorbewer2.org) include a simple checkbox for colorblindness safety which automatically limits the listed options to those that are colorblind safe. Alternately, you could use a browser plug-in (such as Let’s get color blind on Google Chrome) to simulate colorblindness on a particular page.\nOne extreme approach you could take is to dodge this issue entirely and format your graphs such that color either isn’t used at all or only conveys information that is also conveyed in another graph aesthetic. We don’t necessarily recommend this as color–when the palette is chosen correctly–can be a really nice way of making information-dense graphs more informative and easily-navigable by viewers.\n\n\nMultiple Modalities\nRelated to the color conversation is the value of mapping multiple aesthetics to the same variable. By presenting information in multiple ways–even if that seems redundant–you enable a wider audience to gain an intuitive sense of what you’re trying to display.\n\nggplot(data = pie_crab, mapping = aes(x = latitude, y = size, \n1                                      fill = site, shape = site)) +\n  geom_jitter(size = 2, width = 0.1, alpha = 0.6) +  \n2  scale_shape_manual(values = c(21:25, 21:25, 21:23)) +\n  theme_bw() +\n  theme(legend.title = element_blank())\n\n\n1\n\nIn this graph we’re mapping both the fill and shape aesthetics to site\n\n2\n\nThis is a little cumbersome but there are only five ‘fill-able’ shapes in R so we need to reuse some of them to have a unique one for each site. Using fill-able shapes is nice because you get a crisp black border around each point. See ?pch for all available shapes\n\n\n\n\n\n\n\n\n\n\n\nIn the above graph, even though the rainbow palette is not ideal for reasons mentioned earlier, it is now much easier to tell the difference between sites with similar colors. For instance, “NB”, “NIB”, and “PIE” are all shades of light blue/teal. Now that they have unique shapes it is dramatically easier to look at the graph and identify which points correspond to which site.\n\n\n\n\n\n\nDiscussion: Graph Accessibility\n\n\n\nWith a group discuss (some of) the following questions:\n\nWhat are other facets of accessibility that you think are important to consider when making data visualizations?\nWhat changes do you make to your graphs to increase accessibility?\n\nWhat changes could you make going forward?\n\n\n\n\n\n\nPresentation vs. Publication\nOne final element of accessibility to consider is the difference between a ‘presentation-quality’ graph and a ‘publication-quality’ one. While it may be tempting to create a single version of a given graph and use it in both contexts that is likely to be less effective in helping you to get your point across than making small tweaks to two separate versions of what is otherwise the same graph.\n\nPresentation-FocusedPublication-Focused\n\n\nDo:\n\nIncrease size of text/points greatly\n\nIf possible, sit in the back row of the room where you’ll present and look at your graphs from there\n\nConsider adding graph elements that highlight certain graph regions\nPresent summarized data (increases focus on big-picture trends and avoids discussion of minutiae)\nMap multiple aesthetics to the same variables\n\nDon’t:\n\nUse technical language / jargon\nInclude unnecessary background elements\nUse multi-panel graphs (either faceted or plot grid)\n\nIf you have multiple graph panels, put each on its own slide!\n\n\n\nggplot(crab_summary, aes(x = latitude, y = mean, \n1                         shape = reorder(site, latitude),\n                         fill = reorder(site, latitude))) +\n  geom_vline(xintercept = 36.5, color = \"black\", linetype = 1) +\n2  geom_vline(xintercept = 41.5, color = \"black\", linetype = 2) +\n  geom_errorbar(mapping = aes(ymax = mean + std_error, ymin = mean - std_error),\n                width = 0.2) +\n  geom_point(size = 4) + \n  scale_shape_manual(values = c(21:25, 21:25, 21:23)) +\n3  labs(x = \"Latitude\", y = \"Mean Crab Size (mm)\") +\n  theme(legend.title = element_blank(),\n        axis.line = element_line(color = \"black\"),\n        panel.background = element_blank(),\n        axis.title = element_text(size = 17),\n        axis.text = element_text(size = 15))\n\n\n1\n\nWe can use the reorder function to make the order of sites in the legend (from top to bottom) match the order of sites in the graph (from left to right)\n\n2\n\nAdding vertical lines at particular parts in the graph can make comparisons within the same graph easier\n\n3\n\nlabs lets us customize the title and label text of a graph\n\n\n\n\n\n\n\n\n\n\n\n\n\nDo:\n\nIncrease size of text/points slightly\n\nYou want to be legible but you can more safely assume that many readers will be able to increase the zoom of their browser window if needed\n\nPresent un-summarized data (with or without summarized points included)\n\nMany reviewers will want to get a sense for the “real” data so you should include unsummarized values wherever possible\n\nUse multi-panel graphs\n\nIf multiple graphs “tell a story” together, then they should be included in the same file!\n\nMap multiple aesthetics to the same variables\nIf publishing in a journal available in print, check to make sure your graph still makes sense in grayscale\n\nThere are nice browser plug-ins (like Grayscale the Web for Google Chrome) for this too\n\n\nDon’t:\n\nInclude unnecessary background elements\nAdd graph elements that highlight certain graph regions\n\nYou can–and should–lean more heavily on the text of your publication to discuss particular areas of a graph\n\n\n\nggplot() +\n  geom_point(pie_crab, mapping = aes(x = latitude, y = size,\n                                     color = reorder(site, latitude)),\n             pch = 19, size = 1, alpha = 0.3) +\n  geom_errorbar(crab_summary, mapping = aes(x = latitude, y = mean, \n                                  ymax = mean + std_error, \n                                  ymin = mean - std_error),\n                width = 0.2) +\n  geom_point(crab_summary, mapping = aes(x = latitude, y = mean, \n                           shape = reorder(site, latitude),\n                           fill = reorder(site, latitude)),\n            size = 4) +\n  scale_shape_manual(values = c(21:25, 21:25, 21:23)) +\n1  labs(x = \"Latitude\", y = \"Mean Crab Carapace Width (mm)\") +\n  theme(legend.title = element_blank(),\n        axis.line = element_line(color = \"black\"),\n        panel.background = element_blank(),\n        axis.title = element_text(size = 15),\n        axis.text = element_text(size = 13))\n\n\n1\n\nHere we are using a reasonable amount of technical language",
    "crumbs": [
      "Phase II -- Plan",
      "Data Visualization"
    ]
  },
  {
    "objectID": "mod_data-viz.html#ordination",
    "href": "mod_data-viz.html#ordination",
    "title": "Data Visualization & Exploration",
    "section": "Ordination",
    "text": "Ordination\nIf you are working with multivariate data (i.e., data where multiple columns are all response variables collectively) you may find ordination helpful. Ordination is the general term for many types of multivariate visualization but typically is used to refer to visualizing a distance or dissimiliarity measure of the data. Such measures collapse all of those columns of response variables into fewer (typically two) index values that are easier to visualize.\nThis is a common approach particularly in answering questions in community ecology or considering a suite of traits (e.g., life history, landscape, etc.) together. While the math behind reducing the dimensionality of your data is interesting, this module is focused on only the visualization facet of ordination so we’ll avoid deeper discussion of the internal mechanics that underpin ordination.\nIn order to demonstrate two types of ordination we’ll use a lichen community composition dataset included in the vegan package. However, ordination approaches are most often used on data with multiple groups so we’ll need to make a simulated grouping column to divide the lichen community data.\n\n# Load library\nlibrary(vegan)\n\n# Grab data\nutils::data(\"varespec\", package = \"vegan\")\n\n# Create a faux group column\ntreatment &lt;- c(rep.int(\"Treatment A\", nrow(varespec) / 2),\n               rep.int(\"Treatment B\", nrow(varespec) / 2))\n\n# Combine into one dataframe\nlichen_df &lt;- cbind(treatment, varespec)\n\n# Check structure of first few columns\nstr(lichen_df[1:5])\n\n'data.frame':   24 obs. of  5 variables:\n $ treatment: chr  \"Treatment A\" \"Treatment A\" \"Treatment A\" \"Treatment A\" ...\n $ Callvulg : num  0.55 0.67 0.1 0 0 ...\n $ Empenigr : num  11.13 0.17 1.55 15.13 12.68 ...\n $ Rhodtome : num  0 0 0 2.42 0 0 1.55 0 0.35 0.07 ...\n $ Vaccmyrt : num  0 0.35 0 5.92 0 ...\n\n\n\nMetric OrdinationNon-Metric Ordination\n\n\nMetric ordinations are typically used when you are concerned with retaining quantitative differences among particular points, even after you’ve collapsed many response variables into just one or two. For example, this is a common approach if you have a table of traits and want to compare the whole set of traits among groups while still being able to interpret the effect of a particular effect on the whole.\nTwo of the more common methods for metric ordination are Principal Components Analysis (PCA), and Principal Coordinates Analysis (PCoA / “metric multidimensional scaling”). The primary difference is that PCA works on the data directly while PCoA works on a distance matrix of the data. We’ll use PCoA in this example because it is closer analog to the non-metric ordination discussed in the other tab. If the holistic difference among groups is of interest, (rather than metric point-to-point comparisons), consider a non-metric ordination approach.\nIn order to perform a PCoA ordination we first need to get a distance matrix of our response variables and then we can actually do the PCoA step. The distance matrix can be calculated with the vegdist function from the vegan package and the pcoa function in the ape package can do the actual PCoA.\n\n# Load needed libraries\nlibrary(vegan); library(ape)\n\n# Get distance matrix\n1lichen_dist &lt;- vegan::vegdist(x = lichen_df[-1], method = \"kulczynski\")\n\n# Do PCoA\npcoa_points &lt;- ape::pcoa(D = lichen_dist)\n\n\n1\n\nThe method argument requires a distance/dissimilarity measure. Note that if you use a non-metric measure (e.g., Bray Curtis, etc.) you lose many of the advantages conferred by using a metric ordination approach.\n\n\n\n\nWith that in hand, we can make our ordination! While you could make this step-by-step on your own, we’ll use the ordination function from the supportR package for convenience. This function automatically uses colorblind safe colors for up to 10 groups and has some useful base plot defaults (as well as including ellipses around the standard deviation of the centorid of all groups).\n\n# Load the library\nlibrary(supportR)\n\n# Make the ordination\nsupportR::ordination(mod = pcoa_points, grps = lichen_df$treatment, \n1                     x = \"topleft\", legend = c(\"A\", \"B\"))\n\n\n1\n\nThis function allows several base plot arguments to be supplied to alter non-critical plot elements (e.g., legend position, point size, etc.)\n\n\n\n\n\n\n\n\n\n\n\nThe percentages included in parentheses on either axis label are the percent of the total variation in the data explained by each axis on its own. Use this information in combination with what the graph looks like to determine how different the groups truly are.\n\n\nNon-metric ordinations are typically used when you care more about the relative differences among groups rather than specific measurements between particular points. For instance, you may want to assess whether the composition of insect communities differs between two experimental treatments. In such a case, your hypothesis likely depends more on the holistic difference between the treatments rather than some quantitative difference on one of the axes.\nThe most common non-metric ordination type is called Nonmetric Multidimensional Scaling (NMS / NMDS). This approach prioritizes making groups that are “more different” further apart than those that are less different. However, NMS uses a dissimilarity matrix which means that the distance between any two specific points cannot be interpreted meaningfully. It is appropriate though to interpret which cloud of points is closer to/further from another in aggregate. If specific distances among points are of interest, consider a metric ordination approach.\nIn order to perform an NMS ordination we’ll first need to calculate a dissimilarity matrix for our response data. The vegan function metaMDS is useful for this. This function has many arguments but the most fundamental are the following:\n\ncomm = the dataframe of response variables (minus any non-numeric / grouping columns)\ndistance = the distance/dissimilarity metric to use\n\nNote that there is no benefit to using a metric distance because when we make the ordination it will become non-metric\n\nk = number of axes to decompose to – typically two so the graph can be simple\ntry = number of attempts at minimizing “stress”\n\nStress is how NMS evaluates how good of a job it did at representing the true differences among groups (lower stress is better)\n\n\n\n# Load needed libraries\nlibrary(vegan)\n\n# Get dissimilarity matrix\ndissim_mat &lt;- vegan::metaMDS(comm = lichen_df[-1], distance = \"bray\", k = 2,\n                             autotransform = F, expand = F, try = 50)\n\nWith that in hand, we can make our ordination! While you could make this step-by-step on your own, we’ll use the ordination function from the supportR package for convenience. This function automatically uses colorblind safe colors for up to 10 groups and has some useful base plot defaults (as well as including ellipses around the standard deviation of the centorid of all groups).\n\n# Load the library\nlibrary(supportR)\n\n# Make the ordination\nsupportR::ordination(mod = dissim_mat, grps = lichen_df$treatment, \n1                     x = \"bottomright\", legend = c(\"A\", \"B\"))\n\n\n1\n\nThis function allows several base plot arguments to be supplied to alter non-critical plot elements (e.g., legend position, point size, etc.)\n\n\n\n\n\n\n\n\n\n\n\nIf the stress is less than 0.15 it is generally considered a good representation of the data. We can see that the ellipses do not overlap which indicates that the community composition of our two groups does seem to differ. We’d need to do real multivariate analysis if we wanted a p-value or AIC score to support that but as a visual tool this is still useful.",
    "crumbs": [
      "Phase II -- Plan",
      "Data Visualization"
    ]
  },
  {
    "objectID": "mod_data-viz.html#maps",
    "href": "mod_data-viz.html#maps",
    "title": "Data Visualization & Exploration",
    "section": "Maps",
    "text": "Maps\nYou may find it valuable to create a map as an additional way of visualizing data. Many synthesis groups do this–particularly when there is a strong spatial component to the research questions and/or hypotheses.\nCheck out the bonus spatial data module for more information on map-making if this is of interest!",
    "crumbs": [
      "Phase II -- Plan",
      "Data Visualization"
    ]
  },
  {
    "objectID": "mod_data-viz.html#additional-resources",
    "href": "mod_data-viz.html#additional-resources",
    "title": "Data Visualization & Exploration",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\nNCEAS Colorblind Safe Color Schemes reference document\n\n\n\nWorkshops & Courses\n\nNCEAS Scientific Computing team’s Coding in the Tidyverse workshop ggplot2 module\nThe Carpentries’ Data Analysis and Visualization in R for Ecologists ggplot2 episode\n\n\n\nWebsites",
    "crumbs": [
      "Phase II -- Plan",
      "Data Visualization"
    ]
  },
  {
    "objectID": "instructors.html",
    "href": "instructors.html",
    "title": "SSECR Design Team",
    "section": "",
    "text": "Marty Downs (she/her) –  ORCID\nNick J Lyon (they/them) –  Website –  GitHub –  ORCID\nCarrie Kappel (she/her) –  Website\n\n\n\n\nIngrid Slette (she/her)\nLi Kui (she/her)\nAngel Chen (she/her)"
  },
  {
    "objectID": "instructors.html#module-designers",
    "href": "instructors.html#module-designers",
    "title": "SSECR Design Team",
    "section": "",
    "text": "Marty Downs (she/her) –  ORCID\nNick J Lyon (they/them) –  Website –  GitHub –  ORCID\nCarrie Kappel (she/her) –  Website\n\n\n\n\nIngrid Slette (she/her)\nLi Kui (she/her)\nAngel Chen (she/her)"
  },
  {
    "objectID": "instructors.html#supporting-organizations",
    "href": "instructors.html#supporting-organizations",
    "title": "SSECR Design Team",
    "section": "Supporting Organizations",
    "text": "Supporting Organizations\n\nNational Center for Ecological Analysis and Synthesis (NCEAS)\nLong Term Ecological Research (LTER) Network Office (LNO)\nNational Science Foundation (NSF)"
  },
  {
    "objectID": "policy_conduct.html",
    "href": "policy_conduct.html",
    "title": "Synthesis Skills for Early Career Researchers",
    "section": "",
    "text": "Group work is a significant part of this course explicitly in the synthesis project facet as well as implicitly by the collaborative nature of many of the modules. We expect that you will be mutually respectful with one another both in and outside of class time. We will ask you questions during the course and during class is also an ideal time for you all to ask us questions that you have on course topics or policies. We don’t believe that “dumb questions” exist, and expect that you treat your peers’ questions with the respect you’d like your questions to be with. We will learn more together in an environment where we build one another up than we would in one where we fail to support one another."
  },
  {
    "objectID": "mod_data-disc.html",
    "href": "mod_data-disc.html",
    "title": "Data Discovery & Management",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase I -- Prepare",
      "Data Discovery"
    ]
  },
  {
    "objectID": "mod_data-disc.html#overview",
    "href": "mod_data-disc.html#overview",
    "title": "Data Discovery & Management",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase I -- Prepare",
      "Data Discovery"
    ]
  },
  {
    "objectID": "mod_data-disc.html#learning-objectives",
    "href": "mod_data-disc.html#learning-objectives",
    "title": "Data Discovery & Management",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nExplore common data repositories\nUse Google search operators to more effectively perform general searches\nDefine fundamental properties of effective data management plans\nCreate a data management plan\nDescribe best practices for documenting data for archiving",
    "crumbs": [
      "Phase I -- Prepare",
      "Data Discovery"
    ]
  },
  {
    "objectID": "mod_data-disc.html#module-content",
    "href": "mod_data-disc.html#module-content",
    "title": "Data Discovery & Management",
    "section": "Module Content",
    "text": "Module Content",
    "crumbs": [
      "Phase I -- Prepare",
      "Data Discovery"
    ]
  },
  {
    "objectID": "mod_data-disc.html#additional-resources",
    "href": "mod_data-disc.html#additional-resources",
    "title": "Data Discovery & Management",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\nThe British Ecological Society’s Better Science Guides – Data Management Guide\n\n\n\nWorkshops & Courses\n\nNCEAS coreR Data Management Essentials lesson\nOpen Science Synthesis for the Delta Science Program’s Data Management Essentials and the FAIR & CARE Principles\nOpen Science Synthesis for the Delta Science Program’s Writing Data Management Plans\nNCEAS Scientific Computing team’s Data Acquisition instructions\n\n\n\nWebsites\n\nEnvironmental Data Initiative (EDI) Data Portal\nDataONE Data Catalog\nOcean Observatories Initiative (OOI) Data Explorer",
    "crumbs": [
      "Phase I -- Prepare",
      "Data Discovery"
    ]
  },
  {
    "objectID": "mod_template.html",
    "href": "mod_template.html",
    "title": "Template",
    "section": "",
    "text": "This page is meant to make it simpler to copy/paste certain page elements among module files.\n\n\n\n\n\n\nActivity: ACTIVITY NAME\n\n\n\nIn a script, attempt the following:\n\nX\n\nHint: hint text!\n\n\n\n\n\n\n\n\n\n\nDiscussion: DISCUSSION NAME\n\n\n\nWith a partner/group discuss (some of) the following questions:\n\n?\n?\n?"
  },
  {
    "objectID": "mod_interactivity.html",
    "href": "mod_interactivity.html",
    "title": "Creating Interactive Apps",
    "section": "",
    "text": "Shiny is a popular tool that allows users to build interactive web applications without the normally pre-requisite web development expertise. In addition to Shiny apps being simpler to build for the programmer they are often used to allow visitors to perform coding tasks without ever actually writing code. These are huge advantages because they reduce or eliminate significant technical barriers in developing truly interactive applications.\nIn synthesis contexts, Shiny can be used for a variety of valuable purposes. You can use it to develop dashboards for sharing data with related communities, allow your team to quickly “play with” exploratory graphs, or even to create a data submission portal (as is the case with some Research Coordination Networks or “RCNs”).\nNote that Shiny can be built in either R or Python ‘under the hood’ but for the purposes of this module we’ll focus on R.",
    "crumbs": [
      "Bonus Modules",
      "Interactivity"
    ]
  },
  {
    "objectID": "mod_interactivity.html#overview",
    "href": "mod_interactivity.html#overview",
    "title": "Creating Interactive Apps",
    "section": "",
    "text": "Shiny is a popular tool that allows users to build interactive web applications without the normally pre-requisite web development expertise. In addition to Shiny apps being simpler to build for the programmer they are often used to allow visitors to perform coding tasks without ever actually writing code. These are huge advantages because they reduce or eliminate significant technical barriers in developing truly interactive applications.\nIn synthesis contexts, Shiny can be used for a variety of valuable purposes. You can use it to develop dashboards for sharing data with related communities, allow your team to quickly “play with” exploratory graphs, or even to create a data submission portal (as is the case with some Research Coordination Networks or “RCNs”).\nNote that Shiny can be built in either R or Python ‘under the hood’ but for the purposes of this module we’ll focus on R.",
    "crumbs": [
      "Bonus Modules",
      "Interactivity"
    ]
  },
  {
    "objectID": "mod_interactivity.html#learning-objectives",
    "href": "mod_interactivity.html#learning-objectives",
    "title": "Creating Interactive Apps",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this topic you will be able to:\n\nDefine the three fundamental components of a Shiny app\nExplain benefits and limitations of interactive approaches to data exploration\nGenerate an interactive app with Shiny\nUse text formatting methods in a Shiny app\nExplore available Shiny layout options\nCreate a Shiny app\nDescribe (briefly) the purpose of deploying a Shiny app",
    "crumbs": [
      "Bonus Modules",
      "Interactivity"
    ]
  },
  {
    "objectID": "mod_interactivity.html#needed-packages",
    "href": "mod_interactivity.html#needed-packages",
    "title": "Creating Interactive Apps",
    "section": "Needed Packages",
    "text": "Needed Packages\nIf you’d like to follow along with the code chunks included throughout this module, you’ll need to install the following packages:\n\n# Note that these lines only need to be run once per computer\n## So you can skip this step if you've installed these before\ninstall.packages(\"tidyverse\")\ninstall.packages(\"shiny\")\ninstall.packages(\"htmltools\")\ninstall.packages(\"lterdatasampler\")\n\nWe’ll load the Tidyverse meta-package here to have access to many of its useful tools when we need them later as well as the shiny package.\n\n# Load needed libraries\nlibrary(tidyverse); library(shiny)",
    "crumbs": [
      "Bonus Modules",
      "Interactivity"
    ]
  },
  {
    "objectID": "mod_interactivity.html#shiny-fundamentals",
    "href": "mod_interactivity.html#shiny-fundamentals",
    "title": "Creating Interactive Apps",
    "section": "Shiny Fundamentals",
    "text": "Shiny Fundamentals\nAll Shiny apps are composed of three pieces: a user interface (UI), a server, and a call to the shinyApp function. The user interface includes everything that the user sees and can interact with; note that this includes both inputs and outputs. The server is responsible for all code operations performed on user inputs in order to generate outputs specified in the UI. The server is not available to the user. Finally, the shinyApp function simply binds together the UI and server and creates a living app. The app appears either in your RStudio or in a new tab on a web browser depending on your settings.\nFor those of you who write your own functions, you may notice that the syntax of Shiny is very similar to the syntax of functions. If you have not already, your quality of life will benefit greatly if you turn on “rainbow parentheses” in RStudio (Tools  Global Options  Code  Display  Check “Use rainbow parentheses” box).\nLet’s consider an artificially simple Shiny app so you can get a sense for the fundamental architecture of this tool.\n\n# Define the UI\n1basic_ui &lt;- shiny::fluidPage(\n  \"Hello there!\"\n)\n\n# Define the server\n2basic_server &lt;- function(input, output){ }\n\n# Generate the app\nshiny::shinyApp(ui = basic_ui, server = basic_server)\n\n\n1\n\nThe fluidPage function is important for leaving flexibility in UI layout which we’ll explore later in the module\n\n2\n\nBecause this app has no inputs or outputs, it doesn’t need anything in the ‘server’ component (though it still does require an empty server!)\n\n\n\n\nIf you copy and run the above code, you should see an app that is a blank white page with “Hello there!” written in the top left in plain text. Congratulations, you have now made your first Shiny app! Now, your reason for exploring this module likely involves an app that actually does something but the fundamental structure of all apps–even skeletal apps like this one–is the same. More complicated apps will certainly have more content in the UI and server sections but all Shiny apps will have this tripartite structure.",
    "crumbs": [
      "Bonus Modules",
      "Interactivity"
    ]
  },
  {
    "objectID": "mod_interactivity.html#interactive-apps",
    "href": "mod_interactivity.html#interactive-apps",
    "title": "Creating Interactive Apps",
    "section": "Interactive Apps",
    "text": "Interactive Apps\nNow that we’ve covered non-reactive apps, let’s create an interactive one! It is important to remember that the user interface needs to contain both the inputs the user can make and the outputs determined by those inputs. The server will be responsible for turning the inputs into outputs but if you want your interactive app to actually show the user the interactivity you need to be careful to include the outputs in the UI.\nEssentially all Shiny UI functions use the same syntax of &lt;value class&gt;Input or &lt;value class&gt;Output. So, determining how you want the user to engage with your app is sometimes as straightforward as identifying the class of the value you want them to interact with. Shiny calls these helper functions “widgets”.\nLet’s consider an app that accepts a single number and returns the square root of that number.\n\n# Define the UI ---- \nreactive_ui &lt;- shiny::fluidPage(\n  \n  # Create input\n1  shiny::numericInput(inputId = \"num_in\",\n                      label = \"Type a number\",\n                      value = 16),\n  \n  # Include some plain text for contextualizing the output\n2  \"Square root is: \",\n  \n  # Create output\n  shiny::textOutput(outputId = \"num_out\")\n  \n) # Close UI\n\n# Define server ----\nreactive_server &lt;- function(input, output){\n  \n  # Reactively accept the input and take the square root of it\n3  root &lt;- shiny::reactive({\n4    sqrt(x = input$num_in)\n  })\n  \n  # Make that value an output of the server/app\n5  output$num_out &lt;- shiny::renderText(\n6    expr = root()\n  ) \n  \n} # Close server\n\n# Generate the app ----\nshiny::shinyApp(ui = reactive_ui, server = reactive_server)\n\n\n1\n\nNote that the argument name is capital “I” but lowercase “d”. Typing inputID is a common and frustrating source of error for Shiny app developers\n\n2\n\nEvery element of the UI–except the last one–needs to end with a comma\n\n3\n\nAll reactive elements (i.e., those that change as soon as the user changes an input) need to be specified inside of reactive with both parentheses and curly braces\n\n4\n\nThe name of this input exactly matches the inputId we defined in the UI. That it is an input is defined by our use of the numericInput widget\n\n5\n\nThe name of this output exactly matches the outputId we told the UI to expect.\n\n6\n\nReactive elements essentially become functions in their own right! So, when we want to use them, we need to include empty parentheses next to their name\n\n\n\n\nWe included a lot of footnote annotations in that code chunk to help provide context but there are a few small comments that are worthwhile to bring up at this stage.\n\nUI outputs and server renders must match\n\nThe widget you use in the UI to return an output must correspond to the function used in the server to generate that output. In this example, we use textOutput in the UI so in the server we use renderText. Essentially all widgets in Shiny use this &lt;class&gt;Output versus render&lt;Class&gt; syntax which can be a big help to visual checks that your app is written correctly. You will need to be sure that whatever the ‘class’ is, it is lowercase in the UI but title case in the server (i.e., only first letter capitalized).\n\nUse section header format\n\nThis app is relatively short but we think effectively hints at how long and convoluted purpose-built Shiny apps can easily become. So, we recommend using section headers in your Shiny app code. You can do this by putting either four hyphens or four hashtags at the end of a comment line (e.g., # Section 1 #### or # My header ----). Headings defined in this way will appear in the bottom left of the “Source” pane of RStudio next to a light orange hashtag symbol. Clicking the text in that area will open a drop-down menu showing all headings in your current file and clicking one of the other headings will instantly jump you to that heading. This can be incredibly convenient when you’re trying to navigate a several hundred line long Shiny app. While rainbow parentheses can be useful for avoiding typos within a section, section headers make it much easier to avoid typos across sections.\nIf you don’t use headings already (or your cursor is on a line before the first heading), the relevant bit of the “Source” pane will just say “(Top Level)” and will not have the golden hashtag symbol.",
    "crumbs": [
      "Bonus Modules",
      "Interactivity"
    ]
  },
  {
    "objectID": "mod_interactivity.html#including-data",
    "href": "mod_interactivity.html#including-data",
    "title": "Creating Interactive Apps",
    "section": "Including Data",
    "text": "Including Data\nYou can also use your Shiny app to work with a full data table! When running your app locally, you only need to read in the data as you normally would then run the app. By having read in the data you will ensure the object is in your environment and accessible to the app. However, keep in mind this will only work in “local” (i.e., non-deployed) contexts. See our–admittedly brief–discussion of deployment at the end of this module.\nLet’s explore an example using data about fiddler crabs (Minuca pugnax) at the Plum Island Ecosystems (PIE) LTER site from the lterdatasampler R package. The app we’re about to create will make a graph between any two (numeric) columns.\n\n# Load lterdatasampler package\nlibrary(lterdatasampler)\n\n# Load fiddler crab data\n1data(pie_crab)\n\n# Define the UI ---- \ndata_ui &lt;- shiny::fluidPage(\n  \n  # Let the user choose which the X axis\n  shiny::selectInput(inputId = \"x_vals\",\n              label = \"Choose the X-axis\",\n2              choices = setdiff(x = names(pie_crab),\n                                y = c(\"date\", \"site\", \"name\")),\n              selected = \"latitude\"),\n  \n  # Also the Y axis\n  shiny::selectInput(inputId = \"y_vals\",\n              label = \"Choose the Y-axis\",\n              choices = setdiff(x = names(pie_crab),\n                                y = c(\"date\", \"site\", \"name\")),\n              selected = \"size\"),\n  \n  # Return the desired plot\n  shiny::plotOutput(outputId = \"crab_graph\")\n              \n) # Close UI\n\n# Define the server ----\ndata_server &lt;- function(input, output){\n  \n  # Reactively identify X & Y axess\n3  picked_x &lt;- shiny::reactive({ input$x_vals })\n  picked_y &lt;- shiny::reactive({ input$y_vals })\n  \n  # Create the desired graph\n  output$crab_graph &lt;- shiny::renderPlot(\n    \n4    ggplot(pie_crab, aes(x = .data[[picked_x()]], y = .data[[picked_y()]])) +\n      geom_point(aes(fill = .data[[picked_x()]]), pch = 21, size = 2.5) +\n      labs(x = stringr::str_to_title(picked_x()),\n           y = stringr::str_to_title(picked_y())) +\n      theme_bw()\n    \n  ) # Close plot rendering\n  \n} # Close server\n\n# Generate the app ----\nshiny::shinyApp(ui = data_ui, server = data_server)\n\n\n1\n\nNote the loading of the data is done outside of the app! You can have the app load its own data but that is more complicated than this example needs to be.\n\n2\n\nTo make our life easier in the server we can exclude non-number columns\n\n3\n\nSee how we’re reactively grabbing both axes?\n\n4\n\nggplot2 requires special syntax to specify axes with quoted column names (which is how reactive Shiny elements from that widget are returned)",
    "crumbs": [
      "Bonus Modules",
      "Interactivity"
    ]
  },
  {
    "objectID": "mod_interactivity.html#layouts",
    "href": "mod_interactivity.html#layouts",
    "title": "Creating Interactive Apps",
    "section": "Layouts",
    "text": "Layouts\nExperimenting with different app layouts can be a fun step in the process of making an app that is as effective as possible! We do recommend that during app development you stick with a very simple user interface because it’ll be easier to make sure your inputs and outputs work as desired. Once you are satisfied with those elements you can relatively easily chengs the UI to help guide users through your app.\nAs implied by that preface, layouts are exclusively an element of the user interface! This is great when you have an app with a complicated server component because you won’t need to mess with that at all to get the UI looking perfect. In the examples below, we’ll generate a non-interactive app so that we can really emphasize the ‘how to’ perspective of using different layouts.\n\nSidebar\nOne of the more common Shiny UI choices is to use a sidebar. The sidebar typically takes up about one third of the width of the app while the remaining two thirds is taken up by the main panel. The sidebar can be nice place to put all the user inputs and have the outputs display in the main panel. This format allows for really clear visual separation between where you want the user to interact with the app versus where the results of their choices can be viewed.\n\n# Define the UI\nsidebar_ui &lt;- shiny::fluidPage(\n  \n  # Define the layout type\n1  shiny::sidebarLayout(\n  \n    # Define what goes in the eponymous sidebar\n    shiny::sidebarPanel(\n      \"Hello from the sidebar!\"\n2      ),\n    \n    # Define what goes in the main panel\n    shiny::mainPanel(\n      \"Hello from the main panel!\"\n3      ) ) )\n\n# Define the server\nbasic_server &lt;- function(input, output){ }\n\n# Generate the app\nshiny::shinyApp(ui = sidebar_ui, server = basic_server)\n\n\n1\n\nNotice that everything else in the UI is wrapped inside this function. If you want something above/below the sidebar vs. main panel you’ll need to put that content outside of this function’s parentheses but still in the fluidPage parentheses\n\n2\n\nBe careful not to forget this comma separating the sidebarPanel and mainPanel functions!\n\n3\n\nThree closing parentheses are needed to close the UI elements. This is why it’s really helpful to use rainbow parentheses in your coding environment!\n\n\n\n\n\n\nTab Panels\nIf you feel that your app is better represented in separate pages, tab panels may be a better layout choice! The result of this layout is a series of discrete tabs along the top of your app. If the user clicks one of them they’ll be able to look at a separate chunk of your app. Inputs in any tab are available to the app’s server and can be outputs in any tab (remember that their is a shared server so it is impossible for it to be otherwise!). Generally it may be a good idea to have inputs and outputs in the same tab so that users can see the interactive app responding to their inputs rather than needing to click back and forth among tabs to see the results of their inputs. For example, you could have an app where users choose what goes on either axis of several graph types and put each graph type on its own tab of the larger Shiny app.\n\n# Define the UI\ntabs_ui &lt;- shiny::fluidPage(\n  \n# Define the layout type\n1  shiny::tabsetPanel(\n  \n    # Define what goes in the first tab\n    shiny::tabPanel(title = \"Tab 1\",\n                    \"Hello from the first tab!\"\n2                    ),\n    \n    # And in the second\n    shiny::tabPanel(title = \"Tab 2\",\n                    \"Welcome to the second tab!\"\n                    ),\n    \n    # And so on\n    shiny::tabPanel(title = \"Tab 3\",\n                    \"Hello yet again!\"\n3                    ) ) )\n\n# Define the server\nbasic_server &lt;- function(input, output){ }\n\n# Generate the app\nshiny::shinyApp(ui = tabs_ui, server = basic_server)\n\n\n1\n\nThis function is comparable to sidebarLayout in that if you want stuff above/below the tab panel area you’ll need to be outside of this function’s parentheses but still in the fluidPage parentheses\n\n2\n\nAgain, just like the sidebarLayout subfunctions, you’ll need a comma after each UI element except the last one\n\n3\n\nHere we’re closing all of the nested UI functions\n\n\n\n\n\n\nOther Layouts\nWe just briefly covered two layout options but hopefully this is a nice indication for the kind of flexibility in user interface that you can expect of Shiny apps! For more information, check out Posit’s Shiny Application Layout Guide. That resource has some really nice examples of these and other layout options that will be well worth checking out as you begin your journey into Shiny.",
    "crumbs": [
      "Bonus Modules",
      "Interactivity"
    ]
  },
  {
    "objectID": "mod_interactivity.html#text-formatting",
    "href": "mod_interactivity.html#text-formatting",
    "title": "Creating Interactive Apps",
    "section": "Text Formatting",
    "text": "Text Formatting\nBeyond making your app have an intuitive layout it can be really helpful to be able to do even simple text formatting to assist your app’s users. For instance, you may want to use sub-headings within the same UI layout component but still want to draw a distinction between two sets of inputs. Additionally you may want to emphasize some tips for best results or hyperlink to your group’s other products. All of these can be accomplished using text formatting tools that are readily available within Shiny.\n\n# Load the `htmltools` library\nlibrary(htmltools)\n\n# Define the UI\ntext_ui &lt;- shiny::fluidPage(\n  \n  # Let's make some headings\n1  htmltools::h1(\"This is a Big Heading\"),\n  htmltools::h3(\"Smaller heading\"),\n  htmltools::h5(\"Even smaller heading!\"), \n  \n  # Now we'll format more text in various (non-heading) ways\n  htmltools::strong(\"Bold text\"),\n  \n2  htmltools::br(),\n\n  htmltools::a(href = \"https://lter.github.io/ssecr/mod_interactivity.html\",\n               \"This text is hyperlinked\",\n3               target = \"_blank\"),\n  \n  htmltools::br(),\n  \n4  htmltools::code(\"This is 'code' text\") )\n\n# Define the server\nbasic_server &lt;- function(input, output){ }\n\n# Generate the app\nshiny::shinyApp(ui = text_ui, server = basic_server)\n\n\n1\n\nHeadings (of any size) automatically include a line break after the heading text\n\n2\n\nThe br function creates a line break\n\n3\n\nWhen the target argument is set to “_blank” it will open a new tab when users click the hyperlinked text. This is ideal because if a user left your app to visit the new site they would lose all of their inputs\n\n4\n\nCode text looks like this",
    "crumbs": [
      "Bonus Modules",
      "Interactivity"
    ]
  },
  {
    "objectID": "mod_interactivity.html#deployment",
    "href": "mod_interactivity.html#deployment",
    "title": "Creating Interactive Apps",
    "section": "Deployment",
    "text": "Deployment\nWhen Shiny apps are only being used by those in your team, keeping them as a code script works well. However, if you’d like those outside of your team to be able to find your app as they would any other website you’ll need to deploy your Shiny app. This process is outside of the scope of this module but is often the end goal of Shiny app development.\nTake a look at Posit’s instructions for deployment for more details but essentially “deployment” is the process of getting your local app hosted on shinyapps.io which gives it a link that anyone can use to access/run your app on their web browser of choice.",
    "crumbs": [
      "Bonus Modules",
      "Interactivity"
    ]
  },
  {
    "objectID": "mod_interactivity.html#additional-interactivity-resources",
    "href": "mod_interactivity.html#additional-interactivity-resources",
    "title": "Creating Interactive Apps",
    "section": "Additional Interactivity Resources",
    "text": "Additional Interactivity Resources\n\nPapers & Documents\n\n\n\n\n\nWorkshops & Courses\n\nPosit’s Welcome to Shiny (for R coders)\n2022 All Scientists’ Meeting Shiny Apps for Sharing Science workshop\n\n\n\nWebsites\n\nPosit’s Shiny website",
    "crumbs": [
      "Bonus Modules",
      "Interactivity"
    ]
  },
  {
    "objectID": "policy_ai.html",
    "href": "policy_ai.html",
    "title": "Synthesis Skills for Early Career Researchers",
    "section": "",
    "text": "Artificial intelligence (AI) tools are increasingly well-known and widely discussed in the context of data science. AI products can increase the efficiency of code writing and are becoming a common part of the data science landscape. For the purposes of this course, we strongly recommend that you do not use AI tools to write code. There is an under-discussed ethical consideration to the use and training of these tools in addition to their known practical limitations. However, the main reason we suggest you not use them for this class though is that leaning too heavily upon AI tools is likely to negatively impact your learning and skill acquisition.\nYou may have prior experience with some of the quantitative skills this course aims to teach but others are likely new to you. During the first steps of learning any new skill, it can be really helpful to struggle a bit in solving problems. Your efforts now will help refine your troubleshooting skills and will likely make it easier to remember how you solved a given problem the next time it arises. Over-use of AI tools can short circuit this pathway to mastery. Once you have become a proficient coder, you will be better able to identify and avoid any distortions or assumptions introduced by relying on AI.\nAI Resources\n\nPratim Ray, P. ChatGPT: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope. 2023. Internet of Things and Cyber-Physical Systems\nTrust, T. ChatGPT & Education Slide Deck. 2023. National Teaching Repository\nCsik, S. Teach Me How to Google. University of California, Santa Barbara (UCSB) Master of Environmental Data Science (MEDS) Program."
  },
  {
    "objectID": "policy_usability.html",
    "href": "policy_usability.html",
    "title": "Synthesis Skills for Early Career Researchers",
    "section": "",
    "text": "We are committed to creating a course that is inclusive in its design. If you encounter barriers, please let the instructors know immediately so that we can determine if there is a design adjustment that can be made or if an accommodation might be needed to overcome the limitations of the design. We are always happy to consider creative solutions as long as they do not compromise the intent of the learning activity. We welcome feedback that will assist us in improving the usability and experience for all students."
  },
  {
    "objectID": "mod_team-sci.html",
    "href": "mod_team-sci.html",
    "title": "Team Science Practices",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase I -- Prepare",
      "Team Science"
    ]
  },
  {
    "objectID": "mod_team-sci.html#overview",
    "href": "mod_team-sci.html#overview",
    "title": "Team Science Practices",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase I -- Prepare",
      "Team Science"
    ]
  },
  {
    "objectID": "mod_team-sci.html#learning-objectives",
    "href": "mod_team-sci.html#learning-objectives",
    "title": "Team Science Practices",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nInterpret and enact current best practices in team science\nIdentify different interaction styles and the effect they have on group dynamics\nIdentify benefits (and potential costs of) diverse teams\nDescribe ways to mitigate costs of diverse teams\nExplain methods for improving the experience of virtual participants on hybrid teams ## Module Content",
    "crumbs": [
      "Phase I -- Prepare",
      "Team Science"
    ]
  },
  {
    "objectID": "mod_team-sci.html#additional-resources",
    "href": "mod_team-sci.html#additional-resources",
    "title": "Team Science Practices",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\nBates et al., Overcome Imposter Syndrome: Contribute to Working Groups and Build Strong Networks. 2024. Biological Conservation\nPeterson, D.M., et al., Team Science: A Syllabus for Success on Big Projects. 2023. Ecology and Evolution\nGaynor, K.M., et al., Ten Simple Rules to Cultivate Belonging in Collaborative Data Science Research Teams. 2022. PLoS Computational Biology\nDeutsch, L., et al., Leading Inter- and Transdisciplinary Research: Lessons from Applying Theories of Change to a Strategic Research Program. 2021. Environmental Science & Policy\nFarrell et al., Training Macrosystems Scientists Requires Both Interpersonal and Technical Skills. 2021. Frontiers in Ecology and the Environment\nHampton & Parker, Collaboration and Productivity in Scientific Synthesis. 2011. BioScience\n\n\n\nWorkshops & Courses\n\nAmelia Liberatore’s Developing a Successful Team workshop slides\nOpen Science Synthesis for the Delta Science Program’s Team Science for Synthesis\nOpen Science Synthesis for the Delta Science Program’s Thinking Preferences\n\n\n\nWebsites",
    "crumbs": [
      "Phase I -- Prepare",
      "Team Science"
    ]
  },
  {
    "objectID": "mod_next-steps.html",
    "href": "mod_next-steps.html",
    "title": "Next Steps & Logic Models",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase IV -- Magnify",
      "Next Steps"
    ]
  },
  {
    "objectID": "mod_next-steps.html#overview",
    "href": "mod_next-steps.html#overview",
    "title": "Next Steps & Logic Models",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase IV -- Magnify",
      "Next Steps"
    ]
  },
  {
    "objectID": "mod_next-steps.html#learning-objectives",
    "href": "mod_next-steps.html#learning-objectives",
    "title": "Next Steps & Logic Models",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nIdentify needed data and target audience(s)\nArticulate connection(s) between proposed investigation and beneficial outcome\nWrite polished, funding-worthy proposals",
    "crumbs": [
      "Phase IV -- Magnify",
      "Next Steps"
    ]
  },
  {
    "objectID": "mod_next-steps.html#module-content",
    "href": "mod_next-steps.html#module-content",
    "title": "Next Steps & Logic Models",
    "section": "Module Content",
    "text": "Module Content",
    "crumbs": [
      "Phase IV -- Magnify",
      "Next Steps"
    ]
  },
  {
    "objectID": "mod_next-steps.html#additional-resources",
    "href": "mod_next-steps.html#additional-resources",
    "title": "Next Steps & Logic Models",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\nDeutsch et al., Leading Inter- and Transdisciplinary Research: Lessons from Applying Theories of Change to a Strategic Research Program. 2021. Environmental Science & Policy\n\n\n\nWorkshops & Courses\n\n\n\n\n\nWebsites\n\nShabanov, I. (@Artifexx) X post on logic models",
    "crumbs": [
      "Phase IV -- Magnify",
      "Next Steps"
    ]
  }
]