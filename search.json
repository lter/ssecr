[
  {
    "objectID": "proj_teams.html",
    "href": "proj_teams.html",
    "title": "Project Teams",
    "section": "",
    "text": "As stated elsewhere in the course page, one of the primary facets of the course is the creation of teams of Synthesis Fellows. These teams collaboratively identify synthesis questions and pursue them throughout the 9-month run of the course. See below for brief descriptions of each project team.\nTeam members and project mentors are listed in reverse alphabetical order by last name and their LTER site affilitation is indicated in parentheses. For more information on LTER sites, check out the site profiles section of the Network website."
  },
  {
    "objectID": "proj_teams.html#overview",
    "href": "proj_teams.html#overview",
    "title": "Project Teams",
    "section": "",
    "text": "As stated elsewhere in the course page, one of the primary facets of the course is the creation of teams of Synthesis Fellows. These teams collaboratively identify synthesis questions and pursue them throughout the 9-month run of the course. See below for brief descriptions of each project team.\nTeam members and project mentors are listed in reverse alphabetical order by last name and their LTER site affilitation is indicated in parentheses. For more information on LTER sites, check out the site profiles section of the Network website."
  },
  {
    "objectID": "proj_teams.html#teams",
    "href": "proj_teams.html#teams",
    "title": "Project Teams",
    "section": "2024 Teams",
    "text": "2024 Teams\n\nCarbon Cycling Responses Across Spatiotemporal Scales\nTeam Members: Yiyang Xu (Plum Island Ecosystems; Virginia Coast Reserve), Taylor Walker (Moorea Coral Reef), Carla López Lloreda (Luquillo), Guopeng Liang (Cedar Creek), Jon Gewirtzman (Harvard Forest; Florida Coastal Everglades), Julie Gan (Arctic), Ricky Brokaw (Santa Barbara Coastal)\nScience Mentor(s): Will Wieder (Niwot Ridge), Marci Litvak (Sevilleta)\n GitHub Repository: lter / ssecr-c-cycling\nProject Question(s): How does carbon cycling respond to environmental change (in experimental manipulations and to natural disturbances)? How does the duration of the disturbance influence how we model carbon cycling projections in the future across ecosystems? Further, what are the mechanisms driving these responses?\n\n\nAbove-Belowground Coupling\nTeam Members: Smriti Pehim Limbu (Konza Prairie), McKinley Nevins (Andrews Forest), Francis Chaves Rodriguez (Konza Prairie), Ashley Bulseco (Plum Island Ecosystems), Abigail Borgmeier (McMurdo Dry Valleys; Sevilleta)\nScience Mentor(s): Serita Frey (Harvard Forest), Meghan Avolio (Konza Prairie)\n GitHub Repository: lter / ssecr-above-belowground-coupling\nProject Question(s): Are above and belowground communities synchronous or decoupled in response to temperature and precipitation variability?\n\n\nDiversity-Stability Relationships\nTeam Members: Junna Wang (Harvard Forest), James Sturges (Florida Coastal Everglades), Kelsey Solomon (Florida Coastal Everglades; Coweeta), Julianna Renzi (Moorea Coral Reef), Pooja Panwar (Hubbard Brook), Katherine Hulting (Kellogg Biological Station), Noam Altman-Kurosaki (Moorea Coral Reef)\nScience Mentor(s): Forest Isbell (Cedar Creek), Laura Dee (Niwot Ridge)\n GitHub Repository: lter / ssecr-diversity-stability\nProject Question(s): How does the strength and direction of diversity-stability relationships vary (1) among ecosystems and (2) between ecosystem-level and community-level stability?\n\n\nEffects of Environmental Drivers Across Ecological Scales\nTeam Members: Bethany Williams, Sierra Perez (Kellogg Biological Station; Cedar Creek), Evald Maceno (Luquillo), Joey Krieger Lodge (Niwot Ridge), Jeremy Collings (Andrews Forest), Allison Case\nScience Mentor(s): Zach Feiner (North Temperate Lakes), Nancy Emery (Niwot Ridge), Joan Dudney\n GitHub Repository: lter / ssecr-driver-scales\nProject Question(s): Are the effects of environmental drivers consistent across ecological scales?"
  },
  {
    "objectID": "policy/usability.html",
    "href": "policy/usability.html",
    "title": "Synthesis Skills for Early Career Researchers",
    "section": "",
    "text": "We are committed to creating a course that is inclusive in its design. If you encounter barriers, please let the instructors know immediately so that we can determine if there is a design adjustment that can be made or if an accommodation might be needed to overcome the limitations of the design. We are always happy to consider creative solutions as long as they do not compromise the intent of the learning activity. We welcome feedback that will assist us in improving the usability and experience for all students."
  },
  {
    "objectID": "policy/conduct.html",
    "href": "policy/conduct.html",
    "title": "Synthesis Skills for Early Career Researchers",
    "section": "",
    "text": "Group work is a significant part of this course explicitly in the synthesis project facet as well as implicitly by the collaborative nature of many of the modules. We expect that you will be mutually respectful with one another both in and outside of class time. We will ask you questions during the course and during class is also an ideal time for you all to ask us questions that you have on course topics or policies. We don’t believe that “dumb questions” exist, and expect that you treat your peers’ questions with the respect you’d like your questions to be with. We will learn more together in an environment where we build one another up than we would in one where we fail to support one another."
  },
  {
    "objectID": "policy/ai.html",
    "href": "policy/ai.html",
    "title": "Synthesis Skills for Early Career Researchers",
    "section": "",
    "text": "Artificial intelligence (AI) tools are increasingly well-known and widely discussed in the context of data science. AI products can increase the efficiency of code writing and are becoming a common part of the data science landscape. For the purposes of this course, we strongly recommend that you do not use AI tools to write code. There is an under-discussed ethical consideration to the use and training of these tools in addition to their known practical limitations. However, the main reason we suggest you not use them for this class though is that leaning too heavily upon AI tools is likely to negatively impact your learning and skill acquisition.\nYou may have prior experience with some of the quantitative skills this course aims to teach but others are likely new to you. During the first steps of learning any new skill, it can be really helpful to struggle a bit in solving problems. Your efforts now will help refine your troubleshooting skills and will likely make it easier to remember how you solved a given problem the next time it arises. Over-use of AI tools can short circuit this pathway to mastery. Once you have become a proficient coder, you will be better able to identify and avoid any distortions or assumptions introduced by relying on AI.\nAI Resources\n\nPratim Ray, P. ChatGPT: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope. 2023. Internet of Things and Cyber-Physical Systems\nTrust, T. ChatGPT & Education Slide Deck. 2023. National Teaching Repository\nCsik, S. Teach Me How to Google. University of California, Santa Barbara (UCSB) Master of Environmental Data Science (MEDS) Program."
  },
  {
    "objectID": "mod_version-control.html",
    "href": "mod_version-control.html",
    "title": "Version Control",
    "section": "",
    "text": "Synthesis projects require an intensely collaborative team spirit complemented by an emphasis on quantitative rigor. While other modules focus on elements of each of these, version control is a useful tool that lives at their intersection. We will shortly embark on a deep dive into version control but broadly it is a method of tracking changes to files (especially code “scripts”) that lends itself remarkably well to coding in groups. The LTER Network Office has a Scientific Computing Team that developed a robust workshop-style set of materials aimed at teaching version control to LTER-funded synthesis working groups. Rather than re-invent these materials for the purposes of SSECR, we will work through parts of these workshop materials.",
    "crumbs": [
      "Phase I -- Prepare",
      "Version Control"
    ]
  },
  {
    "objectID": "mod_version-control.html#overview",
    "href": "mod_version-control.html#overview",
    "title": "Version Control",
    "section": "",
    "text": "Synthesis projects require an intensely collaborative team spirit complemented by an emphasis on quantitative rigor. While other modules focus on elements of each of these, version control is a useful tool that lives at their intersection. We will shortly embark on a deep dive into version control but broadly it is a method of tracking changes to files (especially code “scripts”) that lends itself remarkably well to coding in groups. The LTER Network Office has a Scientific Computing Team that developed a robust workshop-style set of materials aimed at teaching version control to LTER-funded synthesis working groups. Rather than re-invent these materials for the purposes of SSECR, we will work through parts of these workshop materials.",
    "crumbs": [
      "Phase I -- Prepare",
      "Version Control"
    ]
  },
  {
    "objectID": "mod_version-control.html#learning-objectives",
    "href": "mod_version-control.html#learning-objectives",
    "title": "Version Control",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nDescribe how version control facilitates code collaboration\nNavigate GitHub via a web browser\nCreate and edit a repository through GitHub\nDefine fundamental git vocabulary\nSketch the RStudio-to-GitHub order of operations\nUse RStudio, Git, and GitHub to collaborate with version control",
    "crumbs": [
      "Phase I -- Prepare",
      "Version Control"
    ]
  },
  {
    "objectID": "mod_version-control.html#preparation",
    "href": "mod_version-control.html#preparation",
    "title": "Version Control",
    "section": "Preparation",
    "text": "Preparation\nComplete the “Workshop Preparation” steps identified in the linked workshop (see below).",
    "crumbs": [
      "Phase I -- Prepare",
      "Version Control"
    ]
  },
  {
    "objectID": "mod_version-control.html#lter-scicomp-workshop-materials",
    "href": "mod_version-control.html#lter-scicomp-workshop-materials",
    "title": "Version Control",
    "section": "LTER SciComp Workshop Materials",
    "text": "LTER SciComp Workshop Materials\nThe workshop materials we will be working through live here but for convenience we have also embedded the workshop directly into the SSECR course website (see below).",
    "crumbs": [
      "Phase I -- Prepare",
      "Version Control"
    ]
  },
  {
    "objectID": "mod_version-control.html#collaborating-with-git",
    "href": "mod_version-control.html#collaborating-with-git",
    "title": "Version Control",
    "section": "Collaborating with Git",
    "text": "Collaborating with Git\nIt is important to remember that while Git is a phenomenal tool for collaboration, it is not Google Docs! You can work together but you cannot work simultaneously in the same files. Working at the same time is how merge conflicts happen which can be a huge pain to untangle after the fact. Fortunately, avoiding merge conflicts is relatively simple! Here are a few strategies for avoiding conflicts.\n\nSeparate ScriptsSeparate ShiftsForksBranchesSingle Author\n\n\nAt it’s simplest, you can make a separate script for each group member and have each of you work exclusively in your own script. If no one ever works in your script you will never have a merge conflict even if you are working in your script at the same time as someone else is working in theirs.\nYou can do this by all working on separate scripts that are trying to do the same thing or you can delegate a particular script in the workflow to a single person (e.g., one person is the only one allowed to edit the ‘data wrangling’ script, another is the only one allowed to edit the ‘analysis’ script, etc.)\nRecommendation: Worth Discussing!\n\n\nYou might also decide to work together on the same scripts and just stagger the time that you are doing stuff so that all of your changes are made, committed, and pushed before the next person begins work. This is a particularly nice option if you have people in different time zones because someone in Maine can work on code likely before another team member living in Oregon has even woken up much less started working on code.\nFor this to work you will need to communicate extensively with the rest of your team so that you are absolutely sure that you won’t start working before someone else has finished their edits.\nRecommendation: Worth Discussing!\n\n\nGitHub does offer a “fork” feature where people can make a copy of a given repository that they then ‘own’. Forks are connected to the source repository and you can open a pull request to get the edits from one fork into the source repository.\nThis may sound like a perfect fit for collaboration but in reality it introduces significant hurdles! Consider the following:\n\nIt is difficult to know where the “best” version of the code lives\n\nIt is equally likely for the primary code version to be in any group member’s fork (or the original fork). So if you want to re-run a set of analyses you’ll need to hunt down which fork the current script lives in rather than consulting a single repository in which you all work together.\n\nYou essentially gaurantee significant merge conflicts\n\nIf everyone is working independently and submitting pull requests to merge back into the main repository you all but ensure that people will make different edits that GitHub then doesn’t know how to resolve. The pull request will tell you that there are merge conflicts but you still need to fix them yourself–and now that fixing effort must be done in someone else’s fork of the repository.\n\nIt’s not the intended use of GitHub forks\n\nForks are intended for when you want to take a set of code and then “go your own way” with that code base. While there is a mechanism for contributing those edits back to the main repository it’s really better used when you never intend to do a pull request and thus don’t have to worry about eventual merge conflicts. A good example here is you might attend a workshop and decide to offer a similar workshop yourself. You could then fork the original workshop’s repository to serve as a starting point for your version and save yourself from unnecessary labor. It would be bizarre for you to suggest that your workshop should replace the original one even if did begin with that content.\nRecommendation: Don’t Do This\n\n\nGitHub also offers a “branch” feature which is similar to forks in some ways. Branches create parallel workspaces within a single repository as opposed to forks that create a copy of a repository under a different user.\nThese have the same hurdles as forks so check out the first two points in the “Work in Forks” tab. Also, just like forks, this isn’t how branches were meant to be used either! Branches exist so that you can leave some version of the code untouched while simultaneously developing some improvement in a branch. That way the user experiences a seamless upgrade while still allowing you to have a messy development period. Branches are not intended for multiple people to be working on the same things at the same time and merge conflicts are the likely outcome of using branches in this way.\nRecommendation: Don’t Do This\n\n\nYou may be tempted to just delegate all code editing to a single person in the group. While this strategy does guarantee that there will never be a merge conflict it is also deeply inequitable as it places an unfair share of the labor of the project on one person.\nPractically-speaking this also encourages an atmosphere where only one person can even read your group’s code. This makes it difficult for other group members to contribute and ultimately may cause your group to ‘miss out on’ novel insights.\nRecommendation: Don’t Do This",
    "crumbs": [
      "Phase I -- Prepare",
      "Version Control"
    ]
  },
  {
    "objectID": "mod_version-control.html#additional-resources",
    "href": "mod_version-control.html#additional-resources",
    "title": "Version Control",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\nPereira Braga, P.H. et al. Not Just for Programmers: How GitHub can Accelerate Collaborative and Reproducible Research in Ecology and Evolution. 2023. Methods in Ecology and Evolution\nGitHub. Git Cheat Sheet. 2023.\n\n\n\nWorkshops & Courses\n\nBryan J. et al. Happy Git and GitHub for the useR. 2024.\nNational Center for Ecological Analysis and Synthesis (NCEAS) Learning Hub. coreR: Intro to Git and GitHub. 2023.",
    "crumbs": [
      "Phase I -- Prepare",
      "Version Control"
    ]
  },
  {
    "objectID": "mod_team-sci.html",
    "href": "mod_team-sci.html",
    "title": "Team Science Practices",
    "section": "",
    "text": "In this module, we’ll dive into some of the literature underlying our recommended team science practices (the science of team science). We’ll also explore some of the more common types of conflicts that arise in synthesis work and ways to anticipate, prevent (when possible), and resolve those conflicts.",
    "crumbs": [
      "Phase I -- Prepare",
      "Team Science"
    ]
  },
  {
    "objectID": "mod_team-sci.html#overview",
    "href": "mod_team-sci.html#overview",
    "title": "Team Science Practices",
    "section": "",
    "text": "In this module, we’ll dive into some of the literature underlying our recommended team science practices (the science of team science). We’ll also explore some of the more common types of conflicts that arise in synthesis work and ways to anticipate, prevent (when possible), and resolve those conflicts.",
    "crumbs": [
      "Phase I -- Prepare",
      "Team Science"
    ]
  },
  {
    "objectID": "mod_team-sci.html#learning-objectives",
    "href": "mod_team-sci.html#learning-objectives",
    "title": "Team Science Practices",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nInterpret and enact current best practices in team science\nIdentify different interaction styles and the effect they have on group dynamics\nIdentify benefits (and potential costs of) diverse teams\nDescribe ways to mitigate costs of diverse teams\nExplain methods for improving the experience of virtual participants on hybrid teams",
    "crumbs": [
      "Phase I -- Prepare",
      "Team Science"
    ]
  },
  {
    "objectID": "mod_team-sci.html#preparation",
    "href": "mod_team-sci.html#preparation",
    "title": "Team Science Practices",
    "section": "Preparation",
    "text": "Preparation\nEach project group should:\n\nPlan on providing a 3-5 minute project summary and update on recent progress and any questions/issues that are arising\nFinish their internal ground rules (if not already complete)",
    "crumbs": [
      "Phase I -- Prepare",
      "Team Science"
    ]
  },
  {
    "objectID": "mod_team-sci.html#networking-session",
    "href": "mod_team-sci.html#networking-session",
    "title": "Team Science Practices",
    "section": "Networking Session",
    "text": "Networking Session\n\n2024 Guests\n\n\nWe’ll begin this session’s discussion with a conversation among participants in a recent synthesis group that tackled a particularly broad disciplinary stretch. The Synthesizing population and community synchrony to understand drivers of ecological stability across LTER sites brought together mathematicians, modelers, and empiricists to apply novel analytical strategies to understanding the impact of population fluctuations.\nPanelists will briefly introduce themselves and their experience with the synthesis group, which encountered a variety of challenges in their work. Ultimately, they produced several effective and well-received papers, but they had to introduce some innovations in working group approach to get there.\n\nDr. Lauren Hallett, Associate Professor, University of Oregon\nDr. Max Castorani, Associate Professor, University of Virginia\nDr. Jonathan Walter, Senior Researcher, Center for Watershed Sciences, University of California, Davis",
    "crumbs": [
      "Phase I -- Prepare",
      "Team Science"
    ]
  },
  {
    "objectID": "mod_team-sci.html#science-of-team-science",
    "href": "mod_team-sci.html#science-of-team-science",
    "title": "Team Science Practices",
    "section": "Science of Team Science",
    "text": "Science of Team Science\nResearch in management, organizational behavior, and psychology has long focused on the performance of teams–often in military, healthcare and industrial contexts. While many aspects of this work are also relevant to scientific teams, there are some key differences having to do with differences in context, leadership, and incentives. In the early 2000’s–as collaboration in science increased–the need for empirical research into the workings of science teams became apparent. The field of “science of team science” or SiTS was launched in 2006 with a conference at the National Institutes of Health (Stokols et al. 2008).\nA National Academies study on the Science of Team Science (NRC 2015) assembled the existing evidence base and launched a flurry of research into how team composition, coordination, support, and organizational context could improve outcomes for science teams. A new National Academies study on Research and Application in Team Science is currently in progress. Our goal here is not to review the whole field, but to provide a framework for thinking about the team functioning and process and to identify some key team science practices that are supported by both research and practical experience.",
    "crumbs": [
      "Phase I -- Prepare",
      "Team Science"
    ]
  },
  {
    "objectID": "mod_team-sci.html#teams-have-a-predictable-trajectory",
    "href": "mod_team-sci.html#teams-have-a-predictable-trajectory",
    "title": "Team Science Practices",
    "section": "Teams have a Predictable Trajectory",
    "text": "Teams have a Predictable Trajectory\nCreating a team is not just a matter of putting a bunch of people in a room together. Social scientists have identified consistent patterns in the evolution of teams (Tuckman 1965, Tuckman and Jenson 1977). Knowing that this is a process nearly every team experiences may make it (at least somewhat) more comfortable.\n\n\n\n\n\nTeams that are assembled from across organizations must agree to adopt a common set of norms and processes in order to progress from storming to performing. This can feel like a detour from the science, but a modest early investment in developing shared practices pays off in the long run.",
    "crumbs": [
      "Phase I -- Prepare",
      "Team Science"
    ]
  },
  {
    "objectID": "mod_team-sci.html#instrumental-benefits-of-diverse-teams",
    "href": "mod_team-sci.html#instrumental-benefits-of-diverse-teams",
    "title": "Team Science Practices",
    "section": "Instrumental Benefits of Diverse Teams",
    "text": "Instrumental Benefits of Diverse Teams\n\n\nThere is pretty good evidence that collaborative teams produce research that is more novel and has higher impact than work produced by individuals or smaller more homogeneous groups (Lee at al. 2015, Hong and Page 2024). Wooley et al (2010) found evidence for a “collective intelligence” in teams, which is not strongly correlated with the average or maximum individual intelligence of group members but is correlated with the average social sensitivity of group members, the equality in distribution of conversational turn-taking, and the proportion of females in the group.\nSimilarly, in a study of 6.6 million medical research papers, Yang et al. found that mixed gender teams consistently produced more novel and more impactful products. In another bibliographic analysis Abbasi and Jaafari (2013) found that inter-institute and inter-university collaborations resulted in higher-impact publications. Interestingly, the result was much weaker for international collaborations.\n\n\n\n\nNovelty peaks at a team size of about 6 authors (examinining only authors from a single private uinversity. From: Lee et al. 2014\n\n\n\n\n\n\nMixed-gender teams are more likely to produce novel papers than same-gender teams at all team sizes. Mixed-gender teams are also more likely to publish an upper-tail paper than same-gender teams by as much as 14.6%, depending on team sizes.\n\n\n\n\n It seems reasonable to expect that the effects of cultural and economic diversity on teams would be similar to that of gender diversity, but those factors remain harder to parse at this scale. In any case, the bump in creativity or publishing impact is only a happy side effect of assembling a diverse team. The real reason to do so is that it allows us to tackle bigger questions, makes our findings more relevant, our science more fun, and our world more fair. What it does not do (at least in our experience) is make the process faster!",
    "crumbs": [
      "Phase I -- Prepare",
      "Team Science"
    ]
  },
  {
    "objectID": "mod_team-sci.html#a-more-nuanced-view-emerges",
    "href": "mod_team-sci.html#a-more-nuanced-view-emerges",
    "title": "Team Science Practices",
    "section": "A More Nuanced View Emerges",
    "text": "A More Nuanced View Emerges\nThe paradox of team science is that the very factors that slow progress may be exactly the factors that generate new insight – Milliken and Martins’ (1996) double-edged sword. The pressing question becomes not: “Does diversity impove team performance?” but rather: “How and when does diversity improve team performance?”\n\nWhat mechanisms are responsible for the diversity effect?\n\n\n Information Elaboration\n\n\nThe categorization-elaboration model (CEM, van Knippenberg et al. 2004) proposed that information elaboration—-that is, the exchange, discussion, and integration of task-relevant information and perspectives, was responsible for many of the benefits attributed to diverse groups. But later researchers found there were a few necessary conditions for cognitive elaboration to take place and for groups to reap the benefits. Only when team members brought a learning goal orientation to their work and when they remained open to revising their original ideas (Nederveen Pieterse 2013) did diversity improve team performance.\n\n\n Avoiding ‘groupthink’\n\n\nWe are all familiar with the “we’ve always done it this way” effect that can happen when a group of people have been working together for a while. By introducing people from new fields, laboratories, or cultures, that complacent thinking is disrupted. Often, the very act of justifying why we do something the way we do can invite a rethinking and improvement.\n\n\n Metacognition\n\n\nMetacognition, or “thinking about thinking” requires individuals to reflect and articulate their process for acheiving new knowledge. What information goes in? Is information missing? How should it be analyzed and interpreted? Are those conclusions justified?\n\n\n Enhanced group scanning ability and consideration of alternative solutions\n\n\nA science team may include members from different research disciplines, sectors, geographies or cultures. Along each of those axes, team members will have different personal networks and be more (or less) familiar with different literatures, models, communities, tools, and solutions. Collectively, the group has a much broader range of information to draw on…but only if group members feel empowered to contribute.\n\n\n Better task completion and more efficient use of resources\n\n\n“Many hands make light work” the saying goes. Think of a meta-analysis where 10 group members can each read 30 papers instead of 1 individual reading 300 papers. Dividing the workload can speed up the process, but only if there is an efficiant way to manage dividing the work and then bringing the results back together again. Similarly, relying on a few skilled coders can be much more efficient than each individual writing their own code, but unless the group has a mechanism for getting broad input on key decisions, they will lose the value created by bringing together a larger group.\n\n\nGroup discussionConditions and practices support team functioning\n\n\nMeet as project teams in breakout groups for 10 minutes. Each group comes up with one practice that they could include in their group practice guidelines to support each of the above mechanisms. In large group, each project group descibes one of their practices and how they think it will help.\n\n\nIn order for the above mechanisms to operate, teams need to cultivate conditions that encourage all members to contribute at the times and in the ways that they are most skilled and effective. These include:\n\nCultivate a learning goal orientation rather than a product goal orientation. Expect to learn from one another and adapt your expectations and plans (Nederveen et al. 2013).\nRemain open to revising assumptions and world views. When divergent positions are met only with resistance, groupthink gains the upper hand.\nCognitive trust is the rational belief that group members can and will deliver on their portion of the work. When it isn’t present, group members tend to pull back on their own contributions. Good coordination supports cognitive trust by providing clarity and accountability about who agreed to do what work and whether they delivered. It ensures that contributions can be appropiately credited and that work isn’t unnecessarily duplicated. Effective coordination and facilitation make space for all group members to engage.\n\nFast and slow processors can be accommodated by making space for written as well as verbal contributions and allowing “thinking time” before expecting a response.\nVisual, auditory and kinesthetic learners take in information (and are more or less fluent) in different formats. Try to provide key information (and allow input) in more than one format.\nThose with caregiving responsibilities may have unpredictable availability and shorter periods of concentrated effort. A task management system (such as GitHub Projects or Trello) that breaks down tasks into manageable chunks and provides necessary contexts can help them contribute without as much task-switching cost.\nStratgies for managing different geographies include virtual meetings, pulsed contributing times, and asynchronous editing of shared documents.\n\nAffective trust is the belief (usually grounded in common experience) that group members have your best interests in mind. Some strategies for building it include:\n\nSpend social time together - meals, activities when in-person, but also, don’t skimp on icebreakers and check-ins when virtual\nPay attention to mutual respect and speaking time. Explicitly acknowledge and credit new ideas as they come up.\nBe willing to look foolish. Ask the “dumb” questions that surface unquestioned assumptions. When some (leaders especially) make themselves vulnerable, it provides safety for others to do so.\nConsider assigning a vibes-keeper to track when the group becomes impatient, offended, or disengaged.\nSpend time early to talk through various perspectives on the question that may be present in the group.\nAttend to conflicts as they arise.",
    "crumbs": [
      "Phase I -- Prepare",
      "Team Science"
    ]
  },
  {
    "objectID": "mod_team-sci.html#all-contribute-some-none-contribute-all",
    "href": "mod_team-sci.html#all-contribute-some-none-contribute-all",
    "title": "Team Science Practices",
    "section": "All Contribute Some; None Contribute All",
    "text": "All Contribute Some; None Contribute All\n\n\n\nLotrecchiano et al. Figure 1. Overlapping and intersecting competencies across the domains. Colors define the primary domain for each competency.",
    "crumbs": [
      "Phase I -- Prepare",
      "Team Science"
    ]
  },
  {
    "objectID": "mod_team-sci.html#navigating-conflict",
    "href": "mod_team-sci.html#navigating-conflict",
    "title": "Team Science Practices",
    "section": "Navigating Conflict",
    "text": "Navigating Conflict\n\nAll those diverse perspectives will inevitably lead to some degree of conflict, which can be stressful and intimidating. But identifying the components of a conflict and understanding what is at stake for each party can reveal unexpected solutions. In any event, ignoring conflict will only make it more difficult to resolve later. Expect and plan for conflict with the following approaches.\n\nDevelop working agreements (code of conduct + principles)\nDecide how conflicts will be handled together; write down your plan\nHave a backup plan / exit strategy\nReview the policy and how it’s working regularly; make changes\nKnow what systems and supports are available to you and use them.\nBuild capacity for productive struggle and generative conflict with low stakes practice.\n\nIn spite of your best efforts, conflicts will still arise. The satisfaction triangle is a simple and useful approach to identifying the elements contributing to conflicts for each of the participants. When you notice a conflict surfacing, try to work through it using the following steps:\n\nAcknowledge the conflict.\nCommit to a joint problem solving process.\nDescribe your view of the conflict and its impact on you, focusing on specific events and observable behaviors. Ask for their view. Really listen.\nIdentify areas of agreement and disagreement. State preferences and requests clearly. Together, prioritize what to address first.\nFocus on the future with positive intent. Work together to find a solution. Keep communication open. Tend relationships.",
    "crumbs": [
      "Phase I -- Prepare",
      "Team Science"
    ]
  },
  {
    "objectID": "mod_team-sci.html#scenarios",
    "href": "mod_team-sci.html#scenarios",
    "title": "Team Science Practices",
    "section": "Scenarios",
    "text": "Scenarios\nFor the following set of scenarios, you will be randomly assigned to one of three breakout groups for a 20-minute discussion. After you join the group:\n\nBriefly introduce yourselves\nNavigate to the notes document in the cohort folder.\nChoose a notetaker, a reporter (may be the same as the notetaker or not), and a timekeeper (to keep the group moving through the questions).\nA volunteer reads the scenario out loud for the group\nTake a minute or so to think quietly\nDiscuss\nTry to articulate the emotional aspects, the procedural aspects, and the structural aspects of the conflict from each participant’s point of view.\nWhat interventions might have prevented , or could now resolve, the conflict?\n\n\nOvercommitmentAuthorshipResearch Approach\n\n\nAlix, a postdoc in the lab of Dr. Chambers, a well-known hydrologist, enjoys data analysis and offers to assemble and clean the group’s data using MatLab. Although only one other person in the group uses MatLab, Alix seems confident that the program is well-suited for what they need to do and that they can devote enough time to do the work.\nThat winter, Alix’s father is diagnosed with cancer and Alix starts taking him to chemo treatments. They are already half-way through the analysis and it would take weeks for someone else to start over. They hate to let the group down and say that they will be able to work on data while waiting around in doctors’ offices. But as Winter drags into Spring and nothing has happened, the group gets frustrated.\nTerry starts working on the data in R, but struggles to make sense of Alix’s notes. By Fall, their one year of funding runs out and they have nothing to show for it.\nDiscussion questions:\n\nWhich sides of the satisfaction triangle are the different players in this scenario inhabiting? What needs are in conflict?\nIdentify at least three points at which someone could have intervened to change this outcome and better satisfy the needs of each party\nWho and how?\nWhy might they have chosen not to?\nWhat practices could make it easier for them to take action?\n\n\n\nA synthesis group is attempting to connect data on municipal services with wildlife sightings at the urban-wildlands interface. Maria, a PhD student in sociology, is mainly focused on collecting municipal data. During a whole group discussion on where the group might find additional data sources for wildlife observations, she mentions a paper she had seen recently on scraping data from social media.\nLater that week, when the wildlife subgroup meets, they get excited about finding wildlife sightings through NextDoor and begin pursuing the idea. It works out better than they could have hoped and James, an assistant professor in computer science who is preparing his tenure package, spearheads a paper on their use of the technique. At the next full group meeting, he mentions that it is ready to be submitted.\nMaria would have appreciated the chance to work with James and is disappointed and angry that no one mentioned the paper to her.\nDiscussion questions:\n\nWhere is Maria on the satisfaction traingle? Where is James?\nWhat should Maria do now?\nWhat procedural, emotional, and substantive needs might each player have in this situation? How might they balance those needs in seeking a solution?\nHow might the group have avoided the situation in the first place?\n\n\n\nArturo, a senior climate scientist with a large lab, brings two postdocs and a grad student to the first working group meeting of the Causes of Harmful Algal Blooms working group. In deciding which scenarios to evaluate, Arturo posits that temperature, rainfall, and growing degree-days will be the primary drivers and offers to assign one to each of his lab members. Arturo is motivated to participate in the working group because it builds on his prior research on climatic influences on HABs. He and his lab members see the working group as a great learning opportunity for early career researchers.\nJasmine, an assistant professor in agronomy with a heavy teaching load, thinks that fertilizer rate and timing of application will be major drivers. Jasmine is highly motivated to play a leadership role in the working group and needs publications to advance her career, but her time is limited. She has time to find relevant datasets, but can’t commit to doing the analysis and doesn’t have students or postdocs to assign to the project. Looking at the role of coastal agriculture on HABs is central to why she said yes to joining the group.\nDiscussion questions\n\nWhat needs have each of the players in this conflict expressed?\nHow should the group go about allocating effort n order to tackle the research project and meet those needs?\nIs it fair to ask one of Arturo’s postdocs to work outside their field? Why might they want (or not want) to do that?\nWhat other solutions might they consider?\n\n\n\n\n\nWhole Group Discussion\nEach group describes their analysis of their assigned situation and a proposed solution/intervention. Discuss commonalities and any ideas/proposals for updating group ground rules or process plan.\n\n\nReferences\n\nAbbasi, A., and A. Jaafari. Research impact and scholars’ geographical diversity. Journal of Informetrics. 2013\nGraffius, Scott M. Leverage the Phases of Team Development — Forming, Storming, Norming, Performing, and Adjourning — to Help Your Teams be Happier and More Productive: 2023 Update. 2023\nFiore et al. Interdisciplinarity as Teamwork: How the Science of Teams Can Inform Team Science. Small Group Research. 2008\nHong, L., & Page, S. E. Individual selection criteria for optimal team composition. Theory and Decision. 2024\nLee, You-Na, John P. Walsh, Jian Wang. Creativity in scientific teams: Unpacking novelty and impact. Research Policy. 2015\nLotrecchiano, Gaetano R., Deborah DiazGranados, Jennifer Sprecher, Wayne T. McCormack, Damayanthi Ranwala, Kevin Wooten, Daniel Lackland, Heather Billings, and Allan R. Brasier. Individual and team competencies in translational teams. Journal of Clinical and Translational Science. 2021\n\nMilliken, Frances J. and Luis L. Martins. Searching for Common Threads: Understanding the Multiple Effects of Diversity in Organizational Groups. The Academy of Management Review. 1996\nNederveen Pieterse, A., D. van Knippenberg, and D. van Dierendonck. Cultural Diversity and Team Performance: The Role of Team Member Goal Orientation. Academy of Management Journal. 2013\nStokols, D., S. Misra, R. P. Moser, K. L. Hall, and B. K. Taylor. The Ecology of Team Science: Understanding Contextual Influences on Transdisciplinary Collaboration. American Journal of Preventive Medicine *35:S96–S115.2008\nTuckman, B. W. Developmental Sequence in Small Groups. Psychological Bulletin 1965.\nTuckman, B. W., & Jensen, M. A. C. Stages of Small-Group Development Revisited. Group and Organizational Studies 1977\nvan Knippenberg, D., De Dreu, C. K. W., & Homan, A. C. Work Group Diversity and Group Performance: An Integrative Model and Research Agenda. Journal of Applied Psychology 2004\nWoolley, A. W., C. F. Chabris, A. Pentland, N. Hashmi, and T. W. Malone. Evidence for a Collective Intelligence Factor in the Performance of Human Groups. Science *330:686–688.2010\nYang, Y., T. Y. Tian, T. K. Woodruff, B. F. Jones, and B. Uzzi. Gender-diverse teams produce more novel and higher-impact scientific ideas. PNAS. 2022",
    "crumbs": [
      "Phase I -- Prepare",
      "Team Science"
    ]
  },
  {
    "objectID": "mod_team-sci.html#additional-resources",
    "href": "mod_team-sci.html#additional-resources",
    "title": "Team Science Practices",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\nBates, A.E. et al., Overcome Imposter Syndrome: Contribute to Working Groups and Build Strong Networks. Biological Conservation. 2024.\nDeutsch, L., et al., Leading Inter- and Transdisciplinary Research: Lessons from Applying Theories of Change to a Strategic Research Program. Environmental Science & Policy. 2021.\nFolger, J.P., Poole, M.S., & Stutman, R.K. (2017). Working Through Conflict: Strategies for Relationships, Groups, and Organizations (8th ed.). Routledge. 2017\nFarrell, K.J. et al., Training Macrosystems Scientists Requires Both Interpersonal and Technical Skills. Frontiers in Ecology and the Environment. 2021\nGaynor, K.M., et al., Ten Simple Rules to Cultivate Belonging in Collaborative Data Science Research Teams. PLoS Computational Biology. 2022\nHampton, S.E. & Parker, J.N. Collaboration and Productivity in Scientific Synthesis. BioScience. 2011\nLowman, H. et al., Collaborative consortia can boost postdoctoral workforce development. PNAS. 2024\nPeterson, D.M., et al., Team Science: A Syllabus for Success on Big Projects. Ecology and Evolution. 2023\n\n\n\nWorkshops & Courses\n\nLiberatore, A. Developing a Successful Team: Concepts and Strategies to Navigate Change and Conflict Together. 2024.\nNational Center for Ecological Analysis and Synthesis (NCEAS) Learning Hub & Delta Stewardship Council. Open Science Synthesis: Team Science for Synthesis. 2023.\nDelta Stewardship Council. Open Science Synthesis: Thinking Preferences. 2021.\n\n\n\nWebsites\n\nLahr, Heather How to effectively engage teams in interdisciplinary collaborations. 2024",
    "crumbs": [
      "Phase I -- Prepare",
      "Team Science"
    ]
  },
  {
    "objectID": "mod_spatial.html",
    "href": "mod_spatial.html",
    "title": "Working with Spatial Data",
    "section": "",
    "text": "Synthesis projects often have need of spatial datasets. At its simplest, it can be helpful to have a map of the original project locations including in the synthesis dataset. In more complex instances you want to extract spatial data within a certain area of sampling locations. Regardless of ‘why’ you’re using spatial data, it may come up during your primary or synthesis work and thus deserves consideration in this course’s materials. There are many modes of working with spatial data, and not all of these tools require coding literacy but for consistency with the rest of the modules this module will focus on scripted approaches to interacting with spatial data.",
    "crumbs": [
      "Bonus Modules",
      "Spatial Data"
    ]
  },
  {
    "objectID": "mod_spatial.html#overview",
    "href": "mod_spatial.html#overview",
    "title": "Working with Spatial Data",
    "section": "",
    "text": "Synthesis projects often have need of spatial datasets. At its simplest, it can be helpful to have a map of the original project locations including in the synthesis dataset. In more complex instances you want to extract spatial data within a certain area of sampling locations. Regardless of ‘why’ you’re using spatial data, it may come up during your primary or synthesis work and thus deserves consideration in this course’s materials. There are many modes of working with spatial data, and not all of these tools require coding literacy but for consistency with the rest of the modules this module will focus on scripted approaches to interacting with spatial data.",
    "crumbs": [
      "Bonus Modules",
      "Spatial Data"
    ]
  },
  {
    "objectID": "mod_spatial.html#learning-objectives",
    "href": "mod_spatial.html#learning-objectives",
    "title": "Working with Spatial Data",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this topic you will be able to:\n\nDefine the difference between the two major types of spatial data\nManipulate spatial data with R\nCreate maps using spatial data\nIntegrate spatial data with tabular data",
    "crumbs": [
      "Bonus Modules",
      "Spatial Data"
    ]
  },
  {
    "objectID": "mod_spatial.html#preparation",
    "href": "mod_spatial.html#preparation",
    "title": "Working with Spatial Data",
    "section": "Preparation",
    "text": "Preparation\nThis is a “bonus” module and thus was created for asynchronous learners. There is no suggested preparatory work.",
    "crumbs": [
      "Bonus Modules",
      "Spatial Data"
    ]
  },
  {
    "objectID": "mod_spatial.html#networking-session",
    "href": "mod_spatial.html#networking-session",
    "title": "Working with Spatial Data",
    "section": "Networking Session",
    "text": "Networking Session\n\n2024 Guests\n\n\nThis was a bonus module for the 2024-25 cohort and so did not include a networking session.",
    "crumbs": [
      "Bonus Modules",
      "Spatial Data"
    ]
  },
  {
    "objectID": "mod_spatial.html#needed-packages",
    "href": "mod_spatial.html#needed-packages",
    "title": "Working with Spatial Data",
    "section": "Needed Packages",
    "text": "Needed Packages\nIf you’d like to follow along with the code chunks included throughout this module, you’ll need to install the following packages:\n\n# Note that these lines only need to be run once per computer\n## So you can skip this step if you've installed these before\ninstall.packages(\"tidyverse\")\ninstall.packages(\"sf\")\ninstall.packages(\"terra\")\ninstall.packages(\"maps\")\ninstall.packages(\"exactextractr\")",
    "crumbs": [
      "Bonus Modules",
      "Spatial Data"
    ]
  },
  {
    "objectID": "mod_spatial.html#types-of-spatial-data",
    "href": "mod_spatial.html#types-of-spatial-data",
    "title": "Working with Spatial Data",
    "section": "Types of Spatial Data",
    "text": "Types of Spatial Data\nThere are two main types of spatial data: vector and raster. Both types (and the packages they require) are described in the tabs below.\n\nVector DataRaster Data\n\n\nVector data are stored as polygons. Essentially vector data are a set of points and–sometimes–the lines between them that define the edges of a shape. They may store additional data that is retained in a semi-tabular format that relates to the polygon(s) but isn’t directly stored in them.\nCommon vector data types include shape files or GeoJSONs.\n\n# Load needed library\nlibrary(sf)\n\n# Read in shapefile\n1nc_poly &lt;- sf::st_read(dsn = file.path(\"data\", \"nc_borders.shp\"))\n\n\n1\n\nNote that even though we’re only specifying the “.shp” file in this function you must also have the associated files in that same folder. In this case that includes a “.dbf”, “.prj”, and “.shx”, though in other contexts you may have others.\n\n\n\n\nOnce you have read in the shapefile, you can check its structure as you would any other data object. Note that the object has both the ‘data.frame’ class and the ‘sf’ (“simple features”) class. In this case, the shapefile relates to counties in North Carolina and some associated demographic data in those counties.\n\n# Check structure\nstr(nc_poly)\n\nClasses 'sf' and 'data.frame':  100 obs. of  15 variables:\n $ AREA     : num  0.114 0.061 0.143 0.07 0.153 0.097 0.062 0.091 0.118 0.124 ...\n $ PERIMETER: num  1.44 1.23 1.63 2.97 2.21 ...\n $ CNTY_    : num  1825 1827 1828 1831 1832 ...\n $ CNTY_ID  : num  1825 1827 1828 1831 1832 ...\n $ NAME     : chr  \"Ashe\" \"Alleghany\" \"Surry\" \"Currituck\" ...\n $ FIPS     : chr  \"37009\" \"37005\" \"37171\" \"37053\" ...\n $ FIPSNO   : num  37009 37005 37171 37053 37131 ...\n $ CRESS_ID : int  5 3 86 27 66 46 15 37 93 85 ...\n $ BIR74    : num  1091 487 3188 508 1421 ...\n $ SID74    : num  1 0 5 1 9 7 0 0 4 1 ...\n $ NWBIR74  : num  10 10 208 123 1066 ...\n $ BIR79    : num  1364 542 3616 830 1606 ...\n $ SID79    : num  0 3 6 2 3 5 2 2 2 5 ...\n $ NWBIR79  : num  19 12 260 145 1197 ...\n $ geometry :sfc_MULTIPOLYGON of length 100; first list element: List of 1\n  ..$ :List of 1\n  .. ..$ : num [1:27, 1:2] -81.5 -81.5 -81.6 -81.6 -81.7 ...\n  ..- attr(*, \"class\")= chr [1:3] \"XY\" \"MULTIPOLYGON\" \"sfg\"\n - attr(*, \"sf_column\")= chr \"geometry\"\n - attr(*, \"agr\")= Factor w/ 3 levels \"constant\",\"aggregate\",..: NA NA NA NA NA NA NA NA NA NA ...\n  ..- attr(*, \"names\")= chr [1:14] \"AREA\" \"PERIMETER\" \"CNTY_\" \"CNTY_ID\" ...\n\n\nIf desired, we could make a simple R base plot-style map. In this case we’ll do it based on just the county areas so that the polygons are filled with a color corresponding to how large the county is.\n\n# Make a graph\nplot(nc_poly[\"AREA\"], axes = T)\n\n\n\n\n\n\n\n\n\n\nRaster data are stored as values in pixels. The resolution (i.e., size of the pixels) may differ among rasters but in all cases the data are stored at a per-pixel level.\nCommon raster data types include GeoTIFFs (.tif) and NetCDF (.nc) files.\n\n# Load needed library\nlibrary(terra)\n\n# Read in raster\nnc_pixel &lt;- terra::rast(x = file.path(\"data\", \"nc_elevation.tif\"))\n\nOnce you’ve read in the raster file you can check it’s structure as you would any other object but the resulting output is much less informative than for other object classes.\n\n# Check structure\nstr(nc_pixel)\n\nS4 class 'SpatRaster' [package \"terra\"]\n\n\nRegardless, now that we have the raster loaded we can make a simple graph to check out what sort of data is stored in it. In this case, each pixel is 3 arcseconds on each side (~0.0002° latitude/longitude) and contains the elevation (in meters) of that pixel.\n\n# Make a graph\nterra::plot(nc_pixel)",
    "crumbs": [
      "Bonus Modules",
      "Spatial Data"
    ]
  },
  {
    "objectID": "mod_spatial.html#coordinate-reference-systems",
    "href": "mod_spatial.html#coordinate-reference-systems",
    "title": "Working with Spatial Data",
    "section": "Coordinate Reference Systems",
    "text": "Coordinate Reference Systems\nA fundamental problem in spatial data is how to project data collected on a nearly spherical planet onto a two-dimensional plane. This has been solved–or at least clarified–by the use of Coordinate Reference Systems (a.k.a. “CRS”). All spatial data have a CRS that is explicitly identified in the data and/or the metadata because the data are not interpretable without knowing which CRS is used.\nThe CRS defines the following information:\n\nDatum – model for shape of the Earth including the starting coordinate pair and angular units that together define any particular point on the planet\n\nNote that there can be global datums that work for any region of the world and local datums that only work for a particular area\n\nProjection – math for the transformation to get from a round planet to a flat map\nAdditional parameters – any other information necessary to support the projection\n\nE.g., the coordinates at the center of the map\n\n\nSome people use the analogy of peeling a citrus fruit and flattening the peel to describe the components of a CRS. The datum is the choice between a lemon or a grapefruit (i.e., the shape of the not-quite-spherical object) while the projection is the instructions for taking the complete peel and flattening it.\nYou can check and transform the CRS in any scripted language that allows the loading of spatial data though the specifics do differ between the types of spatial data we introduced earlier.\n\nVector CRSRaster CRS\n\n\nFor vector data we can check the CRS with other functions from the sf library. It can be a little difficult to parse all of the information that returns but essentially it is most important that the CRS match that of any other spatial data with which we are working.\n\n# Check CRS\nsf::st_crs(x = nc_poly)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\nOnce you know the CRS, you can transform the data to another CRS if desired. This is a relatively fast operation for vector data because we’re transforming geometric data rather than potentially millions of pixels.\n\n# Transform CRS\n1nc_poly_nad83 &lt;- sf::st_transform(x = nc_poly, crs = 3083)\n\n# Re-check CRS\nsf::st_crs(nc_poly_nad83)\n\n\n1\n\nIn order to transform to a new CRS you’ll need to identify the four-digit EPSG code for the desired CRS.\n\n\n\n\nCoordinate Reference System:\n  User input: EPSG:3083 \n  wkt:\nPROJCRS[\"NAD83 / Texas Centric Albers Equal Area\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"Texas Centric Albers Equal Area\",\n        METHOD[\"Albers Equal Area\",\n            ID[\"EPSG\",9822]],\n        PARAMETER[\"Latitude of false origin\",18,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-100,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",27.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",35,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",1500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",6000000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"State-wide spatial data presentation requiring true area measurements.\"],\n        AREA[\"United States (USA) - Texas.\"],\n        BBOX[25.83,-106.66,36.5,-93.5]],\n    ID[\"EPSG\",3083]]\n\n\n\n\nFor raster data we can check the CRS with other functions from the terra library. It can be a little difficult to parse all of the information that returns but essentially it is most important that the CRS match that of any other spatial data with which we are working.\n\n# Check CRS\nterra::crs(nc_pixel)\n\n[1] \"GEOGCRS[\\\"WGS 84\\\",\\n    ENSEMBLE[\\\"World Geodetic System 1984 ensemble\\\",\\n        MEMBER[\\\"World Geodetic System 1984 (Transit)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G730)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G873)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1150)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1674)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1762)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G2139)\\\"],\\n        ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n        ENSEMBLEACCURACY[2.0]],\\n    PRIMEM[\\\"Greenwich\\\",0,\\n        ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    CS[ellipsoidal,2],\\n        AXIS[\\\"geodetic latitude (Lat)\\\",north,\\n            ORDER[1],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        AXIS[\\\"geodetic longitude (Lon)\\\",east,\\n            ORDER[2],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    USAGE[\\n        SCOPE[\\\"Horizontal component of 3D system.\\\"],\\n        AREA[\\\"World.\\\"],\\n        BBOX[-90,-180,90,180]],\\n    ID[\\\"EPSG\\\",4326]]\"\n\n\nAs with vector data, if desired you can transform the data to another CRS. However, unlike vector data, transforming the CRS of raster data is very computationally intense. If at all possible you should avoid re-projecting rasters. If you must re-project, consider doing so in an environment with greater computing power than a typical laptop. Also, you should export a new raster in your preferred CRS after transforming so that you reduce the likelihood that you need to re-project again later in the lifecylce of your project.\nIn the interests of making this website reasonably quick to re-build, the following code chunk is not actually evaluated but is the correct syntax for this operation.\n\n# Transform CRS\nnc_pixel_nad83 &lt;- terra::project(x = nc_pixel, y = \"epsg:3083\")\n\n# Re-check CRS\nterra::crs(nc_pixel_nad83)",
    "crumbs": [
      "Bonus Modules",
      "Spatial Data"
    ]
  },
  {
    "objectID": "mod_spatial.html#making-maps",
    "href": "mod_spatial.html#making-maps",
    "title": "Working with Spatial Data",
    "section": "Making Maps",
    "text": "Making Maps\nNow that we’ve covered the main types of spatial data as well as how to check the CRS (and transform if needed) we’re ready to make maps! For consistency with other modules on data visualization, we’ll use ggplot2 to make our maps but note that base R also supports map making and there are many useful tutorials elsewhere on making maps in that framework.\nThe maps package includes some useful national and US state borders so we’ll begin by preparing an object that combines both sets of borders.\n\n# Load library\nlibrary(maps)\n\n# Make 'borders' object\nborders &lt;- sf::st_as_sf(maps::map(database = \"world\", plot = F, fill = T)) %&gt;%\n  dplyr::bind_rows(sf::st_as_sf(maps::map(database = \"state\", plot = F, fill = T)))\n\nNote that the simplest way of making a map that includes a raster is to coerce that raster into a dataframe. To do this we will translate each pixel’s geographic coordinates into X and Y values.\n\nnc_pixel_df &lt;- as.data.frame(nc_pixel, xy = T) %&gt;% \n    # Rename the 'actual' data layer more clearly\n    dplyr::rename(elevation_m = SRTMGL3_NC.003_SRTMGL3_DEM_doy2000042_aid0001)\n\nWith the borders object and our modified raster in hand, we can now make a map that includes useful context for state/nation borders. Synthesis projects often cover a larger geographic extent than primary projects so this is particularly useful in ways it might not be for primary research.\n\n# Load library\nlibrary(ggplot2)\n\n# Make map\nggplot(borders) +\n1  geom_sf(fill = \"gray95\") +\n2  coord_sf(xlim = c(-70, -90), ylim = c(30, 40), expand = F) +\n  geom_tile(data = nc_pixel_df, aes(x = x, y = y, fill = elevation_m)) +\n  labs(x = \"Longitude\", y = \"Latitude\")\n\n\n1\n\nThis line is filling our nation polygons with a pale gray (helps to differentiate from ocean)\n\n2\n\nHere we set the map extent so that we’re only getting our region of interest\n\n\n\n\n\n\n\nFrom here we can make additional ggplot2-style modifications as/if needed. This variant of map-making supports adding tabular data objects as well (though they would require separate geometries). Many of the LTER Network Office-funded groups that make maps include points for specific study locations along with a raster layer for an environmental / land cover characteristic that is particularly relevant to their research question and/or hypotheses.",
    "crumbs": [
      "Bonus Modules",
      "Spatial Data"
    ]
  },
  {
    "objectID": "mod_spatial.html#extracting-spatial-data",
    "href": "mod_spatial.html#extracting-spatial-data",
    "title": "Working with Spatial Data",
    "section": "Extracting Spatial Data",
    "text": "Extracting Spatial Data\nBy far the most common spatial operation that LNO-funded synthesis working groups want to perform is extraction of some spatial covariate data within their region of interest. “Extraction” here includes (1) the actual gathering of values from the desired area, (2) summarization of those values, and (3) attaching those summarized values to an existing tabular dataset for further analysis/visualization. As with any coding task there are many ways of accomplishing this end but we’ll focus on one method in the following code chunks: extraction in R via the exactextractr package.\nThis package expects that you’ll want to extract raster data within a the borders described in some type of vector data. If you want the values in all the pixels of a GeoTIFF that fall inside the boundary defined by a shapefile, tools in this package will be helpful.\nWe’ll begin by making a simpler version of our North Carolina vector data. This ensures that the extraction is as fast as possible for demonstrative purposes while still being replicable for you.\n\n# Simplify the vector data\nnc_poly_simp &lt;- nc_poly %&gt;% \n  dplyr::filter(NAME %in% c(\"Wake\", \"Swain\")) %&gt;% \n  dplyr::select(NAME, AREA)\n\n# Check structure to demonstrate simplicity\n1dplyr::glimpse(nc_poly_simp)\n\n\n1\n\nNote that even though we used select to remove all but one column, the geometry information is retained!\n\n\n\n\nRows: 2\nColumns: 3\n$ NAME     &lt;chr&gt; \"Wake\", \"Swain\"\n$ AREA     &lt;dbl&gt; 0.219, 0.141\n$ geometry &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((-78.92082 3..., MULTIPOLYGON (((-83.3317 35..…\n\n\nNow let’s use this simplified object and extract elevation for our counties of interest (normally we’d likely do this for all counties but the process is the same).\n\n# Load needed libraries\nlibrary(exactextractr)\nlibrary(purrr)\n\n# Perform extraction\n1extracted_df &lt;- exactextractr::exact_extract(x = nc_pixel, y = nc_poly_simp,\n2                                             include_cols = c(\"NAME\", \"AREA\"),\n3                                             progress = F) %&gt;%\n  # Collapse to a dataframe\n4  purrr::list_rbind(x = .)\n\n# Check structure\ndplyr::glimpse(extracted_df)\n\n\n1\n\nNote that functions like this one assume that both spatial data objects use the same CRS. We checked that earlier so we’re good but remember to include that check every time you do something like this!\n\n2\n\nAll column names specified here from the vector data (see the y argument) will be retained in the output. Otherwise only the extracted value and coverage fraction are included.\n\n3\n\nThis argument controls whether a progress bar is included. Extremely useful when you have many polygons / the extraction takes a long time!\n\n4\n\nThe default output of this function is a list with one dataframe of extracted values per polygon in your vector data so we’ll unlist to a dataframe for ease of future operations\n\n\n\n\nRows: 521,671\nColumns: 4\n$ NAME              &lt;chr&gt; \"Wake\", \"Wake\", \"Wake\", \"Wake\", \"Wake\", \"Wake\", \"Wak…\n$ AREA              &lt;dbl&gt; 0.219, 0.219, 0.219, 0.219, 0.219, 0.219, 0.219, 0.2…\n$ value             &lt;dbl&gt; 95, 97, 100, 100, 100, 101, 103, 105, 110, 111, 111,…\n$ coverage_fraction &lt;dbl&gt; 0.027789708, 0.084839255, 0.141893625, 0.198947996, …\n\n\nIn the above output we can see that it has extracted the elevation of every pixel within each of our counties of interest and provided us with the percentage of that pixel that is covered by the polygon (i.e., by the shapefile). We can now summarize this however we’d like and–eventually–join it back onto the county data via the column(s) we specified should be retained from the original vector data.",
    "crumbs": [
      "Bonus Modules",
      "Spatial Data"
    ]
  },
  {
    "objectID": "mod_spatial.html#additional-resources",
    "href": "mod_spatial.html#additional-resources",
    "title": "Working with Spatial Data",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\n\n\n\n\nWorkshops & Courses\n\nThe Carpentries. Introduction to Geospatial Raster and Vector Data with R. 2024.\nThe Carpentries. Introduction to R for Geospatial Data. 2024.\nKing, R. Spatial Data Visualization. 2024.\nFlower, J. Introduction to Rasters with terra. 2024.\nClark, S.J., et al. Spatial and Image Data Using GeoPandas. 2023.\n\n\n\nWebsites\n\nNASA. AppEEARS Portal",
    "crumbs": [
      "Bonus Modules",
      "Spatial Data"
    ]
  },
  {
    "objectID": "mod_reports.html",
    "href": "mod_reports.html",
    "title": "Reproducible Reports",
    "section": "",
    "text": "At this point in the course, we anticipate that you’re likely approaching the end of your team’s synthesis project (see our suggested milestones page for more information). As the end of your project and the course as a whole nears, it might be valuable for your group to consider how you can reproducibly document all of the work you’ve been doing for the last several months. “Computational notebooks” (e.g., Quarto documents, Jupyter Notebooks, R Markdown files, etc.) can be a reproducible way of documenting your results in a format that allows you to leverage both your technical skills and your scientific communication skills.\nThis module focuses on the structure and content of these notebooks from a primarily technical lens, so please consult the communicating findings module for the team science perspective.",
    "crumbs": [
      "Phase IV -- Magnify",
      "Reproducible Reports"
    ]
  },
  {
    "objectID": "mod_reports.html#overview",
    "href": "mod_reports.html#overview",
    "title": "Reproducible Reports",
    "section": "",
    "text": "At this point in the course, we anticipate that you’re likely approaching the end of your team’s synthesis project (see our suggested milestones page for more information). As the end of your project and the course as a whole nears, it might be valuable for your group to consider how you can reproducibly document all of the work you’ve been doing for the last several months. “Computational notebooks” (e.g., Quarto documents, Jupyter Notebooks, R Markdown files, etc.) can be a reproducible way of documenting your results in a format that allows you to leverage both your technical skills and your scientific communication skills.\nThis module focuses on the structure and content of these notebooks from a primarily technical lens, so please consult the communicating findings module for the team science perspective.",
    "crumbs": [
      "Phase IV -- Magnify",
      "Reproducible Reports"
    ]
  },
  {
    "objectID": "mod_reports.html#learning-objectives",
    "href": "mod_reports.html#learning-objectives",
    "title": "Reproducible Reports",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nDescribe contexts where computational notebooks are useful\nIdentify the three fundamental elements of a typical notebook\nUse Markdown syntax to accomplish text styling\nCreate notebooks that include a blend of plain text and embedded code chunks\nMake a Quarto website\nDefine the purpose of GitHub Actions\nDeploy a notebook with GitHub Pages",
    "crumbs": [
      "Phase IV -- Magnify",
      "Reproducible Reports"
    ]
  },
  {
    "objectID": "mod_reports.html#preparation",
    "href": "mod_reports.html#preparation",
    "title": "Reproducible Reports",
    "section": "Preparation",
    "text": "Preparation\nTBD (To Be Determined)",
    "crumbs": [
      "Phase IV -- Magnify",
      "Reproducible Reports"
    ]
  },
  {
    "objectID": "mod_reports.html#networking-session",
    "href": "mod_reports.html#networking-session",
    "title": "Reproducible Reports",
    "section": "Networking Session",
    "text": "Networking Session\n\n2024 Guests\n\n\nTo Be Determined!",
    "crumbs": [
      "Phase IV -- Magnify",
      "Reproducible Reports"
    ]
  },
  {
    "objectID": "mod_reports.html#notebook-structure-value",
    "href": "mod_reports.html#notebook-structure-value",
    "title": "Reproducible Reports",
    "section": "Notebook Structure & Value",
    "text": "Notebook Structure & Value\nFiles that combine plain text with embedded code chunks are an excellent way of reproducibly documenting workflows and faciliating conversations about said workflows (or their outputs). Examples of notebook files include Quarto documents, Jupyter Notebooks, and R Markdown files. Regardless of the specific type, all of them function in the same way. Each of them allows you to use code chunks in the same way that you might use a typical script but between the code chunks you can add–and format–plain, human-readable text. Arguably you could do this with comments in a script but this format is built around the idea that this plain text is intended to be interpretable without any prior coding experience. The plain text can be formatted with Markdown syntax (we’ll discuss that in greater depth later) but even unformatted text outside of code chunks is visually ‘easier on the eyes’ than comment lines in scripts.\nAnother shared facet of notebook interfaces is that they are meant to be “rendered” (a.k.a. “knit”) to produce a different file type that translates code chunks and Markdown-formatted text into something that looks much more similar to what you might produce in Microsoft Word or a Google Doc. Typically such files are rendered into PDFs or HTMLs though there are other output options. These rendered files can then be shared (and opened) outside of coding platforms and thus make their content even more accessible to non-coders.\nIn synthesis work these reports can be especially valuable because your team may include those with a wealth of discipline insight but not necessarily coding literacy. Creating reports with embedded code can enable these collaborators to engage more fully than they might otherwise be able to if there was essentially a minimum threshold of coding literacy required in order to contribute. These reports can also be useful documentation of past coding decisions and serve as reminders for judgement calls for which no one in the team remembers the rationale.\n\nStructural Elements\nTypically, these notebooks have three structural components:\n\nYAML\n\nPronounced ‘YEAH-mull’\n\nPlain text\n\nPossibly formatted with Markdown syntax\n\nEmbedded code chunks\n\n\nYAMLMarkdown TextCode Chunks\n\n\nThe YAML (Yet Another Markup Language) defines document-level metadata. Most fundamentally, the YAML defines what file type will be produced when the report is rendered. It can also be used to define the top-level title, author, and date information. Finally, it can change the default options for code chunks throughout the document (more on code chunk options elsewhere).\nDifferent notebook file types will specify the YAML differently but in both Quarto documents and R Markdown files, the YAML is defined in the first few lines of the report and starts/ends with a line containing three hyphens. This looks something like this:\n---\ntitle: \"Reproducible Reports\"\noutput: html_document\n---\n\n\nThe text outside of the YAML and code chunks is plain text that accepts Markdown syntax to accomplish text format tweaks. Dedicated text-formatting software (e.g., Microsoft Word, Gmail, etc.) provides buttons and hot keys for these sorts of format alterations but many programming IDEs do not provide such user interface elements.1 Markdown syntax is used to support this same functionality.\nSome fundamental Markdown options include:\n\n**bold text**  bold text\n_italic text_  italic text\n`code text`  code text\n[hyperlinked text](https://lter.github.io/ssecr/mod_reports.html)  hyperlinked text\n\nFor a more complete glossary of fundamental Markdown syntax options see here. You may also want to explore the ‘back end’ of this course’s website as every page is built using computational notebook files.\n\n\nThe code chunks embedded in notebooks are essentially a fragmented script containing runable code. These chunks may contain code and/or comments and share an environment with one another when rendered (i.e., if you load a particular library in one chunk you’ll be able to use functions from that library in subsequent chunks). When used in concert with the Markdown text in a given notebook the code chunks can be used to effectively demonstrate a workflow while providing as much human-readable context as is desired.\nIn Quarto documents, code chunks look like this2:\n\\```{r demo-chunk}\n1#| echo: true\n\n2# Round pi to 2 digits\nround(x = pi, digits = 2)\n\\```\n\n1\n\nYou may specify chunk-specific options using this syntax (i.e., #| option_name: option_setting). Options you want to apply to all chunks across a notebook should be specified once in the YAML and can exclude the leading #| bit of the format.\n\n2\n\nIf your Markdown text provides sufficient context you may exclude comments in code chunks but opinions differ on which is “more” appropriate\n\n\n\n\n[1] 3.14\n\n\n\n\n\n\n\nScript vs. Report Decision\nSee here for more information.",
    "crumbs": [
      "Phase IV -- Magnify",
      "Reproducible Reports"
    ]
  },
  {
    "objectID": "mod_reports.html#applications",
    "href": "mod_reports.html#applications",
    "title": "Reproducible Reports",
    "section": "Applications",
    "text": "Applications\n\nStatic PDF/HTML files\nFull manuscripts\nDeployed website",
    "crumbs": [
      "Phase IV -- Magnify",
      "Reproducible Reports"
    ]
  },
  {
    "objectID": "mod_reports.html#deployment-github",
    "href": "mod_reports.html#deployment-github",
    "title": "Reproducible Reports",
    "section": "Deployment & GitHub",
    "text": "Deployment & GitHub",
    "crumbs": [
      "Phase IV -- Magnify",
      "Reproducible Reports"
    ]
  },
  {
    "objectID": "mod_reports.html#additional-resources",
    "href": "mod_reports.html#additional-resources",
    "title": "Reproducible Reports",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\n\n\n\n\nWorkshops & Courses\n\nCsik, S. Creating Your Personal Website Using Quarto. 2024.\nNational Center for Ecological Analysis and Synthesis (NCEAS) Learning Hub. coreR: Literate Analysis with Quarto. 2023.\nMecum, B. Reproducible Papers with RMarkdown. 2019.\n\n\n\nWebsites\n\nThe Markdown Guide: Basic Syntax.\nPosit. Welcome to Quarto",
    "crumbs": [
      "Phase IV -- Magnify",
      "Reproducible Reports"
    ]
  },
  {
    "objectID": "mod_reports.html#footnotes",
    "href": "mod_reports.html#footnotes",
    "title": "Reproducible Reports",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that with the addition of the “Visual” tab in RStudio there are button options for many text format changes. Markdown syntax is still useful to know for general knowledge reasons though!↩︎\nNormally code chunks start and end with three backticks (```) but to embed this code chunk example we need to “escape” the first backtick (using a backslash) so that the notebook interprets it correctly.↩︎",
    "crumbs": [
      "Phase IV -- Magnify",
      "Reproducible Reports"
    ]
  },
  {
    "objectID": "mod_next-steps.html",
    "href": "mod_next-steps.html",
    "title": "Next Steps & Logic Models",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase IV -- Magnify",
      "Next Steps"
    ]
  },
  {
    "objectID": "mod_next-steps.html#overview",
    "href": "mod_next-steps.html#overview",
    "title": "Next Steps & Logic Models",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase IV -- Magnify",
      "Next Steps"
    ]
  },
  {
    "objectID": "mod_next-steps.html#learning-objectives",
    "href": "mod_next-steps.html#learning-objectives",
    "title": "Next Steps & Logic Models",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nIdentify needed data and target audience(s)\nArticulate connection(s) between proposed investigation and beneficial outcome\nWrite polished, funding-worthy proposals",
    "crumbs": [
      "Phase IV -- Magnify",
      "Next Steps"
    ]
  },
  {
    "objectID": "mod_next-steps.html#preparation",
    "href": "mod_next-steps.html#preparation",
    "title": "Next Steps & Logic Models",
    "section": "Preparation",
    "text": "Preparation\nTBD (To Be Determined)",
    "crumbs": [
      "Phase IV -- Magnify",
      "Next Steps"
    ]
  },
  {
    "objectID": "mod_next-steps.html#networking-session",
    "href": "mod_next-steps.html#networking-session",
    "title": "Next Steps & Logic Models",
    "section": "Networking Session",
    "text": "Networking Session\n\n2024 Guests\n\n\nTo Be Determined!",
    "crumbs": [
      "Phase IV -- Magnify",
      "Next Steps"
    ]
  },
  {
    "objectID": "mod_next-steps.html#section",
    "href": "mod_next-steps.html#section",
    "title": "Next Steps & Logic Models",
    "section": "…",
    "text": "…",
    "crumbs": [
      "Phase IV -- Magnify",
      "Next Steps"
    ]
  },
  {
    "objectID": "mod_next-steps.html#additional-resources",
    "href": "mod_next-steps.html#additional-resources",
    "title": "Next Steps & Logic Models",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\nDeutsch L., et al., Leading Inter- and Transdisciplinary Research: Lessons from Applying Theories of Change to a Strategic Research Program. 2021. Environmental Science & Policy\n\n\n\nWorkshops & Courses\n\n\n\n\n\nWebsites\n\nShabanov, I. (@Artifexx) Tweet on logic models. 2024.",
    "crumbs": [
      "Phase IV -- Magnify",
      "Next Steps"
    ]
  },
  {
    "objectID": "mod_interactivity.html",
    "href": "mod_interactivity.html",
    "title": "Creating Interactive Apps",
    "section": "",
    "text": "Shiny is a popular tool that allows users to build interactive web applications without the normally pre-requisite web development expertise. In addition to Shiny apps being simpler to build for the programmer they are often used to allow visitors to perform coding tasks without ever actually writing code. These are huge advantages because they reduce or eliminate significant technical barriers in developing truly interactive applications.\nIn synthesis contexts, Shiny can be used for a variety of valuable purposes. You can use it to develop dashboards for sharing data with related communities, allow your team to quickly “play with” exploratory graphs, or even to create a data submission portal (as is the case with some Research Coordination Networks or “RCNs”).\nNote that Shiny can be built in either R or Python ‘under the hood’ but for the purposes of this module we’ll focus on R.",
    "crumbs": [
      "Bonus Modules",
      "Interactivity"
    ]
  },
  {
    "objectID": "mod_interactivity.html#overview",
    "href": "mod_interactivity.html#overview",
    "title": "Creating Interactive Apps",
    "section": "",
    "text": "Shiny is a popular tool that allows users to build interactive web applications without the normally pre-requisite web development expertise. In addition to Shiny apps being simpler to build for the programmer they are often used to allow visitors to perform coding tasks without ever actually writing code. These are huge advantages because they reduce or eliminate significant technical barriers in developing truly interactive applications.\nIn synthesis contexts, Shiny can be used for a variety of valuable purposes. You can use it to develop dashboards for sharing data with related communities, allow your team to quickly “play with” exploratory graphs, or even to create a data submission portal (as is the case with some Research Coordination Networks or “RCNs”).\nNote that Shiny can be built in either R or Python ‘under the hood’ but for the purposes of this module we’ll focus on R.",
    "crumbs": [
      "Bonus Modules",
      "Interactivity"
    ]
  },
  {
    "objectID": "mod_interactivity.html#learning-objectives",
    "href": "mod_interactivity.html#learning-objectives",
    "title": "Creating Interactive Apps",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this topic you will be able to:\n\nDefine the three fundamental components of a Shiny app\nExplain benefits and limitations of interactive approaches to data exploration\nGenerate an interactive app with Shiny\nUse text formatting methods in a Shiny app\nExplore available Shiny layout options\nCreate a Shiny app\nDescribe (briefly) the purpose of deploying a Shiny app",
    "crumbs": [
      "Bonus Modules",
      "Interactivity"
    ]
  },
  {
    "objectID": "mod_interactivity.html#preparation",
    "href": "mod_interactivity.html#preparation",
    "title": "Creating Interactive Apps",
    "section": "Preparation",
    "text": "Preparation\nThis is a “bonus” module and thus was created for asynchronous learners. There is no suggested preparatory work.",
    "crumbs": [
      "Bonus Modules",
      "Interactivity"
    ]
  },
  {
    "objectID": "mod_interactivity.html#networking-session",
    "href": "mod_interactivity.html#networking-session",
    "title": "Creating Interactive Apps",
    "section": "Networking Session",
    "text": "Networking Session\n\n2024 Guests\n\n\nThis was a bonus module for the 2024-25 cohort and so did not include a networking session.",
    "crumbs": [
      "Bonus Modules",
      "Interactivity"
    ]
  },
  {
    "objectID": "mod_interactivity.html#needed-packages",
    "href": "mod_interactivity.html#needed-packages",
    "title": "Creating Interactive Apps",
    "section": "Needed Packages",
    "text": "Needed Packages\nIf you’d like to follow along with the code chunks included throughout this module, you’ll need to install the following packages:\n\n# Note that these lines only need to be run once per computer\n## So you can skip this step if you've installed these before\ninstall.packages(\"tidyverse\")\ninstall.packages(\"shiny\")\ninstall.packages(\"htmltools\")\ninstall.packages(\"lterdatasampler\")\n\nWe’ll load the Tidyverse meta-package here to have access to many of its useful tools when we need them later as well as the shiny package.\n\n# Load needed libraries\nlibrary(tidyverse); library(shiny)",
    "crumbs": [
      "Bonus Modules",
      "Interactivity"
    ]
  },
  {
    "objectID": "mod_interactivity.html#shiny-fundamentals",
    "href": "mod_interactivity.html#shiny-fundamentals",
    "title": "Creating Interactive Apps",
    "section": "Shiny Fundamentals",
    "text": "Shiny Fundamentals\nAll Shiny apps are composed of three pieces: a user interface (UI), a server, and a call to the shinyApp function. The user interface includes everything that the user sees and can interact with; note that this includes both inputs and outputs. The server is responsible for all code operations performed on user inputs in order to generate outputs specified in the UI. The server is not available to the user. Finally, the shinyApp function simply binds together the UI and server and creates a living app. The app appears either in your RStudio or in a new tab on a web browser depending on your settings.\nFor those of you who write your own functions, you may notice that the syntax of Shiny is very similar to the syntax of functions. If you have not already, your quality of life will benefit greatly if you turn on “rainbow parentheses” in RStudio (Tools  Global Options  Code  Display  Check “Use rainbow parentheses” box).\nLet’s consider an artificially simple Shiny app so you can get a sense for the fundamental architecture of this tool.\n\n# Define the UI\n1basic_ui &lt;- shiny::fluidPage(\n  \"Hello there!\"\n)\n\n# Define the server\n2basic_server &lt;- function(input, output){ }\n\n# Generate the app\nshiny::shinyApp(ui = basic_ui, server = basic_server)\n\n\n1\n\nThe fluidPage function is important for leaving flexibility in UI layout which we’ll explore later in the module\n\n2\n\nBecause this app has no inputs or outputs, it doesn’t need anything in the ‘server’ component (though it still does require an empty server!)\n\n\n\n\nIf you copy and run the above code, you should see an app that is a blank white page with “Hello there!” written in the top left in plain text. Congratulations, you have now made your first Shiny app! Now, your reason for exploring this module likely involves an app that actually does something but the fundamental structure of all apps–even skeletal apps like this one–is the same. More complicated apps will certainly have more content in the UI and server sections but all Shiny apps will have this tripartite structure.",
    "crumbs": [
      "Bonus Modules",
      "Interactivity"
    ]
  },
  {
    "objectID": "mod_interactivity.html#interactive-apps",
    "href": "mod_interactivity.html#interactive-apps",
    "title": "Creating Interactive Apps",
    "section": "Interactive Apps",
    "text": "Interactive Apps\nNow that we’ve covered non-reactive apps, let’s create an interactive one! It is important to remember that the user interface needs to contain both the inputs the user can make and the outputs determined by those inputs. The server will be responsible for turning the inputs into outputs but if you want your interactive app to actually show the user the interactivity you need to be careful to include the outputs in the UI.\nEssentially all Shiny UI functions use the same syntax of &lt;value class&gt;Input or &lt;value class&gt;Output. So, determining how you want the user to engage with your app is sometimes as straightforward as identifying the class of the value you want them to interact with. Shiny calls these helper functions “widgets”.\nLet’s consider an app that accepts a single number and returns the square root of that number.\n\n# Define the UI ---- \nreactive_ui &lt;- shiny::fluidPage(\n  \n  # Create input\n1  shiny::numericInput(inputId = \"num_in\",\n                      label = \"Type a number\",\n                      value = 16),\n  \n  # Include some plain text for contextualizing the output\n2  \"Square root is: \",\n  \n  # Create output\n  shiny::textOutput(outputId = \"num_out\")\n  \n) # Close UI\n\n# Define server ----\nreactive_server &lt;- function(input, output){\n  \n  # Reactively accept the input and take the square root of it\n3  root &lt;- shiny::reactive({\n4    sqrt(x = input$num_in)\n  })\n  \n  # Make that value an output of the server/app\n5  output$num_out &lt;- shiny::renderText(\n6    expr = root()\n  ) \n  \n} # Close server\n\n# Generate the app ----\nshiny::shinyApp(ui = reactive_ui, server = reactive_server)\n\n\n1\n\nNote that the argument name is capital “I” but lowercase “d”. Typing inputID is a common and frustrating source of error for Shiny app developers\n\n2\n\nEvery element of the UI–except the last one–needs to end with a comma\n\n3\n\nAll reactive elements (i.e., those that change as soon as the user changes an input) need to be specified inside of reactive with both parentheses and curly braces\n\n4\n\nThe name of this input exactly matches the inputId we defined in the UI. That it is an input is defined by our use of the numericInput widget\n\n5\n\nThe name of this output exactly matches the outputId we told the UI to expect.\n\n6\n\nReactive elements essentially become functions in their own right! So, when we want to use them, we need to include empty parentheses next to their name\n\n\n\n\nWe included a lot of footnote annotations in that code chunk to help provide context but there are a few small comments that are worthwhile to bring up at this stage.\n\nUI outputs and server renders must match\n\nThe widget you use in the UI to return an output must correspond to the function used in the server to generate that output. In this example, we use textOutput in the UI so in the server we use renderText. Essentially all widgets in Shiny use this &lt;class&gt;Output versus render&lt;Class&gt; syntax which can be a big help to visual checks that your app is written correctly. You will need to be sure that whatever the ‘class’ is, it is lowercase in the UI but title case in the server (i.e., only first letter capitalized).\n\nUse section header format\n\nThis app is relatively short but we think effectively hints at how long and convoluted purpose-built Shiny apps can easily become. So, we recommend using section headers in your Shiny app code. You can do this by putting either four hyphens or four hashtags at the end of a comment line (e.g., # Section 1 #### or # My header ----). Headings defined in this way will appear in the bottom left of the “Source” pane of RStudio next to a light orange hashtag symbol. Clicking the text in that area will open a drop-down menu showing all headings in your current file and clicking one of the other headings will instantly jump you to that heading. This can be incredibly convenient when you’re trying to navigate a several hundred line long Shiny app. While rainbow parentheses can be useful for avoiding typos within a section, section headers make it much easier to avoid typos across sections.\nIf you don’t use headings already (or your cursor is on a line before the first heading), the relevant bit of the “Source” pane will just say “(Top Level)” and will not have the golden hashtag symbol.",
    "crumbs": [
      "Bonus Modules",
      "Interactivity"
    ]
  },
  {
    "objectID": "mod_interactivity.html#including-data",
    "href": "mod_interactivity.html#including-data",
    "title": "Creating Interactive Apps",
    "section": "Including Data",
    "text": "Including Data\nYou can also use your Shiny app to work with a full data table! When running your app locally, you only need to read in the data as you normally would then run the app. By having read in the data you will ensure the object is in your environment and accessible to the app. However, keep in mind this will only work in “local” (i.e., non-deployed) contexts. See our–admittedly brief–discussion of deployment at the end of this module.\nLet’s explore an example using data about fiddler crabs (Minuca pugnax) at the Plum Island Ecosystems (PIE) LTER site from the lterdatasampler R package. The app we’re about to create will make a graph between any two (numeric) columns.\n\n# Load lterdatasampler package\nlibrary(lterdatasampler)\n\n# Load fiddler crab data\n1data(pie_crab)\n\n# Define the UI ---- \ndata_ui &lt;- shiny::fluidPage(\n  \n  # Let the user choose which the X axis\n  shiny::selectInput(inputId = \"x_vals\",\n              label = \"Choose the X-axis\",\n2              choices = setdiff(x = names(pie_crab),\n                                y = c(\"date\", \"site\", \"name\")),\n              selected = \"latitude\"),\n  \n  # Also the Y axis\n  shiny::selectInput(inputId = \"y_vals\",\n              label = \"Choose the Y-axis\",\n              choices = setdiff(x = names(pie_crab),\n                                y = c(\"date\", \"site\", \"name\")),\n              selected = \"size\"),\n  \n  # Return the desired plot\n  shiny::plotOutput(outputId = \"crab_graph\")\n              \n) # Close UI\n\n# Define the server ----\ndata_server &lt;- function(input, output){\n  \n  # Reactively identify X & Y axess\n3  picked_x &lt;- shiny::reactive({ input$x_vals })\n  picked_y &lt;- shiny::reactive({ input$y_vals })\n  \n  # Create the desired graph\n  output$crab_graph &lt;- shiny::renderPlot(\n    \n4    ggplot(pie_crab, aes(x = .data[[picked_x()]], y = .data[[picked_y()]])) +\n      geom_point(aes(fill = .data[[picked_x()]]), pch = 21, size = 2.5) +\n      labs(x = stringr::str_to_title(picked_x()),\n           y = stringr::str_to_title(picked_y())) +\n      theme_bw()\n    \n  ) # Close plot rendering\n  \n} # Close server\n\n# Generate the app ----\nshiny::shinyApp(ui = data_ui, server = data_server)\n\n\n1\n\nNote the loading of the data is done outside of the app! You can have the app load its own data but that is more complicated than this example needs to be.\n\n2\n\nTo make our life easier in the server we can exclude non-number columns\n\n3\n\nSee how we’re reactively grabbing both axes?\n\n4\n\nggplot2 requires special syntax to specify axes with quoted column names (which is how reactive Shiny elements from that widget are returned)",
    "crumbs": [
      "Bonus Modules",
      "Interactivity"
    ]
  },
  {
    "objectID": "mod_interactivity.html#layouts",
    "href": "mod_interactivity.html#layouts",
    "title": "Creating Interactive Apps",
    "section": "Layouts",
    "text": "Layouts\nExperimenting with different app layouts can be a fun step in the process of making an app that is as effective as possible! We do recommend that during app development you stick with a very simple user interface because it’ll be easier to make sure your inputs and outputs work as desired. Once you are satisfied with those elements you can relatively easily chengs the UI to help guide users through your app.\nAs implied by that preface, layouts are exclusively an element of the user interface! This is great when you have an app with a complicated server component because you won’t need to mess with that at all to get the UI looking perfect. In the examples below, we’ll generate a non-interactive app so that we can really emphasize the ‘how to’ perspective of using different layouts.\n\nSidebar\nOne of the more common Shiny UI choices is to use a sidebar. The sidebar typically takes up about one third of the width of the app while the remaining two thirds is taken up by the main panel. The sidebar can be nice place to put all the user inputs and have the outputs display in the main panel. This format allows for really clear visual separation between where you want the user to interact with the app versus where the results of their choices can be viewed.\n\n# Define the UI\nsidebar_ui &lt;- shiny::fluidPage(\n  \n  # Define the layout type\n1  shiny::sidebarLayout(\n  \n    # Define what goes in the eponymous sidebar\n    shiny::sidebarPanel(\n      \"Hello from the sidebar!\"\n2      ),\n    \n    # Define what goes in the main panel\n    shiny::mainPanel(\n      \"Hello from the main panel!\"\n3      ) ) )\n\n# Define the server\nbasic_server &lt;- function(input, output){ }\n\n# Generate the app\nshiny::shinyApp(ui = sidebar_ui, server = basic_server)\n\n\n1\n\nNotice that everything else in the UI is wrapped inside this function. If you want something above/below the sidebar vs. main panel you’ll need to put that content outside of this function’s parentheses but still in the fluidPage parentheses\n\n2\n\nBe careful not to forget this comma separating the sidebarPanel and mainPanel functions!\n\n3\n\nThree closing parentheses are needed to close the UI elements. This is why it’s really helpful to use rainbow parentheses in your coding environment!\n\n\n\n\n\n\nTab Panels\nIf you feel that your app is better represented in separate pages, tab panels may be a better layout choice! The result of this layout is a series of discrete tabs along the top of your app. If the user clicks one of them they’ll be able to look at a separate chunk of your app. Inputs in any tab are available to the app’s server and can be outputs in any tab (remember that their is a shared server so it is impossible for it to be otherwise!). Generally it may be a good idea to have inputs and outputs in the same tab so that users can see the interactive app responding to their inputs rather than needing to click back and forth among tabs to see the results of their inputs. For example, you could have an app where users choose what goes on either axis of several graph types and put each graph type on its own tab of the larger Shiny app.\n\n# Define the UI\ntabs_ui &lt;- shiny::fluidPage(\n  \n# Define the layout type\n1  shiny::tabsetPanel(\n  \n    # Define what goes in the first tab\n    shiny::tabPanel(title = \"Tab 1\",\n                    \"Hello from the first tab!\"\n2                    ),\n    \n    # And in the second\n    shiny::tabPanel(title = \"Tab 2\",\n                    \"Welcome to the second tab!\"\n                    ),\n    \n    # And so on\n    shiny::tabPanel(title = \"Tab 3\",\n                    \"Hello yet again!\"\n3                    ) ) )\n\n# Define the server\nbasic_server &lt;- function(input, output){ }\n\n# Generate the app\nshiny::shinyApp(ui = tabs_ui, server = basic_server)\n\n\n1\n\nThis function is comparable to sidebarLayout in that if you want stuff above/below the tab panel area you’ll need to be outside of this function’s parentheses but still in the fluidPage parentheses\n\n2\n\nAgain, just like the sidebarLayout subfunctions, you’ll need a comma after each UI element except the last one\n\n3\n\nHere we’re closing all of the nested UI functions\n\n\n\n\n\n\nOther Layouts\nWe just briefly covered two layout options but hopefully this is a nice indication for the kind of flexibility in user interface that you can expect of Shiny apps! For more information, check out Posit’s Shiny Application Layout Guide. That resource has some really nice examples of these and other layout options that will be well worth checking out as you begin your journey into Shiny.",
    "crumbs": [
      "Bonus Modules",
      "Interactivity"
    ]
  },
  {
    "objectID": "mod_interactivity.html#text-formatting",
    "href": "mod_interactivity.html#text-formatting",
    "title": "Creating Interactive Apps",
    "section": "Text Formatting",
    "text": "Text Formatting\nBeyond making your app have an intuitive layout it can be really helpful to be able to do even simple text formatting to assist your app’s users. For instance, you may want to use sub-headings within the same UI layout component but still want to draw a distinction between two sets of inputs. Additionally you may want to emphasize some tips for best results or hyperlink to your group’s other products. All of these can be accomplished using text formatting tools that are readily available within Shiny.\n\n# Load the `htmltools` library\nlibrary(htmltools)\n\n# Define the UI\ntext_ui &lt;- shiny::fluidPage(\n  \n  # Let's make some headings\n1  htmltools::h1(\"This is a Big Heading\"),\n  htmltools::h3(\"Smaller heading\"),\n  htmltools::h5(\"Even smaller heading!\"), \n  \n  # Now we'll format more text in various (non-heading) ways\n  htmltools::strong(\"Bold text\"),\n  \n2  htmltools::br(),\n\n  htmltools::a(href = \"https://lter.github.io/ssecr/mod_interactivity.html\",\n               \"This text is hyperlinked\",\n3               target = \"_blank\"),\n  \n  htmltools::br(),\n  \n4  htmltools::code(\"This is 'code' text\") )\n\n# Define the server\nbasic_server &lt;- function(input, output){ }\n\n# Generate the app\nshiny::shinyApp(ui = text_ui, server = basic_server)\n\n\n1\n\nHeadings (of any size) automatically include a line break after the heading text\n\n2\n\nThe br function creates a line break\n\n3\n\nWhen the target argument is set to “_blank” it will open a new tab when users click the hyperlinked text. This is ideal because if a user left your app to visit the new site they would lose all of their inputs\n\n4\n\nCode text looks like this",
    "crumbs": [
      "Bonus Modules",
      "Interactivity"
    ]
  },
  {
    "objectID": "mod_interactivity.html#deployment",
    "href": "mod_interactivity.html#deployment",
    "title": "Creating Interactive Apps",
    "section": "Deployment",
    "text": "Deployment\nWhen Shiny apps are only being used by those in your team, keeping them as a code script works well. However, if you’d like those outside of your team to be able to find your app as they would any other website you’ll need to deploy your Shiny app. This process is outside of the scope of this module but is often the end goal of Shiny app development.\nTake a look at Posit’s instructions for deployment for more details but essentially “deployment” is the process of getting your local app hosted on shinyapps.io which gives it a link that anyone can use to access/run your app on their web browser of choice.",
    "crumbs": [
      "Bonus Modules",
      "Interactivity"
    ]
  },
  {
    "objectID": "mod_interactivity.html#additional-interactivity-resources",
    "href": "mod_interactivity.html#additional-interactivity-resources",
    "title": "Creating Interactive Apps",
    "section": "Additional Interactivity Resources",
    "text": "Additional Interactivity Resources\n\nPapers & Documents\n\n\n\n\n\nWorkshops & Courses\n\nPosit. Welcome to Shiny. 2024.\nLyon, N.J., et al. Shiny Apps for Sharing Science. 2022.\n\n\n\nWebsites",
    "crumbs": [
      "Bonus Modules",
      "Interactivity"
    ]
  },
  {
    "objectID": "mod_facilitation.html",
    "href": "mod_facilitation.html",
    "title": "Inclusive Facilitation",
    "section": "",
    "text": "The success of a collaborative synthesis science project rests in no small part on the ability of the team of researchers to work effectively together and to draw on the full range of expertise, knowledge, and capacity in the group. Effective facilitation of team meetings can go a long way toward unlocking the group’s full potential. This module focuses on inclusive facilitation techniques to enable and encourage full, thoughtful, engaged participation during virtual and in person group meetings. Many of the principles underpinning these techniques can also be applied to other aspects of managing your collaboration, outside the meeting setting.",
    "crumbs": [
      "Phase II -- Plan",
      "Inclusive Facilitation"
    ]
  },
  {
    "objectID": "mod_facilitation.html#overview",
    "href": "mod_facilitation.html#overview",
    "title": "Inclusive Facilitation",
    "section": "",
    "text": "The success of a collaborative synthesis science project rests in no small part on the ability of the team of researchers to work effectively together and to draw on the full range of expertise, knowledge, and capacity in the group. Effective facilitation of team meetings can go a long way toward unlocking the group’s full potential. This module focuses on inclusive facilitation techniques to enable and encourage full, thoughtful, engaged participation during virtual and in person group meetings. Many of the principles underpinning these techniques can also be applied to other aspects of managing your collaboration, outside the meeting setting.",
    "crumbs": [
      "Phase II -- Plan",
      "Inclusive Facilitation"
    ]
  },
  {
    "objectID": "mod_facilitation.html#learning-objectives",
    "href": "mod_facilitation.html#learning-objectives",
    "title": "Inclusive Facilitation",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nDescribe benefits of encouraging full, thoughtful, engaged participation\nIdentify methods for ensuring equitable access to participation in a team setting\nIdentify one activity that privileges each thinking style",
    "crumbs": [
      "Phase II -- Plan",
      "Inclusive Facilitation"
    ]
  },
  {
    "objectID": "mod_facilitation.html#preparation",
    "href": "mod_facilitation.html#preparation",
    "title": "Inclusive Facilitation",
    "section": "Preparation",
    "text": "Preparation\nComplete Phase 1 course survey",
    "crumbs": [
      "Phase II -- Plan",
      "Inclusive Facilitation"
    ]
  },
  {
    "objectID": "mod_facilitation.html#networking-session",
    "href": "mod_facilitation.html#networking-session",
    "title": "Inclusive Facilitation",
    "section": "Networking Session",
    "text": "Networking Session\n\n2024 Guests\n\n\nInclusive facilitation is challenging, even when facilitators and participants are quite similar, but when their backgrounds, ages, or positions are different, it can be even more so. This class we’ll talk with Mack White, a graduate student who is serving as PI for the Marine Consumer Nutrient Dynamics synthesis group. He’ll address approaches for inclusive facilitation broadly, as well as the special challenges inherent in facilitating a group of colleagues who are more senior and experienced than he is.\n\nMack White, PhD student with Dr. Jennifer Rehage, Florida International University\n\n\n\n\n\n\n\n\n\n\n\nActivity: Rapid Assessment\n\n\n\nPart 1 (~3 min)\nOn your own:\n\nReflect silently on the following questions and provide input via the anonymous poll\n\nWhat’s one thing your group is doing really well to ensure all team members can participate in meetings and project work?\nWhat’s one area where your team has room to improve its inclusive facilitation practices? What could you be doing better?\n\n\nPart 2 (~7 min)\nIn pairs (from different teams):\n\nShare what’s working, what could be improved (2 mins each)\nCompare notes and share suggestions for things to try\n\n\n\n\n\n\n\n\n\nDiscussion: Rapid Assessment\n\n\n\nWhole group discussion (8 mins)\n\nWhat are some common practices that are working well?\nWhere is our learning edge?\n\nWhat are some common areas you identified where we have room to improve our inclusive facilitation practices?\n\nPlease share an aha moment or idea from your partner that you want to try in your group?\n\n\n\n\nTool Highlight: Polls & Think-Pair-Share\nPolls are useful tools for collecting input. They can be deployed synchronously or asynchronously, anonymously or not. Consider using them before a meeting to understand the group’s starting point and help shape the content. Use them during a meeting to collect real time data. Or use them after a meeting to gather feedback. Zoom’s integrated polling is simple but effective. Other options include slido, mentimeter, and google forms.\nThink-Pair-Share is a common teaching tool that is very effective for engaging groups in reflection and discussion. Starting with individual reflection suits those who like to process ideas in quiet. Discussing in pairs allows everyone to share their ideas and learn from each other before highlights are lifted up to the whole group. “1-2-4-all” is a related microstructure that can be used to rapidly share and sift ideas in any group of eight or more people. In a virtual setting, to simplify breakout room logistics, it’s easiest to just have three levels: individual, breakout groups of 2 or more people, and whole group.",
    "crumbs": [
      "Phase II -- Plan",
      "Inclusive Facilitation"
    ]
  },
  {
    "objectID": "mod_facilitation.html#benefits-of-enabling-full-participation",
    "href": "mod_facilitation.html#benefits-of-enabling-full-participation",
    "title": "Inclusive Facilitation",
    "section": "Benefits of Enabling Full Participation",
    "text": "Benefits of Enabling Full Participation\nCreating the conditions for all team members to feel welcome and able to fully participate advances diversity, equity, and inclusion in science. As we learned in the Team Science Practices module it also has instrumental benefits, as diverse teams have been shown to be more productive and to produce higher impact results than less diverse teams. In part, that productivity can be attributed to the ability of the group to elicit and work with novel ideas and approaches, allowing more innovative analysis and problem solving.\nFacilitating equitable participation also helps to unleash the full capacity of a team to get things done. Too often, potential contributors opt out of offering their skills and talents to a collaborative endeavor because they feel undervalued or unclear on how to contribute. It might not feel worth their time to try to assert an idea or opinion when it doesn’t feel welcome. A process that creates opportunities for everyone to engage and feel included can help avoid this situation.\nFinally, when the time comes for decision making, effective facilitation can ensure that the full range of questions, opinions, and concerns has been surfaced and weighed before the group makes a decision. This is critical. If you move forward with surface-level agreement, but without true alignment, commitment to the decision is likely to erode over time.",
    "crumbs": [
      "Phase II -- Plan",
      "Inclusive Facilitation"
    ]
  },
  {
    "objectID": "mod_facilitation.html#moving-from-debate-to-dialogue",
    "href": "mod_facilitation.html#moving-from-debate-to-dialogue",
    "title": "Inclusive Facilitation",
    "section": "Moving from Debate to Dialogue",
    "text": "Moving from Debate to Dialogue\nDialogue is a collaborative effort to understand and learn from each other. Debate, on the other hand, is an oppositional effort to convince the other side that you are right. Inclusive facilitation aims to support dialogue and skillful discussion. Dialogue allows groups to recognize the limits on their own and others’ individual perspectives and to strive for more coherent thought. Dialogue becomes a container for collective thinking and exploration - a process that can take teams in directions not imagined or planned. In dialogue, all views are treated as equally valid, and different views are presented as a means toward discovering a new view. Participants listen to understand one another, not to win. Complex issues are explored and shared meaning is created. When it comes time to make a decision, skillful discussion is required. Both skillful discussion and dialogue are critical to the collaborative process, and the more artfully a group can move between these two forms of discourse (and out of less productive debate and polite discussion) according to what is needed, the more effective the group will be.\n\nDebatePolite DiscussionSkillful DiscussionDialogue\n\n\n\nIntent to win\nListening to be understood\nPower struggles\nCompetition “turf war”\nLoudest wins\nIdeas judged by who says them\n\n\n\n\nIntent to protect self, others\nSurface friendly\nIdeas judged by friendships/relationships\nImpulsive, based on feeling, low data\nInfluence occurs outside the meeting\nLimited active or empathetic listening\n\n\n\n\nIntent is closure; informed decisions\nBalance influence and inquiry\nFocus on issues not personalities\nReasoning is made explicit\nAsk about assumptions without criticizing\nInfluence is based in logic and data\n\n\n\n\nIntent to build mutual understanding\nListening to understand thoughts and feelings\nAble to suspend assumptions\nEnergy used to find the right questions\n“Container” for collective thinking\nInfluence is found in shared meaning created by groups\n\n\n\n\nDavid Kantor’s Four Player Model is a helpful tool for diagnosing “stuck” patterns of communication, (including entrenched debate and polite discussion), and interceding to help shift to a more productive pattern.\nBased on over 30 years of observation and study of face-to-face communication in many groups, Kantor developed the Four Player Model and a broader theory of Structural Dynamics. The model identifies four actions of effective communication:\n\nMove - Initiate an idea, action, or direction for conversation\nFollow - Continue the direction or flow of the conversation; support a move, either by agreeing or asking for more information\nOppose - Challenge or disagree with the idea, action, or suggested direction\nBystand - Notice and articulate what’s happening in the conversation, add a neutral perspective\n\n\n\n\nEach action in a group conversation can be coded into one of these four action modes. Most of us have one of these modes that we feel most comfortable in and tend to default to in a group. The most effective conversations involve good listening and the skillful use of all four modes. Common “stuck” patterns emerge when groups are not deploying all four actions.\n\n\n\n\nSerial MovesWhat to Do\n\n\nLots of idea generation; may feel like a barrage; no clear thread, decision, or follow through\n\n\nAny of the other modes can help here, since move is the only mode being engaged.\nAdd a follow to give momentum to a particular move and steer the conversation in that direction, e.g., “Can we go back to the idea that Jose put on the table? That felt like a topic that could really use our attention. Shall we focus there?”\nOffer an effective oppose - “We’ve heard a lot of different ideas. I’d like to focus on the one Amelia laid out. I’m interested in the research question, but I don’t think machine learning is going to be the most productive approach. Can we dig in to this one?”\nUse a bystand to bring awareness to and dirsupt the dynamic - “Hey gang, we’re 20 minutes into our call and we’ve put a lot of different topics on the table. Where do we want to focus ourselves so we can walk away with some clear next steps?”\n\n\n\n\nPolite DiscussionWhat to Do\n\n\nMoves are followed with little discussion or resistance; also known as Courteous Compliance\n\n\nPrompt an effective oppose:\n\nWho sees it differently?\nWhat’s at risk here?\nWhat other angles should we consider?\n\nInvite a bystand:\n\nWhere is the group right now?\nWhat are you noticing?\nIs there an elephant in the room that needs to be named?\n\n\n\n\n\nPoint-Counterpoint / DebateWhat to Do\n\n\nIndividuals are locked in a back and forth where each move is met with resistance / opposition\n\n\nInvite a follow:\n\nWhat do you like about the proposal on the table?\nWhat do you agree with that we could build upon?\n\nCoach for a more effective oppose by inviting those who have been opposing to identify some aspect of the idea they do agree with (even if only 2%), in addition to the specific aspects they object to.\nInvite a bystand:\n\nIn addition to the two viewpoints on the table, I’d love to hear from some other perspectives.\nWhat are you noticing?\nWhat might we be missing?\n\n\n\n\n\nCovert OppositionWhat to Do\n\n\nOn the surface, moves are followed, and the conversation appears harmonious, but below the surface, people have unspoken reservations. Opposition tends to be expressed outside the bounds of the conversation or harbored as resentment.\nUneven power dynamics are often behind this pattern - group members defer to the moves of those with more power or seniority.\n\n\nInvite those with more power to experiment with following or bystanding to open up space for other players to make a move.\nPrompt a transparent oppose:\n\nWho sees it differently?\nWhat’s at risk here?\nAre there some cons to the proposed idea?\n\nIf others aren’t comfortable, you can offer an oppose, e.g., by suggesting the limits of the proposed move be tested across different scenarios.\nOffer a bystand, e.g., “I want to offer a reflection from another team I was part of. On that team, we kept having meetings where it seemed like everyone was in agreement, but then we would leave, and over and over again there’d be little follow through and more than a little grousing. People’s real opinions were only coming out in side conversations outside of the meeting. We lost a lot of time and forward momentum because people didn’t feel like they could air their concerns in the larger group. Do you see that happening here? Does anyone have a suggestion for how to make this a safer space to critically discuss ideas?”",
    "crumbs": [
      "Phase II -- Plan",
      "Inclusive Facilitation"
    ]
  },
  {
    "objectID": "mod_facilitation.html#designing-meetings-for-all-thinking-styles",
    "href": "mod_facilitation.html#designing-meetings-for-all-thinking-styles",
    "title": "Inclusive Facilitation",
    "section": "Designing Meetings for All Thinking Styles",
    "text": "Designing Meetings for All Thinking Styles\nAs we learned earlier in the course, people have different thinking preferences which influence what they expect and enjoy in group processes (see Ned Herrmann’s Whole Brain Model below).\n\n\n\nNed Herrmann’s Whole Brain Model\n\n\n\n\n\n\n\n\n\n\n\nActivity: Meeting Design + Facilitation Likes & Dislikes\n\n\n\nGroup brainstorm (10 mins)\nThink about your own dominant thinking preference(s) - analytical, practical, relational, or experimental. Now think back to meetings and group processes you have been a part of. With this thinking preference in mind, what facilitation or meeting design techniques have you really appreciated? What allowed you to fully participate? Conversely, what did you find frustrating?\n\nOpen the EasyRetro collaborative whiteboard (link provided in the chat and in the notes doc)\nFind the column for the thinking preference you focused on\nAdd the specific practices, tools, or techniques that you like by clicking the grey plus sign at the top of that column - one idea per card\nFeel free to emphasize other people’s ideas by clicking the thumbs up\nAdd a comment or question to any card by clicking the speech bubble\n\nNow consider the expectations and preferences of the other thinking styles noted in the figure above and shared by your classmates. Are there any that you commonly overlook when you are planning a meeting?\n\n\n\nTool Highlight: Collaborative Whiteboards\nCollaborative whiteboards are useful tools for capturing ideas from a group during virtual meetings. They range from simple (EasyRetro, Zoom’s whiteboard function) to complex (Mural, miro). Benefits include simultaneous input, the ability to organize information into discrete, movable chunks, and the visual (and lasting) nature of the output.",
    "crumbs": [
      "Phase II -- Plan",
      "Inclusive Facilitation"
    ]
  },
  {
    "objectID": "mod_facilitation.html#ensuring-equitable-access-to-participation",
    "href": "mod_facilitation.html#ensuring-equitable-access-to-participation",
    "title": "Inclusive Facilitation",
    "section": "Ensuring Equitable Access to Participation",
    "text": "Ensuring Equitable Access to Participation\nTo tap diverse perspectives and catalyze productivity and creative problem-solving, we need to design meetings (and projects) so everyone can participate fully, rather than just a few. When tackling complex challenges, voices from the edge are often critical to uncovering new insights and approaches. Democratizing participation doesn’t have to be all about controlling the dominant voices in a group; with thoughtful planning and some simple tools, you can design any conversation so that everyone can contribute.\nA few simple techniques can help:\n\nMix up the format, e.g., combining silent reflection, round robin, breakout groups, plenary, and/or “liberating structures” (more on these below)\nOffer different channels for information sharing - verbal, nonverbal, written, visual, informal, formal\nTrack and stack who wants to speak\nInvite, amplify, and credit “quieter” voices\nUse active listening - reflect back what you think you are hearing in simple terms and check your assumptions regularly\n\nBe creative and empathetic when you design your agenda. Think about your participants and what is going to help all of them participate fully and creatively. Beyond the thinking preferences, you may also want to consider these other dimensions of diversity when planning your process design and facilitation:\n\nIntroverts vs. extroverts\nVisual, auditory and kinesthetic learners\nDisciplinary diversity\nCareer stage\nLanguage\nTime zone (for geographically distributed teams)",
    "crumbs": [
      "Phase II -- Plan",
      "Inclusive Facilitation"
    ]
  },
  {
    "objectID": "mod_facilitation.html#process-design",
    "href": "mod_facilitation.html#process-design",
    "title": "Inclusive Facilitation",
    "section": "Process Design",
    "text": "Process Design\nGood meeting design starts with understanding your purpose and objectives, as well as your participants. Once you understand why you need to meet (your overarching goal) and what you want to accomplish (the specific outcomes you are driving toward), you can turn to how you will accomplish your purpose (i.e. the agenda of activities, timings, and tech) and who will play what roles. You want participants to know their role and how to be at their best.\nA good rule of thumb is to allow 2-3x as much time to plan a meeting as its duration.",
    "crumbs": [
      "Phase II -- Plan",
      "Inclusive Facilitation"
    ]
  },
  {
    "objectID": "mod_facilitation.html#online-meetings",
    "href": "mod_facilitation.html#online-meetings",
    "title": "Inclusive Facilitation",
    "section": "Online Meetings",
    "text": "Online Meetings\nOnline meetings benefit from all the same considerations as in person meetings, plus a little extra care and planning. Keeping your team engaged is doubly challenging in a virtual setting: our computers are full of distractions (email! notifications! internet rabbit holes!) AND as the facilitator, it’s harder to tell whether participants are engaged when all you have to go on is a small video window. Managing people’s energy and attention and creating opportunities for real human connection are real challenges. On the flip side, online meetings allow distributed teams to stay connected and can provide a dynamic and rich platform for shared work.\nIn addition to the general tips above, in online settings:\n\nBe thoughtful and equitable when scheduling across time zones\nDevelop online meeting norms for your team and enforce them (e.g., use of chat, indicating you want to speak)\nAsk a team member to help you monitor the chat and assist participants with tech or connectivity challenges\nEncourage personal connection (e.g., with check ins, invitations to have video on)\nCheck engagement regularly\nProvide breaks (bio breaks, silence, invitations to step away from the screen for reflection)\nMake video optional\nTake advantage of tech tools (breakout rooms, polls, shared notes, virtual whiteboards, recording, transcription, etc.)",
    "crumbs": [
      "Phase II -- Plan",
      "Inclusive Facilitation"
    ]
  },
  {
    "objectID": "mod_facilitation.html#roles",
    "href": "mod_facilitation.html#roles",
    "title": "Inclusive Facilitation",
    "section": "Roles",
    "text": "Roles\nIt’s very difficult to both facilitate a conversation and engage fully in it as a participant. If you add taking notes on top of that, it’s sure to become overwhelming. So recruit some help. The number of roles you need to fill will depend on the size of the group and the complexity of the process. Online meetings particularly benefit from a team approach to facilitation. Share and rotate duties over time:\n\nProcess facilitator - sets tone and pace, mediates conflicts, and ensures all voices are being heard, interpersonal dynamics are positive/effective, and group is staying on task\nMeeting chair (optional) - keeps an eye on the overall vision and progress of the meeting\nTimekeeper - may also be the chair or facilitator\nTech Host - monitors chat, sets up breakout rooms, records meeting, troubleshoots technology as needed in virtual/hybrid meetings\nNotetaker - captures action items and notes, often in a google doc that can be viewed and added to by others; may also produce a meeting summary\nScribe - captures important points that can be seen in real time by the whole group, usually on a whiteboard or flipchart\nSpotter - keeps a running list of who is waiting to speak (especially in large groups or intense discussions)\nRelationship monitor - tracks group dynamics and actively works to help everyone feel included and engaged on personal and social levels, may also be the facilitator\nParticipation monitor - engineers opportunities for participation, quells interrupters, amplifies and credits the messages of quieter participants, may also be the facilitator\n\nAs you get to know your team members, you can start to match people to these different roles based on their skills and recruit them to help.",
    "crumbs": [
      "Phase II -- Plan",
      "Inclusive Facilitation"
    ]
  },
  {
    "objectID": "mod_facilitation.html#alternatives-to-conventional-meeting-structures",
    "href": "mod_facilitation.html#alternatives-to-conventional-meeting-structures",
    "title": "Inclusive Facilitation",
    "section": "Alternatives to Conventional Meeting Structures",
    "text": "Alternatives to Conventional Meeting Structures\nDifferences in thinking and learning styles, disciplinary background, power, and other dimensions of diversity mean that there’s no such thing as a one-size-fits-all approach for participatory processes. Nonetheless, we tend to default to a small set of traditional ways of sharing information and engaging people when we meet. These conventional structures are often either too limiting (presentations, status reports, and managed discussions) or too free-form and disorganized (open discussions and brainstorms) to effectively tap the wisdom of the group (Lipmanowicz and McCandless, 2014). To support the engagement of all participants, we need to break out of those traditional ways of meeting.\nBooks and websites like Liberating Structures, Gamestorming, and the Facilitator’s Guide to Participatory Decisionmaking offer dozens of alternative group processes (see Resources). Known as microstructures or knowledge games, these simple, fun activities are designed to include everyone, distribute control, and unleash creativity. One or more activities can be matched to your intended outcomes and arranged in a sequence to advance the team toward your overall goal. Liberating Structures offers a matching matrix to help you identify microstructures that could fit your needs and an app you can use to browse and assemble strings of activities. Gamestorming organizes their activities into categories (e.g. games for opening, games for decision-making) for exploration.\nHere are a few microstructures that work well for small group virtual meetings. They also work for larger groups and in person settings:\n\nTool Highlights: Microstructures\n\n\n\nMicrostructure\nThinking Preference\nPurpose\nHow It Works\n\n\n\n\nIcebreaker / check in\nRelational\nConnect as a team, start on a positive, human note\nMany versions exist, e.g., one word to describe how you are arriving; one thing you are feeling grateful for today; coolest thing you’ve learned lately; describe where you grew up without using any place names, etc.\n\n\nRound robin / go around\nAnalytical, Relational\nHear from everyone\nEveryone answers the same prompt. Alternatives to going in order: each speaker calls on the next person after they have shared - keeping track of who has / hasn’t spoken keeps people paying attention; popcorn-style - people share in the order that they feel moved to speak\n\n\n1,2,4,all\nAnalytical, Practical, Experimental, Relational\nEngage everyone in generating questions, ideas, and suggestions\nIndividual reflection; Pair share; Two pairs combine and share as a group of 4; Small groups share highlights with whole group\n\n\nMin specs\nExperimental\nSpecify simple rules the group must follow to achieve your purpose\n1,2,4,all format; Individuals brainstorm things the group must do or must not do to achieve its purpose; Share in pairs or small groups; Pare the list down to the minimum set of rules you could follow and still achieve the purpose\n\n\nAffinity Map\nAnalytical, Relational\nSurface ideas, detect patterns, and analyze\nBrainstorm ideas using sticky notes on a wall or virtual whiteboard; Cluster into categories; If useful, prioritize within categories\n\n\nBrainwriting\nAnalytical, Practical, Experimental, Relational\nSurface and elaborate ideas\n(1) Brainstorm ideas in a google doc or virtual whiteboard (or on index cards in person); (2) Read and add to each other’s ideas; (3) Discuss\n\n\nWhat, So What, Now What\nAnalytical, Practical, Experimental\nMake sense of past progress or experiences and decide on future actions\nWhat - As a group, compile the facts and observations relevant to the context; So What - Reflect on the facts and their implications, identify patterns, generate hypotheses; Now What - Draw conclusions - What actions make sense?\n\n\nFist to Five / Gradient of Agreement\nPractical, Relational\nAssess degree of consensus; seek closure\nUse when ready to close a discussion or make a decision; Invite participants to rate their level of agreement with a proposal on a scale of 0-5; Five fingers means “absolute, total agreement or support” and a fist means “complete opposition”\n\n\nPolling\nAnalytical, Practical\nRank alternatives\nBefore you start - clarify how you will use the results - are you gathering information or taking a vote to make a decision?; Decide how many votes per person; In person - use sticky dots; Virtually - use +1s in a google doc or a digital polling tool (e.g., Zoom, Mural, slido)\n\n\nFeasibility-Impact Matrix (see figure below)\nAnalytical, Practical, Experimental\nCompare alternatives\nDiscuss and agree on definitions for two criteria for evaluating ideas: feasibility of implementation and impact potential; Rate each idea against these two axes and map onto 2x2 grid",
    "crumbs": [
      "Phase II -- Plan",
      "Inclusive Facilitation"
    ]
  },
  {
    "objectID": "mod_facilitation.html#harvesting-content",
    "href": "mod_facilitation.html#harvesting-content",
    "title": "Inclusive Facilitation",
    "section": "Harvesting Content",
    "text": "Harvesting Content\nAs you go, and definitely before your meeting is over, engage your team in synthesizing and capturing the information that has been discussed. This helps you to deepen understanding, document your workflow and decisions, and pick up easily next time. Use a consistent system - like a running notes document linked in the calendar item. Graphics or drawings can be a valuable complement to oral and written content in making thinking visible.\n\n\n\nMaking thinking visible, Credit: Nancy Margulies, World Cafe, Flickr\n\n\nConsider using:\n\nGrids to organize information\nConceptual models or mind maps to articulate shared understanding of complex systems\nManifestos, abstracts, and other written collateral to distill ideas\n\nWhen capturing notes, try to use people’s own words; if necessary ask them to distill long or complex points into a headline you can capture. Invite them to offer corrections if the notetaker didn’t capture what they meant.\n\n\n\n\n\n\nActivity: Team Planning\n\n\n\nPart 1 (~1 min)\nOn your own:\n\nThink about an upcoming team meeting that hasn’t yet been planned\n\nWhy will you be meeting? What do you think should be the purpose of that meeting?\n\n\nPart 2 (~20 min)\nIn your project team:\n\nDecide as a group which upcoming meeting you want to focus on\nIdentify a facilitator, timekeeper, reporter for TODAY’s breakout session (not the meeting)\nUse round robin or silent google doc’ing to hear everyone’s answers to the prompt\nPlan your next meeting together (resources: EasyRetro board, tools highlighted above)\n\nAgree on the meeting purpose\nIdentify 1-3 intended outcomes\nDraft an agenda for the meeting\nWhat activities will you use to make your meeting inclusive? Can you include an activity that preferences each thinking style?\nIdentify roles and responsibilities\nWhat’s your plan for harvesting content?\nIdentify any prep work for participants and for the facilitator(s)\n\nDiscuss:\n\nHow might things get off track?\nWhat’s your plan if they do?\n\nModify your plan as needed\n\n\n\n\n\n\n\n\n\nDiscussion: Team Planning & Wrap Up\n\n\n\nWhole group discussion (10 mins)\n\nWhat activities did you identify to help make your meeting inclusive to all the thinking styles on your team?\nWhere would you like advice from the group?\nAre there other questions you are holding related to inclusive facilitation?",
    "crumbs": [
      "Phase II -- Plan",
      "Inclusive Facilitation"
    ]
  },
  {
    "objectID": "mod_facilitation.html#additional-resources",
    "href": "mod_facilitation.html#additional-resources",
    "title": "Inclusive Facilitation",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\nKhuri et al., Inclusive Practice Glossary for Facilitators. 2024.\nCravens, A.E., et al., Science facilitation: navigating the intersection of intellectual and interpersonal expertise in scientific collaboration. 2022.\nWoodley, L. et al., A guide to using virtual events to facilitate community building: Making a PACT for more engaging virtual meetings and events. 2021.\nTarallo, B. & Monlux, M. Surviving the Horror of Online Meetings: How to Facilitate Good Virtual Meetings & Manage Meeting Monsters. 2021.\nLipmanowicz & McCandless. The Surprising Power of Liberating Structures: Simple Rules to Unleash a Culture of Innovation. Liberating Structures Press. 2014.\nKaner, S. Facilitator’s Guide to Participatory Decision-Making (Revised). 2014.\nKantor, D. Reading the room: Group dynamics for coaches and leaders. John Wiley & Sons. 2012.\nGray, D. et al., Gamestorming: A Playbook for Innovators, Rulebreakers, and Changemakers. O’Reilly Media. 2010.\nBohm, D. On Dialogue. Routledge Classics. 2004.\n\n\n\nWorkshops & Courses\n\nIAF Endorsed™ Facilitation Training Programmes\nFacilitation Training Programs recommended by the Facilitator School\n\n\n\nWebsites\n\nLiberating Structures: Including and Unleashing Everyone\nGamestorming\nSciTS and Team Science Resources compiled by INSciTS, the International Network for the Science of Team Science",
    "crumbs": [
      "Phase II -- Plan",
      "Inclusive Facilitation"
    ]
  },
  {
    "objectID": "mod_data-disc.html",
    "href": "mod_data-disc.html",
    "title": "Data Discovery & Management",
    "section": "",
    "text": "Synthesis projects often begin with a few datasets that inspire the questions–and end up incorporating dozens or hundreds of others. Researchers may seek out data that resemble their initial datasets, but come from other climates, ecosystems, or cultural settings. Or they may find that they need data of a completely different kind to establish drivers and context. The best synthesizers are resourceful in their search for data, cautious in evaluating data quality and relevance, and meticulous in documenting data sources, treatments, and related analytical decisions. In this workshop, we will cover all these aspects in enough depth for participants to begin finding and assessing their own project data.",
    "crumbs": [
      "Phase I -- Prepare",
      "Data Discovery"
    ]
  },
  {
    "objectID": "mod_data-disc.html#overview",
    "href": "mod_data-disc.html#overview",
    "title": "Data Discovery & Management",
    "section": "",
    "text": "Synthesis projects often begin with a few datasets that inspire the questions–and end up incorporating dozens or hundreds of others. Researchers may seek out data that resemble their initial datasets, but come from other climates, ecosystems, or cultural settings. Or they may find that they need data of a completely different kind to establish drivers and context. The best synthesizers are resourceful in their search for data, cautious in evaluating data quality and relevance, and meticulous in documenting data sources, treatments, and related analytical decisions. In this workshop, we will cover all these aspects in enough depth for participants to begin finding and assessing their own project data.",
    "crumbs": [
      "Phase I -- Prepare",
      "Data Discovery"
    ]
  },
  {
    "objectID": "mod_data-disc.html#learning-objectives",
    "href": "mod_data-disc.html#learning-objectives",
    "title": "Data Discovery & Management",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nIdentify repositories “known for” a particular type of data\nExplain how to effectively search for data outside of specialized repositories\nCreate a data inventory for identified data that allows for easy re-finding of those data products\nPlan how to download data in a reproducible way\nPerform checks of the fundamental structure of a dataset",
    "crumbs": [
      "Phase I -- Prepare",
      "Data Discovery"
    ]
  },
  {
    "objectID": "mod_data-disc.html#preparation",
    "href": "mod_data-disc.html#preparation",
    "title": "Data Discovery & Management",
    "section": "Preparation",
    "text": "Preparation\nEach Synthesis Fellow should identify five (5) datasets of possible relevance to their project and enter the specified information into the first sheet of the “data-inventory” Google Sheet.",
    "crumbs": [
      "Phase I -- Prepare",
      "Data Discovery"
    ]
  },
  {
    "objectID": "mod_data-disc.html#networking-session",
    "href": "mod_data-disc.html#networking-session",
    "title": "Data Discovery & Management",
    "section": "Networking Session",
    "text": "Networking Session\n\n2024 Guests\n\n\nTo motivate this module and provide some beneficial context, we’re beginning with a conversation with a panel composed of people who work at various organizations with a focus on data management and production. See the tabs below for each year’s panelists and links to their professional sites.\nPanelists will briefly introduce themselves and describe their roles. They will then speak to the kinds of data available at their organization and the strengths, limitations, and quirks of those data products from a synthesis lens. Individuals not associated with data repositories will instead share their experience working with specific types of data. Time allowing, panelists will talk about their experiences working at their organizations more broadly.\n\nDr. Greg Maurer, Environmental Data Initiative (EDI) and Jornada LTER\nDr. Eric Sokol, Staff Scientist, Quantitative Ecology, National Ecological Observatory Network (NEON)\nDr. Nicole Kaplan, Computational Biologist, U.S. Department of Agriculture-Agricultural Research Service (USDA-ARS)\nDr. Steve Formel, Biologist, USGS Science, Analytics, and Synthesis Program and node manager for the Ocean Biodiversity Information System - USA (OBIS-USA) and the Global Biodiversity Information Facility US (GBIF-US)\n\n\nPre-Prepared Questions\n\nWhat policies are in place to ensure responsible use of your data?\nWhat challenges (technical and scientific) do you see in integrating data across platforms and organizations?\nAre you aware of any open sources of code useful for downloading, wrangling, or analyzing data in your repository?\nHow can young scientists and data professionals contribute to the work being done by your organizations?",
    "crumbs": [
      "Phase I -- Prepare",
      "Data Discovery"
    ]
  },
  {
    "objectID": "mod_data-disc.html#data-repositories",
    "href": "mod_data-disc.html#data-repositories",
    "title": "Data Discovery & Management",
    "section": "Data Repositories",
    "text": "Data Repositories\nThere are a lot of specialized data repositories out there. These organizations are either primarily dedicated to storing and managing data or those operations constitute a substantive proportion of their efforts. In synthesis work, you may already have some datasets in-hand at the outset but it likely that you will need to find more data to test your hypotheses. Data repositories are a great way of finding/accessing data that are relevant to your questions.\nYou’ll become familiar with many of these when you need a particular type of data and go searching for it but to help speed you along, see the list below for a non-exhaustive set of some that have proved useful to other synthesis projects in the past. They are in alphabetical order. If the “ Package” column contains the GitHub logo () then the package is available on GitHub but is not available on CRAN (or not available at time of writing).\n\n\n\nName\nDescription\n Package\n\n\n\n\nAmeriFlux\nProvides data on carbon, water, and energy fluxes in ecosystems across the Americas, aiding in climate change and carbon cycle research.\namerifluxr\n\n\nDataONE\nAggregates environmental and ecological data from global sources, focusing on biodiversity, climate, and ecosystem research.\ndataone\n\n\nEDI\nContains a wide range of ecological and environmental datasets, including long-term observational data, experimental results, and field studies from diverse ecosystems.\nEDIutils\n\n\nEES-DIVE\nThe Environmental System Science Data Infrastructure for a Virtual Ecosystem (ESS-DIVE) includes a variety of observational, experimental, modeling and other data products from a wide range of ecological and urban systems.\n–\n\n\nGBIF\nThe Global Biodiversity Information Facility (GBIF) aggregates global species occurrence data and biodiversity records, supporting research in species distribution and conservation.\nrgbif\n\n\nGoogle Earth Engine\nGoogle Earth Engine is a cloud-based geospatial analysis platform that provides access to vast amounts of satellite imagery and environmental data for monitoring and understanding changes in the Earth’s surface.\n rgee\n\n\nMicrosoft Planetary Computer\nThe Microsoft Planetary Computer is a cloud-based platform that combines global environmental datasets with advanced analytical tools to support sustainability and ecological research.\n rstac\n\n\nNASA\nProvides data on earth science, space exploration, and climate, including satellite imagery and observational data for both terrestrial and extraterrestrial studies. Nice GUI-based data download via AppEEARS.\nnasadata\n\n\nNCBI\nHosts genomic and biological data, including DNA, RNA, and protein sequences, supporting genomics and molecular biology research.\nrentrez\n\n\nNEON\nProvides ecological data from U.S. field sites, covering biodiversity, ecosystems, and environmental changes, supporting large-scale ecological research.\nneonUtilities\n\n\nNOAA\nOffers meteorological, oceanographic, and climate data, essential for understanding atmospheric conditions, marine environments, and long-term climate trends.\n EpiNOAA-R\n\n\nOpen Traits Network\nWhile not a repository per se, the Open Traits Network has compiled an extensive lists of repositories for trait data. Check out their repository inventory for trait data\n–\n\n\nUSGS\nHosts data on geology, hydrology, biology, and geography, including topographical maps and natural resource assessments.\ndataRetrieval",
    "crumbs": [
      "Phase I -- Prepare",
      "Data Discovery"
    ]
  },
  {
    "objectID": "mod_data-disc.html#general-data-searches",
    "href": "mod_data-disc.html#general-data-searches",
    "title": "Data Discovery & Management",
    "section": "General Data Searches",
    "text": "General Data Searches\nIf you don’t find what you’re looking for in a particular data repository (or want to look for data not included in one of those platforms), you might want to consider a broader search. For instance, Google is a suprisingly good resource for finding data and–for those familiar with Google Scholar for peer reviewed literature-specific Googling–there is a dataset-specific variant of Google called Google Dataset Search.\n\nSearch Operators\nVirtually all search engines support “operators” to create more effective queries (i.e., search parameters). If you don’t use operators, most systems will just return results that have any of the words in your search which is non-ideal, especially when you’re looking for very specific criteria in candidate datasets.\nSee the tabs below for some useful operators that might help narrow your dataset search even when using more general platforms.\n\nQuotesWildcardPlusORMinusSiteFile TypeIn TitleIn URL\n\n\nUse quotation marks (\"\") to search for an exact phrase. This is particularly useful when you need specific data points or exact wording.\nExample: \"reef biodiversity\"\n\n\nUse an asterisk (*) to search using a placeholder for any word or phrase in the query. This is useful for finding variations of a term.\nExample: Pinus * data\n\n\nUse a plus sign (+) to search using more than one query at the same time. This is useful when you need combinations of criteria to be met.\nExample: bat + cactus\n\n\nUse the ‘or’ operator (OR) operator to search for either one term or another. It broadens your search to include multiple terms.\nExample: \"prairie pollinator\" OR \"grassland pollinator\"\n\n\nUse a minus sign (-; a.k.a. “hyphen”) to exclude certain words from your search. Useful to filter out irrelevant results.\nExample: marine biodiversity data -fishery\n\n\nUse the site operator (site:) to search within a specific website or domain. This is helpful when you’re looking for data from a particular source.\nExample: site:.gov bird data\n\n\nUse the file type operator (filetype:) to search for data with a specific file extension. Useful to make sure the data you find is already in a format you can intteract with.\nExample: filetype:tif precipitation data\n\n\nUse the ‘in title’ operator (intitle:) to search for pages that have a specific word in the title. This can narrow down your results to more relevant pages.\nExample: intitle:\"lithology\"\n\n\nUse the ‘in URL’ operator (inurl:) to search for pages that have a specific word in the URL. This can help locate data repositories or specific datasets.\nExample: inurl:data soil chemistry",
    "crumbs": [
      "Phase I -- Prepare",
      "Data Discovery"
    ]
  },
  {
    "objectID": "mod_data-disc.html#data-inventory-value",
    "href": "mod_data-disc.html#data-inventory-value",
    "title": "Data Discovery & Management",
    "section": "Data Inventory Value",
    "text": "Data Inventory Value\nDocumenting potential datasets (and their metadata) thoroughly in a data inventory provides numerous benefits! These include:\n\nWell-documented data inventory table make it easier for researchers to find and access specific data for reproducible research\nDocumentation will help researchers to quickly understand the context, scope, and limitations of the data, reducing the time spent on preliminary data assessment\nDetailed documentation will speed up the data publication process (e.g., data provenance, the difference among methods, etc.)\n\n\n\n\n\n\n\nActivity: Data Inventory\n\n\n\nPart 1 (~25 min)\nIn your project groups:\n\nReview your data inventory Google Sheet and discuss why you chose the datasets included.\nDiscuss what key information is needed to determine if each dataset is useful for your project. The columns from the detailed data inventory sheet (Data_Inventory_V2) might help to guide the conversation.\n\nDecide on the datasets you want to use in your analysis, ensuring there are at least as many as group members. Each group member will self-assign at least one dataset.\n\nLater, each person will download their assigned dataset\n\nAfter identifying the necessary information, begin filling out the detailed data inventory Google Sheet. Focus on completing the columns that are relevant for making informed decisions.\n\nThis sheet will be shared with another group in Part 2\n\n\nPart 2 (~10 min)\n\nYou are assigned to review the detailed data inventory table (Data_Inventory_V2) from another project group\nEach group member self-assigns one dataset that you want to review.\nTry to find the exact data file to which you were assigned\nDo you agree with the information entered in the data inventory?\nIs there any information you think should be in the data inventory that wasn’t?\n\n\n\n\n\n\n\n\n\nDiscussion: Data Inventory\n\n\n\nReturn to the main room and let’s discuss (some of) the following questions:\n\nWhich elements of the data inventory table made it easier or more difficult to find the data?\nWhat challenges did you encounter while searching for the datasets?\nWhat is your plan for downloading the data?",
    "crumbs": [
      "Phase I -- Prepare",
      "Data Discovery"
    ]
  },
  {
    "objectID": "mod_data-disc.html#downloading-data",
    "href": "mod_data-disc.html#downloading-data",
    "title": "Data Discovery & Management",
    "section": "Downloading Data",
    "text": "Downloading Data\nOnce you’ve found data, filled out your data inventory, and decided which datasets you actually want, it’s time to download some of them! There are several methods you can use and it’s possible that each won’t work in all cases so it’s important to be at least somewhat familiar with several of these tools.\nMost of these methods will work regardless of the format of the data (i.e., its file extension) but the format of the data will be important when you want to ‘read in’ the data and begin to work with it.\nBelow are some example code chunks for five methods of downloading data in a scripted way. There will be contexts where only a Graphical User Interface (“GUI”; [GOO-ee]) is available but the details of that method of downloading are usually specific to the portal you’re accessing so we won’t include an artificial general case.\n\nData Entity URLR PackageBatch DownloadAPI CallCommand Line\n\n\nSometimes you might have a URL directly to a particular dataset (usually one hosted by a data repository). If you copy/paste this URL into your browser the download would automatically begin. However, we want to make our workflows entirely scripted (or close to it) so see the example below for how to download data via a data entity URL.\nThe dataset we download below is one collected at the Santa Barbara Coastal (SBC) LTER on California spiny lobster (Panulirus interruptus) populations.\n\n# Define URL as an object\n1dt_url &lt;- \"https://pasta.lternet.edu/package/data/eml/knb-lter-sbc/77/10/f32823fba432f58f66c06b589b7efac6\"\n\n# Read it into R\nlobster_df &lt;- read.csv(file = dt_url)\n\n\n1\n\nYou can typically find this URL in the repository where you found the dataset\n\n\n\n\n\n\nIf you’re quite lucky, the data you want might be stored in a repository that developed (and maintains!) an  R package. These packages may or may not be on CRAN (packages can often also be found on GitHub or Bioconductor). Typically these packages have a “vignette” that demonstrates how their functions should be used.\nConsider the following example adapted from the USGS dataRetrieval package vignette. Visit USGS National Water Dashboard interactive map to find site numbers and check data availability.\n\n# Load needed packages\n## install.packages(\"librarian\")\nlibrarian::shelf(dataRetrieval)\n\n# Set up the parameters for the Santa Ynez River site\nsiteNumber &lt;- \"11133000\"  # USGS site number for Santa Ynez River at Narrows\nparameterCd &lt;- \"00060\"    # Parameter code for discharge (cubic feet per second)\nstartDate &lt;- \"2024-01-01\" # Start date\nendDate &lt;- \"2024-01-31\"   # End date\n\n# Retrieve daily discharge data\ndischargeData &lt;- readNWISdv(siteNumber, parameterCd, startDate, endDate)\n\n# View the first few rows of the data\nhead(dischargeData)\n\n\n\nYou may want to download several data files hosted in the same repository online for different spatial/temporal replicates. You could try to use the data entity URL or an associated  package (if one exists) or you could write code to do a “batch download” where you’d just download each file using a piece of code that repeats itself as much as needed.\nThe dataset we demonstrate downloading below is NOAA weather station data. Specifically it is the Integrated Surface Data (ISD).\n\n# Specify the start/end years for which you want to download data\ntarget_years &lt;- 2000:2005\n\n# Loop across years\nfor(focal_year in target_years){\n\n  # Message a progress note\n1  message(\"Downloading data for \", focal_year)\n\n  # Assemble the URL manually\n2  focal_url &lt;- paste0( \"https://www1.ncdc.noaa.gov/pub/data/gsod/\", focal_year, \"/gsod_\", focal_year, \".tar\")\n\n  # Assemble your preferred file name once it's downloaded\n3  focal_file &lt;- paste0(\"gsod_\", focal_year, \".tar\")\n\n  # Download the data\n  utils::download.file(url = focal_url, destfile = focal_file, method = \"curl\")\n}\n\n\n1\n\nThis message isn’t required but can be nice! Downloading data can take a long time so including a progress message can re-assure you that your R session hasn’t crashed\n\n2\n\nTo create a working URL you’ll likely need to click an example data file URL and try to exactly mimic its format\n\n3\n\nThis step again isn’t required but can let you exert a useful level of control over the naming convention of your data file(s)\n\n\n\n\n\n\nIn slightly more complicated contexts, you’ll need to make a request via an Application Programming Interface (“API”). As the name might suggest, these platforms serve as a bridge between some application and code. Using such a method to download data is a–relatively–frequent occurrence in synthesis work.\nHere we’ll demonstrate an API call for NOAA’s Tides and Currents data.\n\n# Load needed packages\n## install.packages(\"librarian\")\nlibrarian::shelf(httr, jsonlite)\n\n# Define a 'custom function' to fetch desired data\n1fetch_tide &lt;- function(station_id, product = \"predictions\", datum = \"MLLW\", time_zone = \"lst_ldt\", units = \"english\", interval = \"h\", format = \"json\"){\n\n2  # Custom error flags\n\n  # Get a few key dates (relative to today)\n  yesterday &lt;- Sys.Date() - 1\n  two_days_from_now &lt;- Sys.Date() + 2\n\n  # Adjust begin/end dates\n  begin_date &lt;- format(yesterday, \"%Y%m%d\")\n  end_date &lt;- format(two_days_from_now, \"%Y%m%d\")\n  \n  # Construct the API URL\n3  tide_url &lt;- paste0(\n    \"https://api.tidesandcurrents.noaa.gov/api/prod/datagetter?\",\n    \"product=\", product,\n    \"&application=NOS.COOPS.TAC.WL\",\n    \"&begin_date=\", begin_date,\n    \"&end_date=\", end_date,\n    \"&datum=\", datum,\n    \"&station=\", station_id,\n    \"&time_zone=\", time_zone,\n    \"&units=\", units,\n    \"&interval=\", interval,\n    \"&format=\", format)\n\n  # Make the API request\n  response &lt;- httr::GET(url = tide_url)\n  \n  # If the request is successful...\n  if(httr::status_code(response) == 200){\n    \n    # Parse the JSON response\n    tide_data &lt;- jsonlite::fromJSON(httr::content(response, \"text\", encoding = \"UTF-8\"))\n\n    # And return it\n    return(tide_data)\n\n  # Otherwise...\n  } else {\n\n  # Pass the error message back to the user\n  stop(\"Failed to fetch tide data\\nStatus code: \", httr::status_code(response))\n\n  }\n}\n\n# Invoke the function\ntide_df &lt;- fetch_tide(station_id = \"9411340\")\n\n\n1\n\nWhen you do need to make an API call, a custom function is a great way of standardizing your entries. This way you only need to figure out how to do the call once and from then on you can lean on the (likely more familiar) syntax of the language in which you wrote the function!\n\n2\n\nWe’re excluding error checks for simplicity’s sake but you will want to code informative error checks. Basically you want to consider inputs to the function that would break it and pre-emptively stop the function (with an informative message) when those malformed inputs are received\n\n3\n\nJust like the batch download, we need to assemble the URL that the API is expecting\n\n\n\n\n\n\nWhile many ecologists are trained in programming languages like R or Python, some operations require the Command Line Interface (“CLI”; a.k.a. “shell”, “bash”, “terminal”, etc.). Don’t worry if you’re new to this language! There are a lot of good resources for learning the fundamentals, including The Carpentries’ workshop “The Unix Shell”.\nBelow we demonstrate download via command line for NASA OMI/Aura Sulfur Dioxide (SO2). The OMI science team produces this Level-3 Aura/OMI Global OMSO2e Data Products (0.25 degree Latitude/Longitude grids) for atmospheric analysis.\n\nStep 1: Using the “subset/Get Data” tab on the right-hand side of the data page, generate a list of file names for your specified target area and time period. Download the list of links as a TXT file named “list.txt”. Be sure to document the target area and temporal coverage you selected in your data inventory table.\n\n\nhttps://acdisc.gesdisc.eosdis.nasa.gov/opendap/HDF-EOS5/ncml/Aura_OMI_Level3/OMSO2e.003/2023/OMI-Aura_L3-OMSO2e_2023m0802_v003-2023m0804t120832.he5.ncml.nc4?ColumnAmountSO2[119:659][0:1439],lat[119:659],lon[0:1439]\nhttps://acdisc.gesdisc.eosdis.nasa.gov/opendap/HDF-EOS5/ncml/Aura_OMI_Level3/OMSO2e.003/2023/OMI-Aura_L3-OMSO2e_2023m0805_v003-2023m0807t093718.he5.ncml.nc4?ColumnAmountSO2[119:659][0:1439],lat[119:659],lon[0:1439]\nhttps://acdisc.gesdisc.eosdis.nasa.gov/opendap/HDF-EOS5/ncml/Aura_OMI_Level3/OMSO2e.003/2023/OMI-Aura_L3-OMSO2e_2023m0806_v003-2023m0809t092629.he5.ncml.nc4?ColumnAmountSO2[119:659][0:1439],lat[119:659],lon[0:1439]\nhttps://acdisc.gesdisc.eosdis.nasa.gov/opendap/HDF-EOS5/ncml/Aura_OMI_Level3/OMSO2e.003/2023/OMI-Aura_L3-OMSO2e_2023m0807_v003-2023m0809t092635.he5.ncml.nc4?ColumnAmountSO2[119:659][0:1439],lat[119:659],lon[0:1439]\nhttps://acdisc.gesdisc.eosdis.nasa.gov/opendap/HDF-EOS5/ncml/Aura_OMI_Level3/OMSO2e.003/2023/OMI-Aura_L3-OMSO2e_2023m0808_v003-2023m0810t092721.he5.ncml.nc4?ColumnAmountSO2[119:659][0:1439],lat[119:659],lon[0:1439]\nhttps://acdisc.gesdisc.eosdis.nasa.gov/opendap/HDF-EOS5/ncml/Aura_OMI_Level3/OMSO2e.003/2023/OMI-Aura_L3-OMSO2e_2023m0809_v003-2023m0811t101920.he5.ncml.nc4?ColumnAmountSO2[119:659][0:1439],lat[119:659],lon[0:1439]\n\n\nStep 2: Open a command line window and execute the wget command. Replace the placeholder for username and password with your EarthData login credentials.\n\n\nwget -nc --load-cookies ..\\.urs_cookies --save-cookies ..\\.urs_cookies --keep-session-cookies --user=XXX --password=XXX\n--content-disposition -i list.txt\n\n\nIf you encounter any issue, follow this step-by-step guide on using wget and curl specifically with the GES DISC data system.\n\n\n\n\n\n\n\n\n\n\nActivity: Data Download (~25 mins)\n\n\n\n\nEach member work on the data that you have been assigned.\nDiscuss with your group how to collaborate on coding without creating merge conflicts\n\nMany right answers here so discuss the pros/cons of each and pick one that feels best for your group!\n\nWrite a script for your group to download data using your chosen method\nZoom rooms for each download method will be available. You are encouraged to join the room that corresponds to your chosen method to discuss with others working on the same approach.\n\nIf no datasets in your group’s inventory need the download method you chose, try to run the example codes provided",
    "crumbs": [
      "Phase I -- Prepare",
      "Data Discovery"
    ]
  },
  {
    "objectID": "mod_data-disc.html#data-format-and-structure",
    "href": "mod_data-disc.html#data-format-and-structure",
    "title": "Data Discovery & Management",
    "section": "Data Format and Structure",
    "text": "Data Format and Structure\nCSV and TXT are common formats for data storage. In addition, formats like NetCDF, HDF5, Matlab, and Rdata/RDS are frequently used in research, along with spatial datasets such as geotiff, shapefiles, and raster files (refer to the spatial module for more details).\nIn R, information about data’s structure can be retrieved in a variety of ways. We’ll go ahead and load the dplyr package as well to be able to include one Tidyverse option for evaluating structure.\n\n# Load needed libraries\nlibrary(dplyr)\n\n# Define URL as an object\ndt_url &lt;- \"https://pasta.lternet.edu/package/data/eml/knb-lter-sbc/77/10/f32823fba432f58f66c06b589b7efac6\"\n\n# Read it into R\nlobster_df &lt;- read.csv(file = dt_url)\n\n# Displays the first few rows of the dataset to provide a quick overview\nhead(lobster_df)\n\n#Provides a summary of the dataset, including basic statistics for each variable\nsummary(lobster_df)\n\n# Displays the structure of the dataset, including data types and a preview of the data\nstr(lobster_df)\n\n# Offers a transposed, compact overview of the dataset's structure, data types, and values\n1dplyr::glimpse(lobster_df)\n\n# Checks if there are any missing values (NA) in the dataset\nanyNA(lobster_df)\n\n\n1\n\nThis is the Tidyverse equivalent of str. The information looks different because the data are coerced into a “tibble” behind the scenes",
    "crumbs": [
      "Phase I -- Prepare",
      "Data Discovery"
    ]
  },
  {
    "objectID": "mod_data-disc.html#additional-resources",
    "href": "mod_data-disc.html#additional-resources",
    "title": "Data Discovery & Management",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\nBritish Ecological Society (BES). Better Science Guides: Data Management Guide. 2024.\n\n\n\nWorkshops & Courses\n\nLTER Scientific Computing Team. Data Acquisition Guide for TRY and AppEEARS. 2024.\nNational Center for Ecological Analysis and Synthesis (NCEAS) Learning Hub. coreR: Data Management Essentials. 2023.\nNCEAS Learning Hub. UCSB Faculty Seminar Series: Data Management Essentials and the FAIR & CARE Principles. 2023.\nNCEAS Learning Hub. UCSB Faculty Seminar Series: Writing Data Management Plans. 2023.\n\n\n\nWebsites\n\nEnvironmental Data Initiative (EDI) Data Portal\nDataONE Data Catalog\nOcean Observatories Initiative (OOI) Data Explorer\nGlobal Biodiversity Information Facility (GBIF) Data Portal\niDigBio Digitized Specimen Portal\nLTAR Data Dashboards and Visualizations\nLTAR Group Data within the Ag Data Commons, the digital repository of the National Agricultural Library\nData Is Plural and its data list for exploring the cool datasets in various domains",
    "crumbs": [
      "Phase I -- Prepare",
      "Data Discovery"
    ]
  },
  {
    "objectID": "instructors.html",
    "href": "instructors.html",
    "title": "SSECR Design Team",
    "section": "",
    "text": "Materials like this can only be the product of a strong team lending their unique wisdom and expertise over an extended period of time. We are overjoyed to get to work with the folx identified below and could not ask for better collaborators. We’ve separated out people in our core ‘design team’ from those who provided feedback on a smaller scale but everyone listed on this page was absolutely invaluable to this effort.\nIcon legend:  = ORCID |  = website |  = GitHub |  = publications\n\n\n\nLead: Marty Downs (she/her) – \nLead: Nick J Lyon (they/them) –   \nLead: Carrie Kappel (she/her) – \nLi Kui (she/her) –  \nSarah Elmendorf (she/her) –   \nGregory Maurer (he/him) –   \n\n\n\n\n\nAngel Chen (she/her) –   \nMolly Phillips (she/they) –  \nIngrid Slette (she/her)"
  },
  {
    "objectID": "instructors.html#overview",
    "href": "instructors.html#overview",
    "title": "SSECR Design Team",
    "section": "",
    "text": "Materials like this can only be the product of a strong team lending their unique wisdom and expertise over an extended period of time. We are overjoyed to get to work with the folx identified below and could not ask for better collaborators. We’ve separated out people in our core ‘design team’ from those who provided feedback on a smaller scale but everyone listed on this page was absolutely invaluable to this effort.\nIcon legend:  = ORCID |  = website |  = GitHub |  = publications\n\n\n\nLead: Marty Downs (she/her) – \nLead: Nick J Lyon (they/them) –   \nLead: Carrie Kappel (she/her) – \nLi Kui (she/her) –  \nSarah Elmendorf (she/her) –   \nGregory Maurer (he/him) –   \n\n\n\n\n\nAngel Chen (she/her) –   \nMolly Phillips (she/they) –  \nIngrid Slette (she/her)"
  },
  {
    "objectID": "instructors.html#supporting-organizations",
    "href": "instructors.html#supporting-organizations",
    "title": "SSECR Design Team",
    "section": "Supporting Organizations",
    "text": "Supporting Organizations\n\nNational Center for Ecological Analysis and Synthesis (NCEAS)\nLong Term Ecological Research (LTER) Network Office (LNO)\nNational Science Foundation (NSF)"
  },
  {
    "objectID": "fellows.html",
    "href": "fellows.html",
    "title": "Synthesis Fellows",
    "section": "",
    "text": "We have been overjoyed to work with a wide variety of early career researchers. They represent a breadth of disciplines and ecosystems but are united by being bright, creative, passionate scientists. To ensure they receive appropriate credit for their contributions to this course we’ve invited participants to have their GitHub profiles linked to the course website on this page. This is an opt-in model and as such there may be synthesis fellows who are not listed here but were valued members of the course community.\nFellows are ordered alphabetically by last name.\nIcon legend:  = GitHub |  = ORCID |  = website\n\n2024 Cohort\n\n\n\n\n\nAltman-Kurosaki - Lodge\nLloreda - Xu\n\n\n\n\nNoam T. Altman-Kurosaki –  \nCarla López Lloreda –  \n\n\nAbigail Borgmeier –  \nEvald Maceno –  \n\n\nRichard J. Brokaw –  \nL. McKinley Nevins –  \n\n\nAshley N. Bulseco –  \nPooja Panwar –  \n\n\nAllison Case – \nSierra B. Perez –  \n\n\nFrancis A. Chaves –  \nJulianna J. Renzi –  \n\n\nJeremy A. Collings –   \nKelsey J. Solomon –  \n\n\nShu Han Gan –  \nJames W. Sturges –  \n\n\nJonathan Gewirtzman –  \nTaylor A. Walker –  \n\n\nKatherine A. Hulting –  \nJunna Wang – \n\n\nGuopeng Liang –  \nBethany L. Williams –   \n\n\nSmriti Pehim Limbu –  \nYiyang Xu –  \n\n\nJoey Krieger Lodge – \n–"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Course Overview",
    "section": "",
    "text": "Synthesis Skills for Early Career Researchers (SSECR; [SEE-ker]) is a newly-designed course organized by the Long Term Ecological Research (LTER) Network. This course aims to address the need for more comprehensive interpersonal and data skills in ecology. You can find more information on SSECR at the LTER Network page for the course.\nIf instead you’re interested in joining as a team project mentor you can find more information–and apply–here.",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#course-priorities",
    "href": "index.html#course-priorities",
    "title": "Course Overview",
    "section": "Course Priorities",
    "text": "Course Priorities\n\nSurface and test new synthesis ideas for feasibility\nPrepare more graduate students to be effective participants in/leaders of the synthesis projects\nConnect LTER graduate students across sites\nDevelop intergenerational linkages around synthesis research",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Course Overview",
    "section": "Course Description",
    "text": "Course Description\nThe course is structured around small group synthesis projects, so that lessons are immediately applied to a synthesis problem that is relevant to learners and will likely result in a publication. The course starts with an in-person launch to establish cohort cohesion and ensure that any setup issues are resolved. Participants will pitch projects to the group and assemble a team of collaborators.The ideal configuration would be 6 project teams of 4-5 students each. Prior to the start of the course, the Network Office will recruit a corps of potential project mentors, who will be matched with participant projects and who agree to meet with students approximately 4-5 times per year.\nApplicants will propose modest or exploratory synthesis projects as part of the application process. In addition to ideas stemming from participants’ own work, course mentors will make available a small library of synthesis ideas in search of execution. The in-person kickoff week will focus on cohort-building, pitching projects and assembling project teams, identifying relevant data, and getting set up on servers and collaboration tools.",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#course-scheduling",
    "href": "index.html#course-scheduling",
    "title": "Course Overview",
    "section": "Course Scheduling",
    "text": "Course Scheduling\nCourse participants meet for three hours, weekly, at the same time each week. Sessions alternate between hands-on instruction in technical and soft skills that are relevant for inclusive synthesis and team work on the chosen group projects.\nEach three-hour session will include 2 components:\n\nNetworking (~60 minutes): Presentation by one or more experienced synthesis scientists, describing why and how they conduct synthesis. This diverse group of researchers will be recruited from across the field and course participants will have ample time for discussion with each presenter.\nInstruction (~120 minutes): Each session will focus on a specific instructional topic, with technical skills, team-science skills, and communication topics interspersed throughout the year. The discussion will be limited to official course participants, but instructional materials for each topic will be available online, allowing individuals or site- or topic-based groups to follow along independently.\n\nTechnical skills will build on earlier lessons and are not intended to be stand-alone modules. The course will include social and leadership skills required to bring a synthesis project from idea to completion (or, for larger projects to completed proposal) and will include techniques for ensuring that multiple thinking and learning styles are respected and valued.\n\n\nProject groups will also meet at a time of their own choosing to work on projects. Project mentors are encouraged to participate in work sessions at least 4 times throughout the year.\n\n2024 Schedule\n\n\n\n\n\nDate\nModule\nPrimary Instructor(s)\n\n\n\n\n9/5/24\nVersion Control\nNick Lyon; Angel Chen\n\n\n9/16-20/24\nReproducibility Best Practices\nNick Lyon\n\n\n10/3/24\nData Discovery & Management\nLi Kui; Marty Downs\n\n\n10/17/24\nTeam Sciences Practices\nMarty Downs; Carrie Kappel\n\n\n10/31/24\nInclusive Facilitation\nCarrie Kappel\n\n\n11/14/24\nData Visualization & Exploration\nSarah Elmendorf; Nick Lyon\n\n\n12/5/24\nProject Management\nMarty Downs\n\n\n12/19/24\nSupporting Divergent, Emergent, and Convergent Thinking\nCarrie Kappel\n\n\n1/16/25\nData Wrangling\nNick Lyon\n\n\n1/30/25\nAuthorship & Intellectual Credit\nMarty Downs\n\n\n2/13/25\nCancelled\n\n\n\n2/27/25\nAnalysis & Modeling\nNick Lyon\n\n\n3/13/25\nNext Steps & Logic Models\nMarty Downs\n\n\n3/27/25\nCommunicating Findings\nGabriel de la Rosa\n\n\n4/10/25\nReproducible Reports\nNick Lyon\n\n\n4/24/25\nBonus Module / Optional Prep\nTBD\n\n\n5/8/25\nOptional Prep\nTBD\n\n\n5/22/25\nFinal Project Symposium\nSSECR Participants!\n\n\n5/28/25\nPublic Presentations on LTER Community Call\nSSECR Participants!",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#note-on-coding-languages",
    "href": "index.html#note-on-coding-languages",
    "title": "Course Overview",
    "section": "Note on Coding Languages",
    "text": "Note on Coding Languages\nThe example code throughout this course is in the programming language R. This language is widely used–particularly in the life sciences–but is certainly not the only option for working with data! It is not possible to have variants of this course in every coding language but even though we can’t cover them specifically, we can make some broad points about how coding languages differ that will hopefully help you adapt your thinking if you choose to use a different coding language.\nEven relatively similar coding languages almost always use different syntax. This means that the specific command to accomplish a particular action is likely to differ between languages. The interpretation of certain characters is also likely to differ. For example, R does not interpret indentation while Python requires indented code for nested operations (e.g., code inside of a loop). So, when you know one language, you might (might!) be able to apply the same order of operations but the specific functions and tools you use to accomplish that will almost certainly differ. If you use a language other than what this course uses in its example code, you should focus more on the logical flow in the following demos than on the specific packages and functions that are used.\nFor languages that are more distantly related to one another, it’s possible that differences might be go beyond mere syntax: the core logic or philosophy of the languages might differ. Learning a very different language takes time and is often better approached by starting with the philosophical differences from the language(s) with which you are already familiar. Once you understand these fundamental differences you’ll have a new framework to be able to “plug in” the new syntax which will make that much easier than trying to learn the new vocabulary at the same time as the new logical flow.",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#course-policies",
    "href": "index.html#course-policies",
    "title": "Course Overview",
    "section": "Course Policies",
    "text": "Course Policies\n\nAttendance\nIf you get sick, observe a religious holiday unaccounted for by the SSECR schedule, have to miss class for an interview, or simply don’t think you can handle class on a given day, please email the course instructors as early as possible to let us know that you won’t be in class with a (brief) explanation. This will help us to share resources we’ll cover in class with you and plan for a smaller in-class community while you are out. Our hope is that this course will be somewhere you want to attend, but we totally understand that you have many demands on your time and sometimes life happens!\nPlease keep in mind that your presence in and contributions to class are important both to your understanding of the material and the creation and maintenance of an in-class community.\n\n\nUsability, Accessibility, and Design\nWe are committed to creating a course that is inclusive in its design. If you encounter barriers, please let the instructors know immediately so that we can determine if there is a design adjustment that can be made or if an accommodation might be needed to overcome the limitations of the design. We are always happy to consider creative solutions as long as they do not compromise the intent of the learning activity. We welcome feedback that will assist us in improving the usability and experience for all students.\n\n\nArtificial Intelligence Tools\nArtificial intelligence (AI) tools are increasingly well-known and widely discussed in the context of data science. AI products can increase the efficiency of code writing and are becoming a common part of the data science landscape. For the purposes of this course, we strongly recommend that you do not use AI tools to write code. There is an under-discussed ethical consideration to the use and training of these tools in addition to their known practical limitations. However, the main reason we suggest you not use them for this class though is that leaning too heavily upon AI tools is likely to negatively impact your learning and skill acquisition.\nYou may have prior experience with some of the quantitative skills this course aims to teach but others are likely new to you. During the first steps of learning any new skill, it can be really helpful to struggle a bit in solving problems. Your efforts now will help refine your troubleshooting skills and will likely make it easier to remember how you solved a given problem the next time it arises. Over-use of AI tools can short circuit this pathway to mastery. Once you have become a proficient coder, you will be better able to identify and avoid any distortions or assumptions introduced by relying on AI.\nAI Resources\n\nPratim Ray, P. ChatGPT: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope. 2023. Internet of Things and Cyber-Physical Systems\nTrust, T. ChatGPT & Education Slide Deck. 2023. National Teaching Repository\nCsik, S. Teach Me How to Google. University of California, Santa Barbara (UCSB) Master of Environmental Data Science (MEDS) Program.\n\n\n\nName, Gender Identity, and/or Gender Expression\nYou provided a name when you first applied to be a part of the course but we will gladly honor your request to be addressed by an alternate name. We will also use whichever pronouns you identify with. Please advise us of your pronouns and/or chosen name early in the course so that we can ensure that we treat you respectfully throughout the course.\n\n\nCode of Conduct\nGroup work is a significant part of this course explicitly in the synthesis project facet as well as implicitly by the collaborative nature of many of the modules. We expect that you will be mutually respectful with one another both in and outside of class time. We will ask you questions during the course and during class is also an ideal time for you all to ask us questions that you have on course topics or policies. We don’t believe that “dumb questions” exist, and expect that you treat your peers’ questions with the respect you’d like your questions to be with. We will learn more together in an environment where we build one another up than we would in one where we fail to support one another.",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "mod_credit.html",
    "href": "mod_credit.html",
    "title": "Authorship & Intellectual Credit",
    "section": "",
    "text": "Group synthesis requires many types of contributions, from raw data to idea generation, analysis, group organization, figure development, conceptualizing, writing, and editing. Often, little of this work is directly compensated. Conflicts and misunderstandings about what contributions earn authorship arise – especially in the absence of a clearly articulated policy on intellectual credit. In this module we will discuss a few models for aligning expectations around intellectual credit and how they may differ depending on the participants and the particular product the group is developing. We’ll also take some time for project groups to develop or refine their own intellect credit policies.",
    "crumbs": [
      "Phase III -- Execute",
      "Intellectual Credit"
    ]
  },
  {
    "objectID": "mod_credit.html#overview",
    "href": "mod_credit.html#overview",
    "title": "Authorship & Intellectual Credit",
    "section": "",
    "text": "Group synthesis requires many types of contributions, from raw data to idea generation, analysis, group organization, figure development, conceptualizing, writing, and editing. Often, little of this work is directly compensated. Conflicts and misunderstandings about what contributions earn authorship arise – especially in the absence of a clearly articulated policy on intellectual credit. In this module we will discuss a few models for aligning expectations around intellectual credit and how they may differ depending on the participants and the particular product the group is developing. We’ll also take some time for project groups to develop or refine their own intellect credit policies.",
    "crumbs": [
      "Phase III -- Execute",
      "Intellectual Credit"
    ]
  },
  {
    "objectID": "mod_credit.html#learning-objectives",
    "href": "mod_credit.html#learning-objectives",
    "title": "Authorship & Intellectual Credit",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nDefine types of intellectual contributions to a synthesis project\nDescribe some common frameworks for equitable authorship decisions\nExplain benefits (or avoided costs) of making authorship decisions both collaboratively and transparently\nCreate a draft intellectual credit plan for your team",
    "crumbs": [
      "Phase III -- Execute",
      "Intellectual Credit"
    ]
  },
  {
    "objectID": "mod_credit.html#preparation",
    "href": "mod_credit.html#preparation",
    "title": "Authorship & Intellectual Credit",
    "section": "Preparation",
    "text": "Preparation\nNone required.",
    "crumbs": [
      "Phase III -- Execute",
      "Intellectual Credit"
    ]
  },
  {
    "objectID": "mod_credit.html#networking-session",
    "href": "mod_credit.html#networking-session",
    "title": "Authorship & Intellectual Credit",
    "section": "Networking Session",
    "text": "Networking Session\n\n2024 Guests\n\n\n\nJaclyn Hatala Matthes, Senior Scientist, Harvard Forest. Jackie specializes in land-atmosphere interactions, ecosystem responses to insect and climate disturbance, and scaling water and carbon processes with models. She is a co-PI of the Flux Gradient Synthesis Working Group, which brings together multiple data sources and people to better estimate the methane flux from upland systems at scale.",
    "crumbs": [
      "Phase III -- Execute",
      "Intellectual Credit"
    ]
  },
  {
    "objectID": "mod_credit.html#intellectual-credit-module-content",
    "href": "mod_credit.html#intellectual-credit-module-content",
    "title": "Authorship & Intellectual Credit",
    "section": "Intellectual Credit Module Content",
    "text": "Intellectual Credit Module Content\nIn their Code of Ethics the Ecological Society of America lays out what seems like straightforward guidance for what contributions warrant authorship on a paper:\n\n\nResearchers will claim authorship of a paper only if they have made a substantial contribution. Authorship may legitimately be claimed if researchers\n\nconceived the ideas or experimental design;\nparticipated actively in execution of the study;\nanalyzed and interpreted the data; or\nwrote the manuscript\n\n\n\nThe recommendations from the International Committee of Medical Journal Editors (ICMJE), first published in 1978, updated in 2019, and again in 2025, goes a step farther and requires all 4 of the following responsibilities:\n\n\nSubstantial contributions to the conception or design of the work; or the acquisition, analysis, or interpretation of data for the work; AND\nDrafting the work or reviewing it critically for important intellectual content; AND\nFinal approval of the version to be published; AND\nAgreement to be accountable for all aspects of the work in ensuring that questions related to the accuracy or integrity of any part of the work are appropriately investigated and resolved.\n\n\nBut what are we to do in the context of a multidisciplinary group project, where the study design design often results from multiple overlapping and free-wheeling conversations and trials? Each contributor may have very limited knowledge of the others’ fields or expertise. Fairly judging authorship, let alone “being accountable for all aspects of the work” becomes much more complicated. But readers and journal editors have a right to expect that authors can stand behind their work.\nIn such cases, the potential for misunderstanding and conflict becomes obvious. And, of course, academia’s traditionally hierarchical structures can make it especially difficult for early career researchers to advocate for themselves in the moment. Proactively developing an authorship policy can help head off intra-group conflict, which can be especially destructive for early career researchers. It also helps the group to clarify expectations, maintain accountability, and allows researchers to rehearse potentially difficult conversations before they become personal.\nOf course, it’s useful to bear in mind that human psychology also has a role to play in authorship conflicts. It is quite common for individuals to overestimate their own contribution to the work of a team, especially when the outcome is positive. Nelson et al. (2020) examined the phenomenon in the context of scientific publishing and found that authors almost universally over-estimated their own contribution to a project, at least with respect to how their team members perceived it.\n\n\n\nFig 1. Nelson et al. 2020. The sum of coauthors percent contribution to 10 published manuscripts is shown in (A). The red dash-dotted line represents the sum of coauthors’ contributions after they were given the opportunity to adjust their own percent contribution. In (B) we show the mean percent contribution assigned by coauthors to themselves in Step 1 (“Self”), percent contribution coauthors assigned to themselves after given the opportunity to adjust their contribution not to exceed 100% in Step 2 (“Self-Corrected”), and mean percent contribution assigned to authors by their coauthors (“Other”). Error bars represent standard errors of the means. **p&lt;.01, ***p&lt;.0001.\n\n\nAt each stage of a project, different factors may work to distort, conceal, or amplify the contributions of some authors or potential authors. Take a few minutes to consider each project stage and how misperceptions might arise.",
    "crumbs": [
      "Phase III -- Execute",
      "Intellectual Credit"
    ]
  },
  {
    "objectID": "mod_credit.html#group-discussion",
    "href": "mod_credit.html#group-discussion",
    "title": "Authorship & Intellectual Credit",
    "section": "Group Discussion",
    "text": "Group Discussion\nTake a few minutes to reflect on your own experiences in each of these areas before expanding each of the below panels. In each area, what types of contributions have you noticed get under- or over-valued?\n\n\n\n\n\n\nHypothesis Development\n\n\n\n\n\n\nIn group discussions, the contributions of asynchronous or virtual participants can easily be discounted, because they seem less immediate.\nSimilarly, when an idea is first contributed by an early career-stage participant, it can be overlooked until echoed by a senior participant.\nOften, a seemingly naive question ignites the process that leads to a new way of seeing the problem. In these cases, the conclusion is often remembered, but the question (and the questioner) rarely is.\n\n\n\n\n\n\n\n\n\n\nData Gathering\n\n\n\n\n\n\nData that are easily accessed and downloaded can receive less credit than when the synthesis team needs to make direct contact with the original researcher to get access or permission to use. Data should always be cited, even when they are already publicly available.\nSignificant labor goes into finding, downloading, and cleaning datasets, but it is not glamorous work and often happens in isolation. That does not make it any less essential.\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n\n\n\n\nData analysis is the “meat” of synthesis, but even here there are pitfalls. Consider the originality of the approach and the labor involved when valuing analytical contributions.\nWhile the work of doing the analysis may fall to the more quantitatively skilled team members, key insights at this stage often come from careful review by, and discussion with, field ecologists and savvy communicators. Just because someone didn’t write the R code doesn’t mean they didn’t contribute to analysis.\n\n\n\n\n\n\n\n\n\n\nWriting\n\n\n\n\n\n\nLike framing a house, framing the “story” of a paper provides essential structure and stability – even when covered by the walls and trim (or words and figures) of the built-out product.\nGetting the first few paragraphs on the page is a valuable contribution – even if not a single one of those words makes it to the final draft. Editing and expanding others’ work is far easier than writing de novo.\n\n\n\n\n\n\n\n\n\n\nEditing\n\n\n\n\n\n\nEven when all contributors are fluent writers, the job of merging disparate sections into a single voice is challenging and deserves credit.\nResolving versions, checking references, and formatting are thankless, but essential tasks in getting to a publishable product.",
    "crumbs": [
      "Phase III -- Execute",
      "Intellectual Credit"
    ]
  },
  {
    "objectID": "mod_credit.html#opt-in-v.-opt-out",
    "href": "mod_credit.html#opt-in-v.-opt-out",
    "title": "Authorship & Intellectual Credit",
    "section": "Opt-in v. Opt-Out",
    "text": "Opt-in v. Opt-Out\nThere are two major ways to approach authorship, whether you’re talking about papers, derived data, or software. Opt-in approaches require individuals to be invited to or request involvement in a product and set basic criteria for authorship. Opt-out approaches assume that anyone contributing data, or making contributions to, say, the R repo behind a package, will be credited as an author.\nSome collaboration efforts use a hybrid system, where the primary paper issuing from a data assembly effort is expected to use an opt-out model (and therefore includes all the data contributors), while subsequent papers require potential authors to opt-in and participate in developing the analysis and writing the paper.\n\n\n\n\n\n\nIt’s OK to opt-out!\n\n\n\nResearchers can be reluctant to opt-out of a paper for many reasons. They “should” have time. They need the paper for their CV. Or they are just embarrased to have misjudged their own capacity to contribute. If you find that you can’t contribute at a level that warrants authorship, the kindest thing you can do for your co-authors is to opt out.\n\n\nAuthorship is also not the only way to recognize contributors. When funding is the only contribution to the research, funders should appear in the acknowledgements section. Other contributions, such as translation, line editing, acquisition of permits, or other logistical support are also appropriate for the acknowledgements section, when the relevant individuals have not also contributed to developing the study and writing the paper. Data providers who have not contributed to study design should be credited by citing their data, in the references using a doi and appropriate bibliographic information.",
    "crumbs": [
      "Phase III -- Execute",
      "Intellectual Credit"
    ]
  },
  {
    "objectID": "mod_credit.html#how-to-develop-and-use-an-authorship-policy",
    "href": "mod_credit.html#how-to-develop-and-use-an-authorship-policy",
    "title": "Authorship & Intellectual Credit",
    "section": "How to Develop and Use an Authorship Policy",
    "text": "How to Develop and Use an Authorship Policy\nAn authorship policy will be most helpful if it if it reflects shared values of the research team and is developed before conflicts arise.\nOliver et al. (2018) described six principles that they aimed for their authorship process to support:\n\n\n\n\n\n\n\nFigure 2.From Oliver at al. Fig. 1. A conceptual diagram that shows the strategies for effective collaborative manuscript (MS) development being firmly embedded within and balancing the guiding principles, and the relative order that the practices occur (numbers). Strategies that are on the same row are strongly related, can occur in any order, and are in fact iterative. All strategies should feed back into the team coauthorship policy for evaluation and reflection about whether the practices are fulfilling the guiding principles.\n\n\n\nTheir approach considers the entire life of a project and aims to balance efficiency, creativity, and inclusion – which can seem to be in conflict. It would, for example, be very inefficient to solicit many authors for a student’s dissertation paper, which is likely to only involve a small number of authors in the end.\nTheir starting point is to understand the makeup of the group involved. What are each member’s assumptions about authorship credit and order? These can differ based on their field of study, their country of origin, their seniority, and their role in the group and what they hope to get out of involvement with the group. Articulating these assumptions at the start can avoid later conflicts.\nThen develop a draft authorship policy–either from scratch or starting with one the many frameworks available. Recognize that this policy is a living document that the team can adjust as issues arise.\nAnnouncing ideas that may lead to manuscripts is also critical in their view. It allows the whole team to contribute to developing the idea (creativity) and allows interested parties to opt-in to further development (transparency) and work on the project.\nDetermine the manuscript type. They outline a taxonomy of manuscript types, which will often determine how large the co-author list can or should be and what kinds of co-author management strategies will work best.\n\nDisciplinary research manuscripts. Typical small-group papers. Clarity is required, but no additional special considerations.\nMultidisciplinary research manuscripts. Special considerations include the unfamiliarity of researchers from one discipline with the work of the other and the role of data analysts.\nEssay, commentary, or concept manuscripts. In these “idea papers” contributions may be less clearly delineated and harder to document.\nData manuscripts and database documentation. Data papers often use an opt-out model, where all group participants are included unless they are unable or unwilling to make basic contributions, such as editing or reading and approving the manuscript before submission. This is a convenient way to make sure that all contributors receive credit in at least one publication.\nGraduate student dissertation manuscripts. Depending on the student’s institution and committee, this may require the involvement of fewer authors.\n\nThe decision on an authorship management strategy will flow from the type of paper and the individuals involved. Here, the value of flexibility and communication within the team is key.",
    "crumbs": [
      "Phase III -- Execute",
      "Intellectual Credit"
    ]
  },
  {
    "objectID": "mod_credit.html#an-evolving-landscape",
    "href": "mod_credit.html#an-evolving-landscape",
    "title": "Authorship & Intellectual Credit",
    "section": "An Evolving Landscape",
    "text": "An Evolving Landscape\nCommunity science and knowledge co-production with non-academics, particularly Indigenous knowledge and rights-holders, have been growing rapidly in the last few years. The academic community is still struggling to come to terms with how best to honor and credit such research partners. Authorship is an academic currency that may or may not have value for community partners. Indigenous Peoples can be understandably wary about sharing data related to their land, resources, history, culture, and bodies. The CARE Principles (Collective Benefit, Authority to Control, Responsibility, and Ethics) were published in 2020 help researchers focus on the values most important to Indigenous data stewards, rather than focusing solely on the open-data FAIR Principles (Findable Accessible, Interoperable, and Reproducible) and concrete guidance is gradually being developed to guide researchers. Community science and Indigenous data found in repositories should include guidance on how they can ethically be reused.\nAnother emerging issue in intellectual credit is how to deal with the growing role of AI in research and publishing. Understanding how AI has been used in a research effort is critical to evaluating the reliability of results. Providing figures and asking AI to write a paper yields a very different result than providing bullet points along with the figures, or asking AI to clean up your first draft. In addition, almost every AI system currently available provides results by drawing on millions of public data sources, without any thought of crediting the originators. Journal publishers are starting to provide guidance about how to acknowledge researchers’ use of AI.",
    "crumbs": [
      "Phase III -- Execute",
      "Intellectual Credit"
    ]
  },
  {
    "objectID": "mod_credit.html#breakout-groups",
    "href": "mod_credit.html#breakout-groups",
    "title": "Authorship & Intellectual Credit",
    "section": "Breakout Groups",
    "text": "Breakout Groups\nNext, we’ll take a closer look at three approaches to authorship policies from teams of varying sizes. Three breakout groups will focus on one policy each and report back on the approaches strengths and weaknesses. Please take 10 minutes to scan the assigned authorship policy and another 15 minutes to discuss it’s implications, what it might miss, how it might distort or align incentives for collaboration, data sharing, and freeloading.\n\nNutrient Network Authorship Policy\nExpanded Authorship Guidelines (Cooke et al. 2021)\nCReDiT Framework (Brand et al. 2015)",
    "crumbs": [
      "Phase III -- Execute",
      "Intellectual Credit"
    ]
  },
  {
    "objectID": "mod_credit.html#group-discussion-1",
    "href": "mod_credit.html#group-discussion-1",
    "title": "Authorship & Intellectual Credit",
    "section": "Group discussion",
    "text": "Group discussion\nWe contend that there is no one “right” authorship policy. But each policy we’ve covered has advantages in certain situations. What matters most is that the research team discusses their approach and records it in an accessible location where it can be revisited and updated as needed.",
    "crumbs": [
      "Phase III -- Execute",
      "Intellectual Credit"
    ]
  },
  {
    "objectID": "mod_credit.html#project-team-time",
    "href": "mod_credit.html#project-team-time",
    "title": "Authorship & Intellectual Credit",
    "section": "Project Team Time",
    "text": "Project Team Time\nGather in your project teams and begin to build out your own authorship approach, if you have not already. A few key questions to get you started follow:\n\nHow will you let participants know about papers and products?\nBeyond your core group, have others been involved in ways that might warrant authorship? How will they learn about upcoming products.\nDo you want to operate on an opt-in or an opt-out basis?\nWhat kinds of products do you expect to produce?\nWhat kinds of contributions do you think are most important?\nWhat is the minimum requirement for being a paper author? A dataset author? A package creator?",
    "crumbs": [
      "Phase III -- Execute",
      "Intellectual Credit"
    ]
  },
  {
    "objectID": "mod_credit.html#additional-resources",
    "href": "mod_credit.html#additional-resources",
    "title": "Authorship & Intellectual Credit",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\nAllen, L. et al., How Can We Ensure Visibility and Diversity in Research Contributions? How the Contributor Role Taxonomy (CReDiT) is Helping the Shift from Authorship to Contributorship. 2018. Learned Publishing\nBrand, et al. Beyond authorship: Attribution, contribution, collaboration, and credit. 2015. Learned Publishing, 28(2), 151–155.\nDahlin, K.M. et al., Hear Every Voice, Working Groups that Work. 2019. Frontiers in Ecology and the Environment\nJennings, L., Jones, K., Taitingfong, R. et al. Governance of Indigenous data in open earth systems science. Nat Commun 16, 572 (2025).\nLund, B.D. and Naheem, K.T. (2024), Can ChatGPT be an author? A study of artificial intelligence authorship policies in top academic journals. Learned Publishing, 37: 13-21.\nNelson, P.R. et al. Authors overestimate their contribution to scientific work, demonstrating a strong bias. 2020. PNAS.\nPuebla, I. et al. Ten simple rules for recognizing data and software contributions in hiring, promotion, and tenure. 2024. PLoS Computational Biology\nWilkinson, M., Dumontier, M., Aalbersberg, I. et al. The FAIR Guiding Principles for scientific data management and stewardship. Sci Data 3, 160018 (2016).\n\n\n\nWebsites\n\nEcological Society of America Code of Ethics. Authorship criteria found at #22.\nAmerican Geophysical Union Publications and Ethics Policies.\nSoil Organic Matter Synthesis Group authorship policy\nNutrient Network (NutNet) authorship policy\nHerbivory Variability Network (HerbVar) participation guidelines",
    "crumbs": [
      "Phase III -- Execute",
      "Intellectual Credit"
    ]
  },
  {
    "objectID": "mod_data-viz.html",
    "href": "mod_data-viz.html",
    "title": "Data Visualization & Exploration",
    "section": "",
    "text": "Data visualization is a fundamental part of working with data. Visualization can be only used in the final stages of a project to make figures for publication but it can also be hugely valuable for quality control and hypothesis development processes. This module focuses on the fundamentals of graph creation in an effort to empower you to apply those methods in the various contexts where you might find visualization to be helpful.",
    "crumbs": [
      "Phase II -- Plan",
      "Data Visualization"
    ]
  },
  {
    "objectID": "mod_data-viz.html#overview",
    "href": "mod_data-viz.html#overview",
    "title": "Data Visualization & Exploration",
    "section": "",
    "text": "Data visualization is a fundamental part of working with data. Visualization can be only used in the final stages of a project to make figures for publication but it can also be hugely valuable for quality control and hypothesis development processes. This module focuses on the fundamentals of graph creation in an effort to empower you to apply those methods in the various contexts where you might find visualization to be helpful.",
    "crumbs": [
      "Phase II -- Plan",
      "Data Visualization"
    ]
  },
  {
    "objectID": "mod_data-viz.html#learning-objectives",
    "href": "mod_data-viz.html#learning-objectives",
    "title": "Data Visualization & Exploration",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nExplain how data visualization can be used to explore data\nDefine fundamental ggplot2 vocabulary\nIdentify appropriate graph types for given data type/distribution\nDiscuss differences between presentation- and publication-quality graphs\nExplain how your graphs can be made more accessible",
    "crumbs": [
      "Phase II -- Plan",
      "Data Visualization"
    ]
  },
  {
    "objectID": "mod_data-viz.html#preparation",
    "href": "mod_data-viz.html#preparation",
    "title": "Data Visualization & Exploration",
    "section": "Preparation",
    "text": "Preparation\n\nEach Synthesis fellow should download one data file identified for your group’s project\nIf you are a Mac user, install XQuartz\nIf you are an R user, run the following code:\n\n\ninstall.packages(\"librarian\")\nlibrarian::shelf(tidyverse, summarytools, datacleanr, lterdatasampler, supportR, cowplot)",
    "crumbs": [
      "Phase II -- Plan",
      "Data Visualization"
    ]
  },
  {
    "objectID": "mod_data-viz.html#networking-session",
    "href": "mod_data-viz.html#networking-session",
    "title": "Data Visualization & Exploration",
    "section": "Networking Session",
    "text": "Networking Session\n\n2024 Guests\n\n\nWe’ll have two guests to kick off today’s class. Each has been involved in synthesis as an early career researcher and each uses visualization in different ways to assess, clarify, and communicate their data and analyses.\n\nTim Ohlert, Postdoctoral Researcher, Colorado State University; DroughtNet Coordinator\nKyle Cavanaugh, Associate Professor, UCLA Institute of the Environment and Sustainability and the UCLA Geography Department",
    "crumbs": [
      "Phase II -- Plan",
      "Data Visualization"
    ]
  },
  {
    "objectID": "mod_data-viz.html#data-visualization-the-synthesis-workflow",
    "href": "mod_data-viz.html#data-visualization-the-synthesis-workflow",
    "title": "Data Visualization & Exploration",
    "section": "Data Visualization & The Synthesis Workflow",
    "text": "Data Visualization & The Synthesis Workflow\nAs shown in the graphic below, visualization can be valuable throughout the lifecycle of a synthesis project, albeit in different ways at different phases of a project.\n\n\n\nDiagram of data stages from raw data to published products. Credit: Margaret O’Brian & Li Kui & Sarah Elmendorf",
    "crumbs": [
      "Phase II -- Plan",
      "Data Visualization"
    ]
  },
  {
    "objectID": "mod_data-viz.html#visualization-for-exploration",
    "href": "mod_data-viz.html#visualization-for-exploration",
    "title": "Data Visualization & Exploration",
    "section": "Visualization for Exploration",
    "text": "Visualization for Exploration\nExploratory data visualization is an important part of any scientific project. Before launching into analysis it is valuable to make some simple plots to scan the contents. These plots may reveal any number of issues, such as typos, sensor calibration problems or differences in the protocol over time.\nThese “fitness for use” visualizations are even more critical for synthesis projects. In synthesis, we are often repurposing publicly available datasets to answer questions that differ from the original motivations for data collection. As a result, the metadata included with a published dataset may be insufficient to assess whether the data are useful for your group’s question. Datasets may not have been carefully quality-controlled prior to publication and could include any number of ‘warts’ that can complicate analyses or bias results. Some of these idiosyncrasies you may be able to anticipate in advance (e.g. spelling errors in taxonomy) and we encourage you to explicitly test for those and rectify them during the data harmonization process (see the Data Wrangling module). Others may come as a surprise.\nDuring the early stages of a synthesis project, you will want to gain skill to quickly scan through large volumes of data. The figures you make will typically be for internal use only, and therefore have low emphasis on aesthetics.\n\nExploratory Visualization Applications\nSpecific applications of exploratory data visualization include identifying:\n\nDataset coverage (temporal, spatial, taxonomic)\n\nFor example, the metadata might indicate a dataset covers the period 2010-2020. That could mean one data point in 2010 and one in 2020! This may not be useful for a time-series analysis.\n\nErrors in metadata\n\nDo the units “make sense” with the figure? Typos in metadata do occur, so if you find yourself with elephants weighing only a few grams, it may be necessary to reach out to the dataset contact.\n\nDifferences in methodology\n\nDo the data from sequential years, replicate sites, different providers generally fall into the same ranges or is there sensor drift or changes in protocols that need to be addressed?\nA risk of synthesis projects is that you may find you are comparing apples to oranges across datasets, as the individual datasets included in your project were likely not collected in a coordinated fashion.\nA benefit of synthesis projects is you will typically have large volumes of data, collected from many locations or timepoints. This data volume can be leveraged to give you a good idea of how your response variable looks at a ‘typical’ location as well as inform your gestalt sense of how much site-to-site, study-to-study, or year-to-year variability is expected. In our experience, where one particular dataset, or time period, strongly differs from the others, the most common root cause is differences in methodology that need to be addressed in the data harmonization process.\n\n\nIn the data exploration stage you may find:\n\nHarmonization issues\n\nAre all your datasets measured in units that can be converted to the same units?\nIf not, can you envision metrics (relative abundance? Effect size?) that would make datasets intercomparable?\n\nSome entire datasets cannot be used\nParts of some datasets cannot be used\nAdditional quality control is needed (e.g. filtering large outliers)\n\nThese steps are an important precursor to the data harmonization stage, where you will process the datasets you have selected into an analysis-ready format.\n\n\n\n\n\n\nActivity: Data Sleuth\n\n\n\nIn this activity, you’ll play the role of data detective. You will have many potential datasets to look through. It is important to do it correctly, but you likely won’t need or want to develop boutique code to examine each dataset, especially since some may be discarded after an initial pass.\nAs a project team, discuss the following points:\n\nDecide on a structure for tracking results of exploratory data checks\n\nGit issues? Additional columns in your team-data-inventory google sheet? Something else?\nDraft a list of ‘generic checks’ you would want to apply to each dataset before inclusion in your synthesis\n\nUse the summarytools and/or datacleanr packages to explore one exemplar dataset that you intend to include in your project\n\nDiscuss any issues you discover\nCreate a “to do” list for the exemplar dataset that details additional steps needed to make that dataset analysis ready (e.g. remove 1993 due to incomplete sampling, convert concentrations from mmols to mg/L, contact dataset providers to ask about anomalous values in April 2021)\nNote we will work on skills to implement these steps in the Data Wrangling module in a few weeks.\nRevise the list of ‘generic checks’ for remaining datasets as necessary\n\nIf you choose to save any images and/or code you used in your exploratory data visualization, decide on a naming convention and storage location\n\nWill you add these files to your .gitignore or do you plan on committing them?\n\nWhat additional plots would you ideally make that are not available through these generic tools?\n\n\nsummarytools Packagedatacleanr Package\n\n\n\n# Load the library\nlibrary(summarytools)\n\n# Load data\ndataset_1 &lt;- read_csv(\"your_file_name_here.csv\")\n\n# View the data in your Rstudio environment\n1summarytools::view(summarytools::dfSummary(dataset_1), footnote = NA)\n\n# Alternatively,save the results for viewing later, or to share with your team\nprint(summarytools::dfSummary(dataset_1), footnote = NA,\n      file = 'dataset_01_summary.html')\n\n\n1\n\nCareful! Use lowercase ‘v’ in the view function of the summarytools package\n\n\n\n\n\n\n\n# Load the library\nlibrary(datacleanr)\n\n# Load data\ndataset_1 &lt;- read_csv(\"your_file_name_here.csv\")\n\n# Launch the shiny app and view the data interactively\ndatacleanr::dcr_app(dataset_1)\n\n\n\n\n\nBoth of these packages have extensive vignettes and online instructional materials. See here for one from summarytools and here for one from datacleanr.",
    "crumbs": [
      "Phase II -- Plan",
      "Data Visualization"
    ]
  },
  {
    "objectID": "mod_data-viz.html#graphing-with-ggplot2",
    "href": "mod_data-viz.html#graphing-with-ggplot2",
    "title": "Data Visualization & Exploration",
    "section": "Graphing with ggplot2",
    "text": "Graphing with ggplot2\nYou may already be familiar with the ggplot2 package in R but if you are not, it is a popular graphing library based on The Grammar of Graphics. Every ggplot is composed of four elements:\n\nA ‘core’ ggplot function call\nAesthetics\nGeometries\nTheme\n\nNote that the theme component may be implicit in some graphs because there is a suite of default theme elements that applies unless otherwise specified.\nThis module will use example data to demonstrate these tools but as we work through these topics you should feel free to substitute a dataset of your choosing! If you don’t have one in mind, you can use the example dataset shown in the code chunks throughout this module. This dataset comes from the lterdatasampler R package and the data are about fiddler crabs (Minuca pugnax) at the Plum Island Ecosystems (PIE) LTER site.\n\n# Load needed libraries\nlibrary(tidyverse); library(lterdatasampler)\n\n# Load the fiddler crab dataset\ndata(pie_crab)\n\n# Check its structure\nstr(pie_crab)\n\ntibble [392 × 9] (S3: tbl_df/tbl/data.frame)\n $ date         : Date[1:392], format: \"2016-07-24\" \"2016-07-24\" ...\n $ latitude     : num [1:392] 30 30 30 30 30 30 30 30 30 30 ...\n $ site         : chr [1:392] \"GTM\" \"GTM\" \"GTM\" \"GTM\" ...\n $ size         : num [1:392] 12.4 14.2 14.5 12.9 12.4 ...\n $ air_temp     : num [1:392] 21.8 21.8 21.8 21.8 21.8 ...\n $ air_temp_sd  : num [1:392] 6.39 6.39 6.39 6.39 6.39 ...\n $ water_temp   : num [1:392] 24.5 24.5 24.5 24.5 24.5 ...\n $ water_temp_sd: num [1:392] 6.12 6.12 6.12 6.12 6.12 ...\n $ name         : chr [1:392] \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" ...\n\n\nWith this dataset in hand, let’s make a series of increasingly customized graphs to demonstrate some of the tools in ggplot2.\n\n1. Starter Graph2. Custom Theme3. Multiple Geometries4. Multiple Datasets\n\n\nLet’s begin with a scatterplot of crab size on the Y-axis with latitude on the X. We’ll forgo doing anything to the theme elements at this point to focus on the other three elements.\n\n1ggplot(data = pie_crab, mapping = aes(x = latitude, y = size, fill = site)) +\n2  geom_point(pch = 21, size = 2, alpha = 0.5)\n\n\n1\n\nWe’re defining both the data and the X/Y aesthetics in this top-level bit of the plot. Also, note that each line ends with a plus sign\n\n2\n\nBecause we defined the data and aesthetics in the ggplot() function call above, this geometry can assume those mappings without re-specificying\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can improve on this graph by tweaking theme elements to make it use fewer of the default settings.\n\nggplot(data = pie_crab, mapping = aes(x = latitude, y = size, fill = site)) +\n  geom_point(pch = 21, size = 2, alpha = 0.5) +\n1  theme(legend.title = element_blank(),\n        panel.background = element_blank(),\n        axis.line = element_line(color = \"black\"))\n\n\n1\n\nAll theme elements require these element_... helper functions. element_blank removes theme elements but otherwise you’ll need to use the helper function that corresponds to the type of theme element (e.g., element_text for theme elements affecting graph text)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can further modify ggplot2 graphs by adding multiple geometries if you find it valuable to do so. Note however that geometry order matters! Geometries added later will be “in front of” those added earlier. Also, adding too much data to a plot will begin to make it difficult for others to understand the central take-away of the graph so you may want to be careful about the level of information density in each graph. Let’s add boxplots behind the points to characterize the distribution of points more quantitatively.\n\nggplot(data = pie_crab, mapping = aes(x = latitude, y = size, fill = site)) +\n1  geom_boxplot(pch = 21) +\n  geom_point(pch = 21, size = 2, alpha = 0.5) +\n  theme(legend.title = element_blank(), \n        panel.background = element_blank(),\n        axis.line = element_line(color = \"black\"))\n\n\n1\n\nBy putting the boxplot geometry first we ensure that it doesn’t cover up the points that overlap with the ‘box’ part of each boxplot\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot2 also supports adding more than one data object to the same graph! While this module doesn’t cover map creation, maps are a common example of a graph with more than one data object. Another common use would be to include both the full dataset and some summarized facet of it in the same plot.\nLet’s calculate some summary statistics of crab size to include that in our plot.\n\n# Load the supportR library\nlibrary(supportR)\n\n# Summarize crab size within latitude groups\ncrab_summary &lt;- supportR::summary_table(data = pie_crab, groups = c(\"site\", \"latitude\"),\n                                        response = \"size\", drop_na = TRUE)\n\n# Check the structure\nstr(crab_summary)\n\n'data.frame':   13 obs. of  6 variables:\n $ site       : chr  \"BC\" \"CC\" \"CT\" \"DB\" ...\n $ latitude   : num  42.2 41.9 41.3 39.1 30 39.6 41.6 33.3 42.7 34.7 ...\n $ mean       : num  16.2 16.8 14.7 15.6 12.4 ...\n $ std_dev    : num  4.81 2.05 2.36 2.12 1.8 2.72 2.29 2.42 2.3 2.34 ...\n $ sample_size: int  37 27 33 30 28 30 29 30 28 25 ...\n $ std_error  : num  0.79 0.39 0.41 0.39 0.34 0.5 0.43 0.44 0.43 0.47 ...\n\n\nWith this data object in-hand, we can make a graph that includes both this and the original, unsummarized crab data. To better focus on the ‘multiple data objects’ bit of this example we’ll pare down on the actual graph code.\n\n1ggplot() +\n  geom_point(pie_crab, mapping = aes(x = latitude, y = size, fill = site),\n             pch = 21, size = 2, alpha = 0.2) + \n2  geom_errorbar(crab_summary, mapping = aes(x = latitude,\n                                            ymax = mean + std_error,\n                                            ymin = mean - std_error),\n                width = 0.2) +\n  geom_point(crab_summary, mapping = aes(x = latitude, y = mean, fill = site),\n             pch = 23, size = 3) + \n  theme(legend.title = element_blank(),\n        panel.background = element_blank(),\n        axis.line = element_line(color = \"black\"))\n\n\n1\n\nIf you want multiple data objects in the same ggplot2 graph you need to leave this top level ggplot() call empty! Otherwise you’ll get weird errors with aesthetics later in the graph\n\n2\n\nThis geometry adds the error bars and it’s important that we add it before the summarized data points themselves if we want the error bars to be ‘behind’ their respective points\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nActivity: Graph Creation (P1)\n\n\n\nIn a script, attempt the following with one of either yours or your group’s datasets:\n\nMake a graph using ggplot2\n\nInclude at least one geometry\nInclude at least one aesthetic (beyond X/Y axes)\nModify at least one theme element from the default",
    "crumbs": [
      "Phase II -- Plan",
      "Data Visualization"
    ]
  },
  {
    "objectID": "mod_data-viz.html#streamlining-graph-aesthetics",
    "href": "mod_data-viz.html#streamlining-graph-aesthetics",
    "title": "Data Visualization & Exploration",
    "section": "Streamlining Graph Aesthetics",
    "text": "Streamlining Graph Aesthetics\nSynthesis projects often generate an entire network of inter-related papers. Ensuring that all graphs across papers from a given team have a similar “feel” is a nice way of implying a certain standard of robustness for all of your group’s projects. However, copy/pasting the theme elements of your graphs can (A) be cumbersome to do even once and (B) needs to be re-done every time you make a change anywhere. Fortunately, there is a better way!\nggplot2 supports adding theme elements to an object that can then be reused as needed elsewhere. This is the same theory behind wrapping repeated operations into custom functions.\n\n# Define core theme elements\ntheme_synthesis &lt;- theme(legend.position = \"none\",\n                         panel.background = element_blank(),\n                         axis.line = element_line(color = \"black\"),\n1                         axis.text = element_text(size = 13))\n\n# Create a graph\nggplot(pie_crab, aes(y = water_temp, x = air_temp, color = size, size = size)) +\n  geom_point() +\n  theme_synthesis +\n2  theme(legend.position = \"right\")\n\n\n1\n\nThis theme element controls the text on the tick marks. axis.title controls the text in the labels of the axes\n\n2\n\nAs a bonus, subsequent uses of theme() will replace defaults defined in your earlier theme object. So, you can design a set of theme elements that are usually appropriate and then easily change just some of them as needed\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nActivity: Graph Creation (P2)\n\n\n\nIn a script, attempt the following:\n\nRemove all theme edits from the graph you made in the preceding activity and assign them to a separate object\n\nThen add that object to your graph\n\nMake a second (different) graph and add your consolidated theme object to that graph as well",
    "crumbs": [
      "Phase II -- Plan",
      "Data Visualization"
    ]
  },
  {
    "objectID": "mod_data-viz.html#multi-panel-graphs",
    "href": "mod_data-viz.html#multi-panel-graphs",
    "title": "Data Visualization & Exploration",
    "section": "Multi-Panel Graphs",
    "text": "Multi-Panel Graphs\nIt is sometimes the case that you want to make a single graph file that has multiple panels. For many of us, we might default to creating the separate graphs that we want, exporting them, and then using software like Microsoft PowerPoint to stitch those panels into the single image we had in mind from the start. However, as all of us who have used this method know, this is hugely cumbersome when your advisor/committee/reviewers ask for edits and you now have to redo all of the manual work behind your multi-panel graph.\nFortunately, there are two nice entirely scripted alternatives that you might consider: Faceted graphs and Plot grids. See below for more information on both.\n\nFacetsPlot Grids\n\n\nIn a faceted graph, every panel of the graph has the same aesthetics. These are often used when you want to show the relationship between two (or more) variables but separated by some other variable. In synthesis work, you might show the relationship between your core response and explanatory variables but facet by the original study. This would leave you with one panel per study where each would show the relationship only at that particular study.\nLet’s check out an example.\n\nggplot(pie_crab, aes(x = date, y = size, color = site))+\n  geom_point(size = 2) +\n1  facet_wrap(. ~ site) +\n  theme_bw() +\n2  theme(legend.position = \"none\")\n\n\n1\n\nThis is a ggplot2 function that assumes you want panels laid out in a regular grid. There are other facet_... alternatives that let you specify row versus column arrangement. You could also facet by multiple variables by putting something to the left of the tilde\n\n2\n\nWe can remove the legend because the site names are in the facet titles in the gray boxes\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn a plot grid, each panel is completely independent of all others. These are often used in publications where you want to highlight several different relationships that have some thematic connection. In synthesis work, your hypotheses may be more complicated than in primary research and such a plot grid would then be necessary to put all visual evidence for a hypothesis in the same location. On a practical note, plot grids are also a common way of circumventing figure number limits enforced by journals.\nLet’s check out an example that relies on the cowplot library.\n\n# Load a needed library\nlibrary(cowplot)\n\n# Create the first graph\n1crab_p1 &lt;- ggplot(pie_crab, aes(x = site, y = size, fill = site)) +\n  geom_violin() +\n2  coord_flip() +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n# Create the second\ncrab_p2 &lt;- ggplot(pie_crab, aes(x = air_temp, y = water_temp)) +\n  geom_errorbar(aes(ymax = water_temp + water_temp_sd, ymin = water_temp - water_temp_sd),\n                 width = 0.1) +\n3  geom_errorbarh(aes(xmax = air_temp + air_temp_sd, xmin = air_temp - air_temp_sd),\n                 width = 0.1) +\n  geom_point(aes(fill = site), pch = 23, size = 3) +\n  theme_bw()\n\n# Assemble into a plot grid\n4cowplot::plot_grid(crab_p1, crab_p2, labels = \"AUTO\", nrow = 1)\n\n\n1\n\nNote that we’re assigning these graphs to objects!\n\n2\n\nThis is a handy function for flipping X and Y axes without re-mapping the aesthetics\n\n3\n\nThis geometry is responsible for horizontal error bars (note the “h” at the end of the function name)\n\n4\n\nThe labels = \"AUTO\" argument means that each panel of the plot grid gets the next sequential capital letter. You could also substitute that for a vector with labels of your choosing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nActivity: Graph Creation (P3)\n\n\n\nIn a script, attempt the following:\n\nAssemble the two graphs you made in the preceding two activities into the appropriate type of multi-panel graph",
    "crumbs": [
      "Phase II -- Plan",
      "Data Visualization"
    ]
  },
  {
    "objectID": "mod_data-viz.html#accessibility-considerations",
    "href": "mod_data-viz.html#accessibility-considerations",
    "title": "Data Visualization & Exploration",
    "section": "Accessibility Considerations",
    "text": "Accessibility Considerations\nAfter you’ve made the graphs you need, it is good practice to revisit them with to ensure that they are as accessible as possible. You can of course also do this during the graph construction process but it is sometimes less onerous to tackle as a penultimate step in the figure creation process. There are many facets to accessibility and we’ve tried to cover just a few of them below.\n\nColor Choice\nOne of the more well-known facets of accessibility in data visualization is choosing colors that are “colorblind safe”. Such palettes still create distinctive colors for those with various forms of color blindness (e.g., deuteranomoly, protanomaly, etc.). The classic red-green heatmap for instance is very colorblind unsafe in that people with some forms of colorblindness cannot distinguish between those colors (hence the rise of the yellow-blue heatmap in recent years). Unforunately, the ggplot2 default rainbow palette–while nice for exploratory purposes–is not colorlbind sfae.\nSome websites (such as colorbewer2.org) include a simple checkbox for colorblindness safety which automatically limits the listed options to those that are colorblind safe. Alternately, you could use a browser plug-in (such as Let’s get color blind on Google Chrome) to simulate colorblindness on a particular page.\nOne extreme approach you could take is to dodge this issue entirely and format your graphs such that color either isn’t used at all or only conveys information that is also conveyed in another graph aesthetic. We don’t necessarily recommend this as color–when the palette is chosen correctly–can be a really nice way of making information-dense graphs more informative and easily-navigable by viewers.\n\n\nMultiple Modalities\nRelated to the color conversation is the value of mapping multiple aesthetics to the same variable. By presenting information in multiple ways–even if that seems redundant–you enable a wider audience to gain an intuitive sense of what you’re trying to display.\n\nggplot(data = pie_crab, mapping = aes(x = latitude, y = size, \n1                                      fill = site, shape = site)) +\n  geom_jitter(size = 2, width = 0.1, alpha = 0.6) +  \n2  scale_shape_manual(values = c(21:25, 21:25, 21:23)) +\n  theme_bw() +\n  theme(legend.title = element_blank())\n\n\n1\n\nIn this graph we’re mapping both the fill and shape aesthetics to site\n\n2\n\nThis is a little cumbersome but there are only five ‘fill-able’ shapes in R so we need to reuse some of them to have a unique one for each site. Using fill-able shapes is nice because you get a crisp black border around each point. See ?pch for all available shapes\n\n\n\n\n\n\n\n\n\n\n\nIn the above graph, even though the rainbow palette is not ideal for reasons mentioned earlier, it is now much easier to tell the difference between sites with similar colors. For instance, “NB”, “NIB”, and “PIE” are all shades of light blue/teal. Now that they have unique shapes it is dramatically easier to look at the graph and identify which points correspond to which site.\n\n\n\n\n\n\nDiscussion: Graph Accessibility\n\n\n\nWith a group discuss (some of) the following questions:\n\nWhat are other facets of accessibility that you think are important to consider when making data visualizations?\nWhat changes do you make to your graphs to increase accessibility?\n\nWhat changes could you make going forward?\n\n\n\n\n\n\nPresentation vs. Publication\nOne final element of accessibility to consider is the difference between a ‘presentation-quality’ graph and a ‘publication-quality’ one. While it may be tempting to create a single version of a given graph and use it in both contexts that is likely to be less effective in helping you to get your point across than making small tweaks to two separate versions of what is otherwise the same graph.\n\nPresentation-FocusedPublication-FocusedOther Considerations\n\n\nDo:\n\nIncrease size of text/points greatly\n\nIf possible, sit in the back row of the room where you’ll present and look at your graphs from there\n\nConsider adding graph elements that highlight certain graph regions\nPresent summarized data (increases focus on big-picture trends and avoids discussion of minutiae)\nMap multiple aesthetics to the same variables\n\nDon’t:\n\nUse technical language / jargon\nInclude unnecessary background elements\nUse multi-panel graphs (either faceted or plot grid)\n\nIf you have multiple graph panels, put each on its own slide!\n\n\n\nggplot(crab_summary, aes(x = latitude, y = mean, \n1                         shape = reorder(site, latitude),\n                         fill = reorder(site, latitude))) +\n  geom_vline(xintercept = 36.5, color = \"black\", linetype = 1) +\n2  geom_vline(xintercept = 41.5, color = \"black\", linetype = 2) +\n  geom_errorbar(mapping = aes(ymax = mean + std_error, ymin = mean - std_error),\n                width = 0.2) +\n  geom_point(size = 4) + \n  scale_shape_manual(values = c(21:25, 21:25, 21:23)) +\n3  labs(x = \"Latitude\", y = \"Mean Crab Size (mm)\") +\n  theme(legend.title = element_blank(),\n        axis.line = element_line(color = \"black\"),\n        panel.background = element_blank(),\n        axis.title = element_text(size = 17),\n        axis.text = element_text(size = 15))\n\n\n1\n\nWe can use the reorder function to make the order of sites in the legend (from top to bottom) match the order of sites in the graph (from left to right)\n\n2\n\nAdding vertical lines at particular parts in the graph can make comparisons within the same graph easier\n\n3\n\nlabs lets us customize the title and label text of a graph\n\n\n\n\n\n\n\n\n\n\n\n\n\nDo:\n\nIncrease size of text/points slightly\n\nYou want to be legible but you can more safely assume that many readers will be able to increase the zoom of their browser window if needed\n\nPresent un-summarized data (with or without summarized points included)\n\nMany reviewers will want to get a sense for the “real” data so you should include unsummarized values wherever possible\n\nUse multi-panel graphs\n\nIf multiple graphs “tell a story” together, then they should be included in the same file!\n\nMap multiple aesthetics to the same variables\nIf publishing in a journal available in print, check to make sure your graph still makes sense in grayscale\n\nThere are nice browser plug-ins (like Grayscale the Web for Google Chrome) for this too\n\n\nDon’t:\n\nInclude unnecessary background elements\nAdd graph elements that highlight certain graph regions\n\nYou can–and should–lean more heavily on the text of your publication to discuss particular areas of a graph\n\n\n\nggplot() +\n  geom_point(pie_crab, mapping = aes(x = latitude, y = size,\n                                     color = reorder(site, latitude)),\n             pch = 19, size = 1, alpha = 0.3) +\n  geom_errorbar(crab_summary, mapping = aes(x = latitude, y = mean, \n                                  ymax = mean + std_error, \n                                  ymin = mean - std_error),\n                width = 0.2) +\n  geom_point(crab_summary, mapping = aes(x = latitude, y = mean, \n                           shape = reorder(site, latitude),\n                           fill = reorder(site, latitude)),\n            size = 4) +\n  scale_shape_manual(values = c(21:25, 21:25, 21:23)) +\n1  labs(x = \"Latitude\", y = \"Mean Crab Carapace Width (mm)\") +\n  theme(legend.title = element_blank(),\n        axis.line = element_line(color = \"black\"),\n        panel.background = element_blank(),\n        axis.title = element_text(size = 15),\n        axis.text = element_text(size = 13))\n\n\n1\n\nHere we are using a reasonable amount of technical language\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome other factors you might consider regardless of where the graphs will be embedded include:\n\nWhite Background. Ensure figures have a plain, white background for clarity and compatibility with journal formats.\nHigh Resolution. Use a resolution of at least 300 dpi for print quality. Journals often specify the minimum dpi required.\nBounding Box and Borders. Add a bounding box or border if it enhances clarity, but avoid excessive framing unless necessary to separate elements clearly.\nClear Axis Labels. Label axes with clear, concise descriptions, including units of measurement (e.g., “Temperature (°C)”). Use readable font sizes that remain legible when scaled.\nConsistent Font Style and Size. Use a uniform font style (e.g., Arial, Helvetica) across all figures and a size that is readable but not overwhelming (typically 8–12 points).\nColor Scheme. Choose a color palette that remains clear in both color and grayscale. Use distinct colors for different categories or groups, and avoid colors that may be difficult for colorblind readers to differentiate (e.g., red-green combinations).\nLegend Placement. Place legends within the figure space if possible, ensuring they don’t overlap data or distract from the main content. Keep legends concise.\nMinimal Gridlines. Use minimal and subtle gridlines for reference, but avoid heavy or cluttered lines that may distract from the data.\nError Bars and Statistical Indicators. Add error bars, confidence intervals, or statistical significance markers as needed to represent variability and support interpretation.\nDescriptive Figure Caption. Include a detailed caption that summarizes the figure’s purpose, data source, and any essential methods or abbreviations. Captions should be self-contained to ensure figures are understandable independently.",
    "crumbs": [
      "Phase II -- Plan",
      "Data Visualization"
    ]
  },
  {
    "objectID": "mod_data-viz.html#code-demo-post-harmonization-visualization",
    "href": "mod_data-viz.html#code-demo-post-harmonization-visualization",
    "title": "Data Visualization & Exploration",
    "section": "Code Demo: Post-Harmonization Visualization",
    "text": "Code Demo: Post-Harmonization Visualization\nAfter harmonizing your data, you’ll want to generate one last set of ‘sanity check’ plots to make sure (1) you have interpreted the metadata correctly (2) you haven’t made any obvious errors in the harmonization and (3) your data are ready for analysis. Nothing is less fun than finding out your analytical results are due to an error in the underlying data.\nThe following is a multi-part code demonstration of three common post-harmonization uses of visualization. In addition to being useful graphs, there is also example code on how to export multiple panels of graphs into separate pages of a PDF which can be really helpful when reviewing exploratory visualizations as a group (without needing to scroll through a ton of separate graph files).\n\nAdditional Needed Packages\nIf you’d like to follow along with the code chunks included throughout this demo, you’ll need to install the following packages:\n\n## install.packages(\"librarian\")\nlibrarian::shelf(tidyverse, scales, ggforce, slider)\n\nThe three sets of plots below encompass many of the most common data structures we have encountered types in ecological synthesis projects. These include quantitative measurements collected over many sites, taxonomic data collected over many sites, and seasonal time series data.\n\nGraph All Numeric VariablesTaxonomic ConsistencySeasonal Time Series\n\n\nIt can be helpful to visualize all numeric variables in your dataset, grouped by site (or dataset source) to check that the data have been homogenized correctly. As an example, we’ll use a 2019 dataset on lake water quality, chemistry, and zooplankton community composition near the Niwot Ridge LTER. The dataset is a survey of 16 high alpine lakes and has structure similar to one that might be included in a multi-site synthesis. For more information on these data, check out the data package on EDI.\n\n# Read in data\n1green_biochem &lt;- read.csv(file = file.path(\"data\", \"green-lakes_water-chem-zooplank.csv\")) %&gt;%\n  dplyr::mutate(date = as.Date(date))\n\n# Check structure\nstr(green_biochem)\n\n\n1\n\nNote that you could also read in this data directly from EDI. See ~line 31 of this script for a syntax example\n\n\n\n\n'data.frame':   391 obs. of  14 variables:\n $ local_site : chr  \"Blue Lake\" \"Blue Lake\" \"Blue Lake\" \"Blue Lake\" ...\n $ location   : chr  \"LAKE\" \"LAKE\" \"LAKE\" \"LAKE\" ...\n $ depth      : num  0 1 2 3 4 5 6 7 8 9 ...\n $ date       : Date, format: \"2016-07-08\" \"2016-07-08\" ...\n $ time       : chr  \"09:11:00\" \"09:13:00\" \"09:14:00\" \"09:16:00\" ...\n $ chl_a      : num  0.521 NA NA NA NA NA NA NA NA NA ...\n $ pH         : num  6.75 6.78 6.72 6.67 6.57 6.55 6.52 6.51 6.48 6.49 ...\n $ temp       : num  2.8 2.8 2.73 2.72 7.72 2.65 2.65 2.65 2.64 2.65 ...\n $ std_conduct: num  8 9 10 9 10 9 9 9 9 9 ...\n $ conduct    : num  4 5 6 6 6 5 5 5 5 6 ...\n $ DO         : num  8.23 8.14 8.14 8.05 8.11 8.07 8.21 8.19 8.17 8.16 ...\n $ sat        : num  60.9 60.1 60.2 59.4 59.8 59.4 60.3 60.3 60.1 60 ...\n $ secchi     : num  6.25 NA NA NA NA NA NA NA NA NA ...\n $ PAR        : num  1472 872 690 530 328 ...\n\n\nOnce we have the data file, we can programmatically identify all columns that R knows to be numeric.\n\n# determine which columns are numeric in green_biochem\nnumcols &lt;- green_biochem %&gt;%\n1  dplyr::select(dplyr::where(~ is.numeric(.x) == TRUE)) %&gt;%\n  names(.) %&gt;% \n  sort(.)\n\n# Check that out\n2numcols\n\n\n1\n\nThe tilde (~) is allowing us to evaluate each column against this conditional\n\n2\n\nYou may notice that these columns all have \"num\" next to them in their structure check. The scripted method is dramatically faster and more reproducible than writing these names down by hand\n\n\n\n\n [1] \"chl_a\"       \"conduct\"     \"depth\"       \"DO\"          \"PAR\"        \n [6] \"pH\"          \"sat\"         \"secchi\"      \"std_conduct\" \"temp\"       \n\n\nNow that we have our data and a vector of numeric column names, we can generate a multi-page PDF of scatterplots where each page is specific to a numeric variable and each graph panel within a given page reflects the time series at each site.\n\n# Open PDF 'device'\n1grDevices::pdf(file = file.path(\"qc_all_numeric.pdf\"))\n\n# Loop across numeric variables\nfor (var in numcols) {\n  \n  # Create a set of graphs for one variable\n  myplot &lt;- ggplot(green_biochem, aes(x = date, y = .data[[var]])) +\n2    geom_point(alpha = 0.5) +\n    facet_wrap(. ~ local_site)\n  \n  # Print that variable\n  print(myplot)\n}\n\n# Close the device\n3dev.off()\n\n\n1\n\nThis function tells R that the following code should be saved as a PDF\n\n2\n\nA scatterplot may not be the best tool for your data; adjust appropriately\n\n3\n\nThis function (when used after a ‘device’ function like grDevices::pdf) tells R when to stop adding things to the PDF and actually save it\n\n\n\n\nThe first page of the resulting plot should look something like the following, with each page having the same content but a different variable on the Y axis.\n\n\n\n\n\n\n\n\n\n\n\nTaxonomic time series can be tricky to work with due to inconsistencies in nomenclature and/or sampling effort. In particular, ‘pseudoturnover’ where one species ‘disappears’ with or without the simultaneous ‘appearance’ of another taxa can be indicative of either true extinctions, or changes in species names, or changes in methodology that cause particular taxa not to be detected. A second complication is that taxonomic data are often archived as ‘presence-only’ so it is necessary to infer the absences based on sampling methodology and add them to your dataset before analysis.\nWhile there are doubtless many field-collected datasets that have this issue, we’ve elected to simulate data so that we can emphasize the visualization elements of this problem while avoiding the “noise” typical of real data. This simulation is not necessarily vital to the visualization so we’ve left it out of the following demo. However, if that is of interest to you, see this script–in particular ~line 41 through ~80.\nA workflow for establishing taxonomic consistency and plotting the results is included below.\n\n# Read in data\ntaxa_df &lt;- read.csv(file.path(\"data\", \"simulated-taxa-df.csv\"))\n\n# Check structure\nstr(taxa_df)\n\n'data.frame':   1025 obs. of  4 variables:\n $ year : int  2010 2010 2010 2010 2010 2010 2010 2010 2010 2010 ...\n $ plot : int  1 1 1 1 1 1 1 1 1 1 ...\n $ taxon: chr  \"Taxon_A\" \"Taxon_B\" \"Taxon_C\" \"Taxon_D\" ...\n $ count: int  8 11 7 13 14 15 11 6 9 7 ...\n\n\nFirst, we’ll define units of sampling (year, plot and taxon) and ‘pad out’ the zeros. In this example, we have only added zeroes for taxa-plot-year combinations where that taxa is present in at least one year at a given plot. Again, this zero-padding is prerequisite to the visualization but not necessarily part of it so see ~lines 84-117 of the prep script if that process is of interest.\n\n# Read in data\nwithzeros &lt;- read.csv(file.path(\"data\", \"simulated-taxa-df_with-zeros.csv\")) %&gt;% \n  dplyr::mutate(plot = factor(plot))\n\n# Check structure\n1str(withzeros)\n\n\n1\n\nNotice how there are more rows than the preceding data object and several new zeroes in the first few rows?\n\n\n\n\n'data.frame':   1100 obs. of  4 variables:\n $ plot : Factor w/ 10 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 2 3 4 5 5 6 7 8 ...\n $ taxon: chr  \"Taxon_A\" \"Taxon_A\" \"Taxon_A\" \"Taxon_A\" ...\n $ year : int  2014 2014 2019 2014 2014 2014 2019 2014 2014 2013 ...\n $ n    : int  0 0 0 0 0 0 0 0 0 0 ...\n\n\nNow that we have the data in the format we need, we’ll create a plot of species counts over time with zeros filled in. Because there are many plots and it is difficult to see so many panels on the same page, we’ll use the facet_wrap_paginate function from the ggforce package to create a multi-page PDF output.\n\n# Create the plot of species counts over time (with zeros filled in)\nmyplot &lt;- ggplot(withzeros, aes(x = year, y = n, group = plot, color = plot)) +\n  geom_line() +\n  scale_x_continuous(breaks = scales::pretty_breaks()) +\n1  ggforce::facet_wrap_paginate(~ taxon, nrow = 2, ncol = 2)\n\n# Start the PDF output\ngrDevices::pdf(file.path(\"counts_by_taxon_with_zeros.pdf\"),\n               width = 9, height = 5)\n\n# Loop across pages (defined by `ggforce::facet_wrap_paginate`)\nfor (i in seq_along(ggforce::n_pages(myplot))) {\n  \n  page_plot &lt;- myplot + \n      ggforce::facet_wrap_paginate(~taxon, page = i, \n                                   nrow = 2, ncol = 2)\n  \n  print(page_plot)\n}\n\n# Close the PDF output\ndev.off()\n\n\n1\n\nThis allows a faceted graph to spread across more than one page. See ?ggforce::facet_wrap_paginate for details\n\n\n\n\nThe first page of the resulting plot should look something like this:\n\n\n\n\n\n\n\n\n\nNotice how “Taxon_A” is absent from all plots in 2014 whereas “Taxon_B” has extremely high counts in the same year. Often this can signify inconsistent use of taxonomic names over time.\n\n\nFor time series, intra-annual variation can often make data issues difficult to spot. In these cases, it can be helpful to plot each year onto the same figure and compare trends across study years.\nAs an example, we’ll use a 2024 dataset on streamflow near the Niwot Ridge LTER. The dataset is a 22 year time-series of daily streamflow. For more information on these data, check out the data package on EDI.\n\n# Read data\n1green_streamflow &lt;- read.csv(file.path(\"data\", \"green-lakes_streamflow.csv\"))\n\n# Check structure\nstr(green_streamflow)\n\n\n1\n\nNote again that you could also read in this data directly from EDI. See ~line 129 of this script for a syntax example\n\n\n\n\n'data.frame':   15451 obs. of  6 variables:\n $ LTER_site  : chr  \"NWT\" \"NWT\" \"NWT\" \"NWT\" ...\n $ local_site : chr  \"gl4\" \"gl4\" \"gl4\" \"gl4\" ...\n $ date       : chr  \"1981-06-12\" \"1981-06-13\" \"1981-06-14\" \"1981-06-15\" ...\n $ discharge  : num  9786 8600 7600 6700 5900 ...\n $ temperature: num  NA NA NA NA NA NA NA NA NA NA ...\n $ notes      : chr  \"flow data estimated from intermittent observations\" \"flow data estimated from intermittent observations\" \"flow data estimated from intermittent observations\" \"flow data estimated from intermittent observations\" ...\n\n\nLet’s now calculate a moving average encompassing the 5 values before and after each focal value.\n\n# Do necessary wrangling\nstream_data &lt;- green_streamflow %&gt;%\n  # Calculate moving average for each numeric variable\n  dplyr::mutate(dplyr::across(.cols = dplyr::all_of(c(\"discharge\", \"temperature\")),\n                              .fns = ~ slider::slide_dbl(.x = .x, .f = mean,\n                                                         .before = 5, .after = 5),\n                              .names = \"{.col}_move.avg\" )) %&gt;%\n  # Handle date format issues\n  dplyr::mutate(yday = lubridate::yday(date),\n                year = lubridate::year(date))\n\n# Check the structure of that\nstr(stream_data)\n\n'data.frame':   15451 obs. of  10 variables:\n $ LTER_site           : chr  \"NWT\" \"NWT\" \"NWT\" \"NWT\" ...\n $ local_site          : chr  \"gl4\" \"gl4\" \"gl4\" \"gl4\" ...\n $ date                : chr  \"1981-06-12\" \"1981-06-13\" \"1981-06-14\" \"1981-06-15\" ...\n $ discharge           : num  9786 8600 7600 6700 5900 ...\n $ temperature         : num  NA NA NA NA NA NA NA NA NA NA ...\n $ notes               : chr  \"flow data estimated from intermittent observations\" \"flow data estimated from intermittent observations\" \"flow data estimated from intermittent observations\" \"flow data estimated from intermittent observations\" ...\n $ discharge_move.avg  : num  7299 6699 5992 5527 5274 ...\n $ temperature_move.avg: num  NA NA NA NA NA NA NA NA NA NA ...\n $ yday                : num  163 164 165 166 167 168 169 170 171 172 ...\n $ year                : num  1981 1981 1981 1981 1981 ...\n\n\nPlot seasonal timeseries of each numeric variable as points with the moving average included as lines\n\n# Start PDF output\ngrDevices::pdf(file = file.path(\"qc_all_numeric_seasonal.pdf\"))\n\n# Loop across variables\nfor (var in c(\"discharge\", \"temperature\")) {\n  \n  # Make the graph\n  myplot &lt;- ggplot(stream_data, aes(x = yday, group = year, color = year)) +\n1    geom_point(aes(y = .data[[var]])) +\n2    geom_line(aes(y = .data[[paste0(var, \"_move.avg\")]])) +\n    viridis::scale_color_viridis()\n  \n  # Print it\n  print(myplot)\n}\n\n# End PDF creation\ndev.off()\n\n\n1\n\nAdd points based on the year\n\n2\n\nAdding lines based on the average\n\n\n\n\nThe first page of the resulting figure should look something like this:\n\n\n\n\n\n\n\n\n\nOne of these years is not like the others…",
    "crumbs": [
      "Phase II -- Plan",
      "Data Visualization"
    ]
  },
  {
    "objectID": "mod_data-viz.html#multivariate-visualization",
    "href": "mod_data-viz.html#multivariate-visualization",
    "title": "Data Visualization & Exploration",
    "section": "Multivariate Visualization",
    "text": "Multivariate Visualization\nIf you are working with multivariate data (i.e., data where multiple columns are all response variables collectively) you may need to use visualization methods unique to that data structure. For more information, check out the bonus multivariate visualization module.",
    "crumbs": [
      "Phase II -- Plan",
      "Data Visualization"
    ]
  },
  {
    "objectID": "mod_data-viz.html#maps",
    "href": "mod_data-viz.html#maps",
    "title": "Data Visualization & Exploration",
    "section": "Maps",
    "text": "Maps\nYou may find it valuable to create a map as an additional way of visualizing data. Many synthesis groups do this–particularly when there is a strong spatial component to the research questions and/or hypotheses. Check out the bonus spatial data module for more information on map-making if this is of interest!",
    "crumbs": [
      "Phase II -- Plan",
      "Data Visualization"
    ]
  },
  {
    "objectID": "mod_data-viz.html#additional-resources",
    "href": "mod_data-viz.html#additional-resources",
    "title": "Data Visualization & Exploration",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\nChang, W. et al., ggplot2: Elegant Graphics for Data Analysis. 3rd edition. 2023.\nNational Center for Ecological Analysis and Synthesis (NCEAS). Colorblind Safe Color Schemes. 2022.\nWilke, C.O. Fundamentals of Data Visualization. 2020.\n\n\n\nWorkshops & Courses\n\nThe Carpentries. Data Analysis and Visualization in R for Ecologists: Data Visualization with ggplot2. 2024.\nThe Carpentries. Data Analysis and Visualization in Python for Ecologists: Making Plots with plotnine. 2024.\nLTER Scientific Computing Team. Coding in the Tidyverse: ‘Visualize’ Module. 2023.\n\n\n\nWebsites\n\nThe R Graph Gallery",
    "crumbs": [
      "Phase II -- Plan",
      "Data Visualization"
    ]
  },
  {
    "objectID": "mod_findings.html",
    "href": "mod_findings.html",
    "title": "Communicating Findings",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase IV -- Magnify",
      "Communicating Findings"
    ]
  },
  {
    "objectID": "mod_findings.html#overview",
    "href": "mod_findings.html#overview",
    "title": "Communicating Findings",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase IV -- Magnify",
      "Communicating Findings"
    ]
  },
  {
    "objectID": "mod_findings.html#learning-objectives",
    "href": "mod_findings.html#learning-objectives",
    "title": "Communicating Findings",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nDefine elements of (in)effective communication\nIdentify relevant audiences for a particular work\nDetermine audience motivations and interest\nTranslate communication into various formats based on efficacy with target group",
    "crumbs": [
      "Phase IV -- Magnify",
      "Communicating Findings"
    ]
  },
  {
    "objectID": "mod_findings.html#preparation",
    "href": "mod_findings.html#preparation",
    "title": "Communicating Findings",
    "section": "Preparation",
    "text": "Preparation\nTBD (To Be Determined)",
    "crumbs": [
      "Phase IV -- Magnify",
      "Communicating Findings"
    ]
  },
  {
    "objectID": "mod_findings.html#networking-session",
    "href": "mod_findings.html#networking-session",
    "title": "Communicating Findings",
    "section": "Networking Session",
    "text": "Networking Session\n\n2024 Guests\n\n\nTo Be Determined!",
    "crumbs": [
      "Phase IV -- Magnify",
      "Communicating Findings"
    ]
  },
  {
    "objectID": "mod_findings.html#effective-communication",
    "href": "mod_findings.html#effective-communication",
    "title": "Communicating Findings",
    "section": "Effective Communication",
    "text": "Effective Communication",
    "crumbs": [
      "Phase IV -- Magnify",
      "Communicating Findings"
    ]
  },
  {
    "objectID": "mod_findings.html#publishing-code-data-and-results",
    "href": "mod_findings.html#publishing-code-data-and-results",
    "title": "Communicating Findings",
    "section": "Publishing Code, Data, and Results",
    "text": "Publishing Code, Data, and Results",
    "crumbs": [
      "Phase IV -- Magnify",
      "Communicating Findings"
    ]
  },
  {
    "objectID": "mod_findings.html#data-management-plans",
    "href": "mod_findings.html#data-management-plans",
    "title": "Communicating Findings",
    "section": "Data Management Plans",
    "text": "Data Management Plans",
    "crumbs": [
      "Phase IV -- Magnify",
      "Communicating Findings"
    ]
  },
  {
    "objectID": "mod_findings.html#additional-resources",
    "href": "mod_findings.html#additional-resources",
    "title": "Communicating Findings",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\n\n\n\n\nWorkshops & Courses\n\nNCEAS Learning Hub & Delta Stewardship Council. Open Science Synthesis: Communicating Your Science. 2023.\n\n\n\nWebsites\n\nCOMPASS. The Message Box",
    "crumbs": [
      "Phase IV -- Magnify",
      "Communicating Findings"
    ]
  },
  {
    "objectID": "mod_multivar-viz.html",
    "href": "mod_multivar-viz.html",
    "title": "Multivariate Visualization",
    "section": "",
    "text": "Under construction; check back later!",
    "crumbs": [
      "Bonus Modules",
      "Multivariate Visualization"
    ]
  },
  {
    "objectID": "mod_multivar-viz.html#overview",
    "href": "mod_multivar-viz.html#overview",
    "title": "Multivariate Visualization",
    "section": "",
    "text": "Under construction; check back later!",
    "crumbs": [
      "Bonus Modules",
      "Multivariate Visualization"
    ]
  },
  {
    "objectID": "mod_multivar-viz.html#learning-objectives",
    "href": "mod_multivar-viz.html#learning-objectives",
    "title": "Multivariate Visualization",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nDefine some common multivariate visualization methods",
    "crumbs": [
      "Bonus Modules",
      "Multivariate Visualization"
    ]
  },
  {
    "objectID": "mod_multivar-viz.html#preparation",
    "href": "mod_multivar-viz.html#preparation",
    "title": "Multivariate Visualization",
    "section": "Preparation",
    "text": "Preparation\nThis is a “bonus” module and thus was created for asynchronous learners. There is no suggested preparatory work.",
    "crumbs": [
      "Bonus Modules",
      "Multivariate Visualization"
    ]
  },
  {
    "objectID": "mod_multivar-viz.html#networking-session",
    "href": "mod_multivar-viz.html#networking-session",
    "title": "Multivariate Visualization",
    "section": "Networking Session",
    "text": "Networking Session\n\n2024 Guests\n\n\nThis was a bonus module for the 2024-25 cohort and so did not include a networking session.",
    "crumbs": [
      "Bonus Modules",
      "Multivariate Visualization"
    ]
  },
  {
    "objectID": "mod_multivar-viz.html#needed-packages",
    "href": "mod_multivar-viz.html#needed-packages",
    "title": "Multivariate Visualization",
    "section": "Needed Packages",
    "text": "Needed Packages\nIf you’d like to follow along with the code chunks included throughout this module, you’ll need to install the following packages:\n\n# Note that these lines only need to be run once per computer\n## So you can skip this step if you've installed these before\ninstall.packages(\"vegan\")\ninstall.packages(\"ape\")\ninstall.packages(\"supportR\")",
    "crumbs": [
      "Bonus Modules",
      "Multivariate Visualization"
    ]
  },
  {
    "objectID": "mod_multivar-viz.html#ordination",
    "href": "mod_multivar-viz.html#ordination",
    "title": "Multivariate Visualization",
    "section": "Ordination",
    "text": "Ordination\nOrdination is the general term for many types of multivariate visualization but typically is used to refer to visualizing a distance or dissimiliarity measure of the data. Such measures collapse multiple columns of response variables into fewer (typically two) index values that are easier to visualize.\nThis is a common approach particularly in answering questions in community ecology or considering a suite of traits (e.g., life history, landscape, etc.) together. While the math behind reducing the dimensionality of your data is interesting, this module is focused on only the visualization facet of ordination so we’ll avoid deeper discussion of the internal mechanics that underpin ordination.\nIn order to demonstrate two types of ordination we’ll use a lichen community composition dataset included in the vegan package. However, ordination approaches are most often used on data with multiple groups so we’ll need to make a simulated grouping column to divide the lichen community data.\n\n# Load library\nlibrary(vegan)\n\n# Grab data\nutils::data(\"varespec\", package = \"vegan\")\n\n# Create a faux group column\ntreatment &lt;- c(rep.int(\"Treatment A\", nrow(varespec) / 2),\n               rep.int(\"Treatment B\", nrow(varespec) / 2))\n\n# Combine into one dataframe\nlichen_df &lt;- cbind(treatment, varespec)\n\n# Check structure of first few columns\nstr(lichen_df[1:5])\n\n'data.frame':   24 obs. of  5 variables:\n $ treatment: chr  \"Treatment A\" \"Treatment A\" \"Treatment A\" \"Treatment A\" ...\n $ Callvulg : num  0.55 0.67 0.1 0 0 ...\n $ Empenigr : num  11.13 0.17 1.55 15.13 12.68 ...\n $ Rhodtome : num  0 0 0 2.42 0 0 1.55 0 0.35 0.07 ...\n $ Vaccmyrt : num  0 0.35 0 5.92 0 ...\n\n\n\nMetric OrdinationNon-Metric Ordination\n\n\nMetric ordinations are typically used when you are concerned with retaining quantitative differences among particular points, even after you’ve collapsed many response variables into just one or two. For example, this is a common approach if you have a table of traits and want to compare the whole set of traits among groups while still being able to interpret the effect of a particular effect on the whole.\nTwo of the more common methods for metric ordination are Principal Components Analysis (PCA), and Principal Coordinates Analysis (PCoA / “metric multidimensional scaling”). The primary difference is that PCA works on the data directly while PCoA works on a distance matrix of the data. We’ll use PCoA in this example because it is closer analog to the non-metric ordination discussed in the other tab. If the holistic difference among groups is of interest, (rather than metric point-to-point comparisons), consider a non-metric ordination approach.\nIn order to perform a PCoA ordination we first need to get a distance matrix of our response variables and then we can actually do the PCoA step. The distance matrix can be calculated with the vegdist function from the vegan package and the pcoa function in the ape package can do the actual PCoA.\n\n# Load needed libraries\nlibrary(vegan); library(ape)\n\n# Get distance matrix\n1lichen_dist &lt;- vegan::vegdist(x = lichen_df[-1], method = \"kulczynski\")\n\n# Do PCoA\npcoa_points &lt;- ape::pcoa(D = lichen_dist)\n\n\n1\n\nThe method argument requires a distance/dissimilarity measure. Note that if you use a non-metric measure (e.g., Bray Curtis, etc.) you lose many of the advantages conferred by using a metric ordination approach.\n\n\n\n\nWith that in hand, we can make our ordination! While you could make this step-by-step on your own, we’ll use the ordination function from the supportR package for convenience. This function automatically uses colorblind safe colors for up to 10 groups and has some useful base plot defaults (as well as including ellipses around the standard deviation of the centorid of all groups).\n\n# Load the library\nlibrary(supportR)\n\n# Make the ordination\nsupportR::ordination(mod = pcoa_points, grps = lichen_df$treatment, \n1                     x = \"topleft\", legend = c(\"A\", \"B\"))\n\n\n1\n\nThis function allows several base plot arguments to be supplied to alter non-critical plot elements (e.g., legend position, point size, etc.)\n\n\n\n\n\n\n\n\n\n\n\nThe percentages included in parentheses on either axis label are the percent of the total variation in the data explained by each axis on its own. Use this information in combination with what the graph looks like to determine how different the groups truly are.\n\n\nNon-metric ordinations are typically used when you care more about the relative differences among groups rather than specific measurements between particular points. For instance, you may want to assess whether the composition of insect communities differs between two experimental treatments. In such a case, your hypothesis likely depends more on the holistic difference between the treatments rather than some quantitative difference on one of the axes.\nThe most common non-metric ordination type is called Nonmetric Multidimensional Scaling (NMS / NMDS). This approach prioritizes making groups that are “more different” further apart than those that are less different. However, NMS uses a dissimilarity matrix which means that the distance between any two specific points cannot be interpreted meaningfully. It is appropriate though to interpret which cloud of points is closer to/further from another in aggregate. If specific distances among points are of interest, consider a metric ordination approach.\nIn order to perform an NMS ordination we’ll first need to calculate a dissimilarity matrix for our response data. The vegan function metaMDS is useful for this. This function has many arguments but the most fundamental are the following:\n\ncomm = the dataframe of response variables (minus any non-numeric / grouping columns)\ndistance = the distance/dissimilarity metric to use\n\nNote that there is no benefit to using a metric distance because when we make the ordination it will become non-metric\n\nk = number of axes to decompose to – typically two so the graph can be simple\ntry = number of attempts at minimizing “stress”\n\nStress is how NMS evaluates how good of a job it did at representing the true differences among groups (lower stress is better)\n\n\n\n# Load needed libraries\nlibrary(vegan)\n\n# Get dissimilarity matrix\ndissim_mat &lt;- vegan::metaMDS(comm = lichen_df[-1], distance = \"bray\", k = 2,\n                             autotransform = F, expand = F, try = 50)\n\nWith that in hand, we can make our ordination! While you could make this step-by-step on your own, we’ll use the ordination function from the supportR package for convenience. This function automatically uses colorblind safe colors for up to 10 groups and has some useful base plot defaults (as well as including ellipses around the standard deviation of the centorid of all groups).\n\n# Load the library\nlibrary(supportR)\n\n# Make the ordination\nsupportR::ordination(mod = dissim_mat, grps = lichen_df$treatment, \n1                     x = \"bottomright\", legend = c(\"A\", \"B\"))\n\n\n1\n\nThis function allows several base plot arguments to be supplied to alter non-critical plot elements (e.g., legend position, point size, etc.)\n\n\n\n\n\n\n\n\n\n\n\nIf the stress is less than 0.15 it is generally considered a good representation of the data. We can see that the ellipses do not overlap which indicates that the community composition of our two groups does seem to differ. We’d need to do real multivariate analysis if we wanted a p-value or AIC score to support that but as a visual tool this is still useful.",
    "crumbs": [
      "Bonus Modules",
      "Multivariate Visualization"
    ]
  },
  {
    "objectID": "mod_multivar-viz.html#additional-resources",
    "href": "mod_multivar-viz.html#additional-resources",
    "title": "Multivariate Visualization",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\nAbdi, H. & Williams, L.J., Principal Coponent Analysis. WIREs Computational Statistics. 2010.\nRamette, A., Multivariate Analyses in Microbial Ecology. FEMS Microbial Ecology. 2007.\nJackson, D.A., Stopping Rules in Principal Components Analysis: A Comparison of Heuristical and Statistical Approaches. Ecology. 1993.\nClarke, K.R., Non-Parametric Multivariate Analyses of Changes in Community Structure. Australian Journal of Ecology. 1993.\nKenkel, N.C. & Oroloci, L., Applying Metric and Nonmetric Multidimensional Scaling to Ecological Studies: Some New Results. Ecology. 1986.\n\n\n\nWorkshops & Courses\n\n\n\n\n\nWebsites",
    "crumbs": [
      "Bonus Modules",
      "Multivariate Visualization"
    ]
  },
  {
    "objectID": "mod_project-mgmt.html",
    "href": "mod_project-mgmt.html",
    "title": "Project Management",
    "section": "",
    "text": "Team science works best when all participants know when and how they can contribute. Effective project management allows team members to work on many tasks asynchronously, without losing track of the overall context or the progress in related areas of the project. The effort required is returned many times in avoided frustration and unnecessary backtracking and duplication of effort. Developing an explicit logic model as a team helps identify and resolve differences in the mental models and assumptions that team members from diverse disciplines bring to an analysis. It also helps clarify all of the information that will be needed and the appropriate order of steps.",
    "crumbs": [
      "Phase II -- Plan",
      "Project Management"
    ]
  },
  {
    "objectID": "mod_project-mgmt.html#overview",
    "href": "mod_project-mgmt.html#overview",
    "title": "Project Management",
    "section": "",
    "text": "Team science works best when all participants know when and how they can contribute. Effective project management allows team members to work on many tasks asynchronously, without losing track of the overall context or the progress in related areas of the project. The effort required is returned many times in avoided frustration and unnecessary backtracking and duplication of effort. Developing an explicit logic model as a team helps identify and resolve differences in the mental models and assumptions that team members from diverse disciplines bring to an analysis. It also helps clarify all of the information that will be needed and the appropriate order of steps.",
    "crumbs": [
      "Phase II -- Plan",
      "Project Management"
    ]
  },
  {
    "objectID": "mod_project-mgmt.html#learning-objectives",
    "href": "mod_project-mgmt.html#learning-objectives",
    "title": "Project Management",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nArticulate key principles of project management\nDefine common approaches for defining project scope\nIdentify and make explicit internal logical leaps\nDevelop (or refine) the project management framework for your team project",
    "crumbs": [
      "Phase II -- Plan",
      "Project Management"
    ]
  },
  {
    "objectID": "mod_project-mgmt.html#preparation",
    "href": "mod_project-mgmt.html#preparation",
    "title": "Project Management",
    "section": "Preparation",
    "text": "Preparation\nNo specific preparation required.",
    "crumbs": [
      "Phase II -- Plan",
      "Project Management"
    ]
  },
  {
    "objectID": "mod_project-mgmt.html#networking-session",
    "href": "mod_project-mgmt.html#networking-session",
    "title": "Project Management",
    "section": "Networking Session",
    "text": "Networking Session\n\n2024 Guests\n\n\nThe Silica Exports synthesis working group faced a number of coordination challenges – including work context (federal agency and academia), widely dispersed geographic locations (participants and data from 6 continents), and multiple data repositories. Their approach to project management helped them to overcome these obstacles and assemble an exceptional database that is broadly available and a set of novel analyses.\n\nKathi Jo Jankowski, Research Ecologist, Upper Midwest Environmental Sciences Center, U.S. Geological Survey",
    "crumbs": [
      "Phase II -- Plan",
      "Project Management"
    ]
  },
  {
    "objectID": "mod_project-mgmt.html#what-is-project-management",
    "href": "mod_project-mgmt.html#what-is-project-management",
    "title": "Project Management",
    "section": "What is Project Management?",
    "text": "What is Project Management?\nThere are dozens of formal project management frameworks/approaches, most of which come out of industries (such as software development, construction, and manufacturing) that require teams of people to work together with efficiency and accountability. You’ve probably heard the names tossed around: Agile, Scrum, Lean Sigma Six, Kanban, etc. There are whole industries devoted to training project managers and developing apps to support them.\nThe trick, for scientific research projects, is to identify the approaches that also allow questions and goals to evolve along the way – and that don’t require a full-time project manager to implement.\n\nSome common elements of project management schemes\n\n\n\n\n\n\n\nPM Activity\nIndustry Outcome\nScience Outcome\n\n\n\n\nBuild a logical sequence of steps\nEnsure needed resources/people tools are available when needed\nKnow which skills are needed when; know what form the output of each script/analysis needs to take\n\n\nIdentify dependencies\nAvoid supply chain lags\nKnow what data you need and when you need them\n\n\nClarify responsibilities/accountabilities\nDocument productivity and responsibility\nAvoid duplication of effort; keep the project moving; support authorship/credit agreements\n\n\nIdentify and mitigate risks\nKnow when to abort or revise project goals\nAssess threats to project success early and make a plan B",
    "crumbs": [
      "Phase II -- Plan",
      "Project Management"
    ]
  },
  {
    "objectID": "mod_project-mgmt.html#how-to-start",
    "href": "mod_project-mgmt.html#how-to-start",
    "title": "Project Management",
    "section": "How to Start",
    "text": "How to Start\nLike outlining a paper, there are a few basic steps to developing a solid project management plan. And it’s easy to think you can skip over the first steps because they seem so obvious. The trouble is, they may be obvious to each team member in different ways. Taking the time to make sure you all see them the same way can save worlds of headache down the road.\n\nDefine the project\nAgree on the goal and the timeline\nIf the timeline is determined externally (as for SSECR or for most grants), you’ll need to start with the timeline (possibly budget) and scope the project to fit the time and funds you have available.\nDevelop a list of the steps needed to get from where you are to where you need to be.\nPlace the steps in order and define the inputs and outputs of each step.\nIdentify the people responsible for each step and an approximate timeline for its completion.\nIdentify time points for check-in and re-evaluation\n\nHonestly, you could do all of this in a spreadsheet, but there are a few types of visualizations that a lot of people find helpful, which we’ll discuss later today. First, though, we’ll remind ourselves of why the investment in planning time is so valuable.\n\n\n\n\n\n\nFull Group Discussion: Project Management “Fails”\n\n\n\nAnyone who has completed a moderately complex task has stories about planning failures: critical tools left back at the lab when doing field work; key recipe ingredients not purchased before the guests arrive; logistical questions un-asked and un-planned for; mismatched assumptions revealed only at the last moment…\nPart 1 (~3 minutes)\nReflect silently on one of your own project management “fails”\n\nWhat went wrong?\nHow could planning have prevented or improved the situation?\n\nPart 2 (~12 minutes)\n\nShare your project management “fails”\nIf time allows, consider what aspects of planning are most likely to be skipped or overlooked",
    "crumbs": [
      "Phase II -- Plan",
      "Project Management"
    ]
  },
  {
    "objectID": "mod_project-mgmt.html#defining-the-project",
    "href": "mod_project-mgmt.html#defining-the-project",
    "title": "Project Management",
    "section": "Defining the Project",
    "text": "Defining the Project\nFor defining the project and agreeing on the goals, mind maps are useful individual or group exercises. Mind maps offer a lightly-structured way of exploring a focus area. The graphical approach helps to surface unexpected connections and reveal gaps in understanding. Tools for developing them can be as simple as pen and paper, basic drawing apps, a zoom whiteboard, or online collaboration and project planning tools such as Miro, Mural, and FigJam.\nStart with a general topic or question, then expand to the precedents and implications. Then expand from there to the assumptions, modifying conditions, or follow-on implications related to each of your precedents or implications. Often, looking at a problem head-on reveals little new, but surfacing and testing the underlying assumptions can open up fresh territory.\n\n\n\nFigure 1. Mind Map Example\n\n\nThe mind map will likely surface many relevant and un-answered questions, but time is limited. You’ll need to choose one on which to focus. One way to prioritize them is to place them on a two-by-two matrix of payoff v. feasibility. The most important payoff for a particular team may be scientific novelty, practical importance, or whether it’s a fun and engaging question for the group. Often, the most interesting questions will also require the most effort and the group will need to determine what trade-offs they are willing to accept. But sometimes, a question emerges that is both interesting and feasible–perhaps because a new source of data has just become available or simply because no one saw the question from that angle before.\n\n\n\nFigure 2. Feasibility Matrix Example",
    "crumbs": [
      "Phase II -- Plan",
      "Project Management"
    ]
  },
  {
    "objectID": "mod_project-mgmt.html#outline-your-analysis",
    "href": "mod_project-mgmt.html#outline-your-analysis",
    "title": "Project Management",
    "section": "Outline Your Analysis",
    "text": "Outline Your Analysis\nOnce the team has settled on a question and specific set of processes to explore or test, it’s worth sketching out the steps in your analysis. By making the inputs, processing, and outputs of each step explicit, you’ll identify needed data sooner and avoid (some) backtracking.\nThe process of building an explicit analysis description as a team can quickly reveal mismatches in team members’ mental models and uncover hidden assumptions. Making those (often disciplinary) differences visible early can mitigate some of the biggest risks of interdisciplinary research and also offer a way into the most novel insights. A well-structured analysis plan can also be a touchstone to return to throughout the life of your project, keeping the team on-track and avoiding mission creep.\n\n\n\nFigure 3. Analysis Flowchart for a (relatively) straightforward test of controls on Silica exports in streamflow.",
    "crumbs": [
      "Phase II -- Plan",
      "Project Management"
    ]
  },
  {
    "objectID": "mod_project-mgmt.html#plan-the-work",
    "href": "mod_project-mgmt.html#plan-the-work",
    "title": "Project Management",
    "section": "Plan the Work",
    "text": "Plan the Work\nOnce your team goals and the necessary steps are agreed on, it becomes much easier to build a project plan that clarifies the timeline and team member responsibilities. Especially with limited-term projects such as ours, a Gantt chart can be a particularly valuable way to map out project dependencies and responsibilities. With each task arranged on a timeline, it becomes easier to spot potential dependencies that could otherwise hold up the project. With simultaneous tasks stacked vertically, it becomes apparent when the planned workload is unrealistic.\n\nBefore diving into any specific tool, start by making a list of your goal and the steps necessary to get there.\nPlace them into an outline in the order they must proceed.\nSome steps cannot start until others are finished, while others can proceed simultaneously.\n\nSequential steps get a new number, while steps that can be done in parallel get the same number with different letters.\n\nAttach an estimated time frame to each step and identify a responsible team member (tentative at this point).\nArrange the steps in colored bars along a horizontal timeline.\nUse the visualization to assess which steps are likely to be bottlenecks and when the team may be relying too heavily on one individual for mission-critical work.\n\nA robust support infrastructure exists for the project management profession – including formal training and a multitude of (often pricey) apps. But for the job of managing research collaborations, we’ve found that the single most important factor is whether your collaborators will use the system. For that to happen, they must have easy access to it, ideally without setting up a new account and as part of something they already do every day.\nFor non-coders, we’ve included a Google sheet template of a Gantt chart, pictured below.\n\n\n\nFigure 4. Gantt Chart Example using Google Sheets\n\n\nBut if you’re already working in GitHub or GitLab to collaborate on code, why not take advantage of GitHub’s project management capabilities? In GitHub, issues from your repos can be assigned to projects (which sit at a level above repos), allowing an overarching view of all the tasks associated with a project. Issues can also be created in the project itself. Both kinds of issues can be assigned to individuals, marked with topical tags, and assigned start and due dates. The examples below draw from the repository we are using to organize the first iteration of this course.\n\n\n\nFigure 5. Organizing with GitHub Projects\n\n\nTwo additional views make it even easier to track overall progress and individual workloads. The “roadmap” view in GitHub functions like an interactive Gantt chart, once start and target dates are assigned to an issue. In the roadmap view, issues can by dragged forward and back in time and the issue’s start or end date will adjust accordingly. The stack of issues can be sorted by start date, target date, topic tags or assigned collaborator.\nThe “sliced tasks view” also allows each collaborator to see only their own assigned tasks.\n\n\n\nFigure 5. Gantt Chart Example using GitHub Projects\n\n\n\n\n\n\n\n\nProject Group Discussion: Develop a project timeline/Gantt chart\n\n\n\nInstructions are included for using either GitHub or a Google sheet template to complete a project Gantt chart. GitHub has more capabilities, especially for accessing individual task lists, but the Google Sheet template is included in case group members are not yet comfortable with GitHub.\n\nBreakout into project groups.\nDecide whether you will use the Google sheet template or GitHub.\nIf using GitHub, go to your project repository and find the project that has been created for you (top center menu). It will open in a Kanban Board view.\n\nBegin adding tasks/issues (bottom of each list). Click on the task title to open it and add a brief description. A nice touch here is to add a checklist of subtasks.\nChoose task leads and add them to each task card (supporting group members can also be added).\nAdd approximate start dates and target end dates for each task.\nWhen you have most of your major tasks entered, switch to the roadmap view and shift the timelines as needed by extending the boxes or dragging them forward or back.\nReview the chart. What appear to be the bottlenecks? How could you mitigate them? Can subgroups work simultaneously on different sub-projects? Do you need to think about re-scoping the project?\n\nIf using the Google sheet template, copy it into your project folder. The template has two tabs. The manual one requires you to update the dates and graphical representation separately, but is a little more intuitive to edit and allows you to change bar color by assigned task lead. The “basic” tab will update the graph when you update task dates.\n\nWorking backwards from the end of May, estimate how long each task will take and enter it in the spreadsheet.\nAdd or remove steps as needed for your proposed project.\nChoose task leads (they should expect to draw on other group members for assistance and feedback)\nReview the chart. What appear to be the bottlenecks? How could you mitigate them? Can subgroups work simultaneously on different sub-projects? Do you need to think about re-scoping the project?\nRevise task dates as needed.\n\nRegardless of which tool you used, decide on a plan for checking in and updating the Gantt chart. Every group meeting? Every Monday? Once a month?\n\nAfter 30 minutes, return to whole group and discuss any insights or innovations.",
    "crumbs": [
      "Phase II -- Plan",
      "Project Management"
    ]
  },
  {
    "objectID": "mod_project-mgmt.html#additional-resources",
    "href": "mod_project-mgmt.html#additional-resources",
    "title": "Project Management",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nWorkshops & Courses\n\nLTER Scientific Computing Team. Collaborative Coding with GitHub: Issues. 2024.\nLTER Scientific Computing Team. Collaborative Coding with GitHub: Milestones. 2024.\nLTER Scientific Computing Team. Collaborative Coding with GitHub: Projects. 2024.\nNCEAS Learning Hub & Delta Stewardship Council. Open Science Synthesis: Logic Models and Synthesis Development. 2021.\nNCEAS Project Management Workshop for new synthesis working groups\n\n\n\nWeb-based Project Management Tools\n\nOpen Source\n\nGitHub Documentation: Issues and Projects\n\n\n\nCommercial\n\nAsana. Task and project management. Free up to 10 members.\nWrike. Tasks and project management. Capable for individuals. $10 pp per month for teams.\nClickUp. Capable free plan. Less intuitive to set up.\nMiro. Free for up to three projects. More focus on diagrams and brianstorming than ongoing project management.\nEasy Retro",
    "crumbs": [
      "Phase II -- Plan",
      "Project Management"
    ]
  },
  {
    "objectID": "mod_reproducibility.html",
    "href": "mod_reproducibility.html",
    "title": "Reproducibility Best Practices",
    "section": "",
    "text": "As we set out to engage with the synthesis skills this course aims to offer, it will be helpful to begin with a careful consideration of “reproducibility.” Because synthesis projects draw data from many sources and typically involve many researchers working in concert, reproducibility is particularly important. In individual projects, adhering to reproducibility best practices is certainly a good goal but failing to do so for synthesis projects can severely limit the work in a more significant way than for those individual projects. “Reproducibility” is a wide sphere encompassing many different–albeit related–topics so it can be challenging to feel well-equipped to evaluate how well we are following these guidelines in our own work. In this module, we will cover a few fundamental facets of reproducibility and point to some considerations that may encourage you to push yourself to elevate your practices to the next level.",
    "crumbs": [
      "Phase I -- Prepare",
      "Reproducibility"
    ]
  },
  {
    "objectID": "mod_reproducibility.html#overview",
    "href": "mod_reproducibility.html#overview",
    "title": "Reproducibility Best Practices",
    "section": "",
    "text": "As we set out to engage with the synthesis skills this course aims to offer, it will be helpful to begin with a careful consideration of “reproducibility.” Because synthesis projects draw data from many sources and typically involve many researchers working in concert, reproducibility is particularly important. In individual projects, adhering to reproducibility best practices is certainly a good goal but failing to do so for synthesis projects can severely limit the work in a more significant way than for those individual projects. “Reproducibility” is a wide sphere encompassing many different–albeit related–topics so it can be challenging to feel well-equipped to evaluate how well we are following these guidelines in our own work. In this module, we will cover a few fundamental facets of reproducibility and point to some considerations that may encourage you to push yourself to elevate your practices to the next level.",
    "crumbs": [
      "Phase I -- Prepare",
      "Reproducibility"
    ]
  },
  {
    "objectID": "mod_reproducibility.html#learning-objectives",
    "href": "mod_reproducibility.html#learning-objectives",
    "title": "Reproducibility Best Practices",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nIdentify core tenets of reproducibility best practices\nCreate robust workflow documentation\nImplement reproducible project organization strategies\nDiscuss methods for improving the reproducibility of your code products\nSummarize FAIR and CARE data principles\nEvaluate the FAIR/CAREness of your work",
    "crumbs": [
      "Phase I -- Prepare",
      "Reproducibility"
    ]
  },
  {
    "objectID": "mod_reproducibility.html#preparation",
    "href": "mod_reproducibility.html#preparation",
    "title": "Reproducibility Best Practices",
    "section": "Preparation",
    "text": "Preparation\nThere is no suggested preparatory work for this module.",
    "crumbs": [
      "Phase I -- Prepare",
      "Reproducibility"
    ]
  },
  {
    "objectID": "mod_reproducibility.html#lego-activity",
    "href": "mod_reproducibility.html#lego-activity",
    "title": "Reproducibility Best Practices",
    "section": "Lego Activity",
    "text": "Lego Activity\nBefore we dive into the world of reproducibility for synthesis projects, we thought it would be fun (and informative!) to begin with an activity that is a useful analogy for the importance of some of the concepts we’ll cover today. The LEGO activity was designed by Mary Donaldson and Matt Mahon at the University of Glasgow. The full materials can be accessed here.",
    "crumbs": [
      "Phase I -- Prepare",
      "Reproducibility"
    ]
  },
  {
    "objectID": "mod_reproducibility.html#project-documentation-organization",
    "href": "mod_reproducibility.html#project-documentation-organization",
    "title": "Reproducibility Best Practices",
    "section": "Project Documentation & Organization",
    "text": "Project Documentation & Organization\nMuch of the popular conversation around reproducibility centers on reproducibility as it pertains to code. That is definitely an important facet but before we write even a single line it is vital to consider project-wide reproducibility. “Perfect” code in a project that isn’t structured thoughtfully can still result in a project that isn’t reproducible. On the other hand, “bad” code can be made more intelligible when it is placed in a well-documented/organized project!\n\nDocumentation\nDocumenting a project can feel daunting but it is often not as hard as one might imagine and always well worth the effort! One simple practice you can adopt to dramatically improve the reproducibility of your project is to create a “README” file in the top-level of your project’s folder system. This file can be formatted however you’d like but generally READMEs should include:\n\nProject overview written in plain language\nBasic table of contents for the primary folders in your project folder\nBrief description of the file naming scheme you’ve adopted for this project.\n\nYour project’s README becomes the ‘landing page’ for those navigating your repository and makes it easy for team members to know where documentation should go (in the README!). You may also choose to create a README file for some of the sub-folders of your project. This can be particularly valuable for your “data” folder(s) as it is an easy place to store data source/provenance information that might be overwhelming to include in the project-level README file.\nFinally, you should choose a place to keep track of ideas, conversations, and decisions about the project. While you can take notes on these topics on a piece of paper, adopting a digital equivalent is often helpful because you can much more easily search a lengthy document when it is machine readable. We will discuss GitHub during the Version Control module but GitHub offers something called Issues that can be a really effective place to record some of this information.\n\n\n\n\n\n\nActivity: Create a README\n\n\n\nCreate a draft README for one of your research projects. If all of your projects already have READMEs (very impressive!) revisit the one with the least detail.\n\nInclude a 2-4 sentence description of the project objectives / hypotheses\nIdentify and describe (in 1 sentence) the primary sub-folders in the project\nIf your chosen project includes scripts, summarize each and indicate which script(s) they depend on and which depend on them\n\nFeel free to put your personal flair on the README! If there is other information you feel would be relevant to an outsider looking at your project, you can definitely add that.\n\n\n\n\nFundamental Structure\n\nThe simplest way of beginning a reproducible project is adopting a good file organization system. There is no single “best” way of organizing your projects’ files as long as you are consistent. Consistency will make your system–whatever that consists of–understandable to others.\nHere are some rules to keep in mind as you decide how to organize your project:\n\nUse one folder per project\n\nKeeping all inputs, outputs, and documentation in a single folder makes it easier to collaborate and share all project materials. Also, most programming applications (RStudio, VS Code, etc.) work best when all needed files are in the same folder.\nNote that how you define “projct” may affect the number of folders you need! Some synthesis projects may separate data harmonization into its own project while for others that same effort might not warrant being considered as a separate project. Similarly, you may want to make a separate folder for each manuscript your group plans on writing so that the code for each paper is kept separate.\n\nOrganize content with sub-folders\n\nPutting files that share a purpose or source into logical sub-folders is a great idea! This makes it easy to figure out where to put new content and reduces the effort of documenting project organization. Don’t feel like you need to use an intricate web of sub-folders either! Just one level of sub-folders is enough for many projects.\n\nCraft informative file names\n\nAn ideal file name should give some information about the file’s contents, purpose, and relation to other project files. Some of that may be reinforced by folder names, but the file name itself should be inherently meaningful. This lets you change folder names without fear that files would also need to be re-named.\n\n\n\n\n\n\nDiscussion: Project Structure\n\n\n\nWith a partner discuss (some of) the following questions:\n\nHow do you typically organize your projects’ files?\nWhat benefits do you see of your current approach?\nWhat–if any–limitations to your system have you experienced?\nDo you think your structure would work well in a team environment?\n\nIf not, what changes might you make to better fit that context?\n\n\n\n\n\nNaming Tips\nWe’ve brought up the importance of naming several times already but haven’t actually discussed the specifics of what makes a “good” name for a file or folder. Consider the adopting some (or all!) of the file name tips we outline below.\n\nNames should be sorted by a computer and human in the same way\n\nComputers sort files/folders alphabetically and numerically. Sorting alphabetically rarely matches the order scripts in a workflow should be run. If you add step numbers to the start of each file name the computer will sort the files in an order that makes sense for the project. You may also want to “zero pad” numbers so that all numbers have the same number of digits (e.g., “01” and “10” vs. “1” and “10”).\n\nNames should avoid spaces and special characters\n\nSpaces and special characters (e.g., é, ü, etc.) cause errors in some computers (particularly Windows operating systems). You can replace spaces with underscores or hyphens to increase machine readability. Avoid using special characters as much as possible. You should also be consistent about casing (i.e., lower vs. uppercase).\n\nNames should use consistent delimiters\n\nDelimiters are characters used to separate pieces of information in otherwise plain text. Underscores are a commonly used example of this. If a file/folder name has multiple pieces of information, you can separate these with a delimiter to make them more readable to people and machines. For example, you could name a folder “coral_reef_data” which would be more readable than “coralreefdata”.\nYou may also want to use multiple delimiters to indicate different things. For instance, you could use underscores to differentiate categories and then use hyphens instead of spaces between words.\n\nNames should use “slugs” to connect inputs and outputs\n\nSlugs are human-readable, unique pieces of file names that are shared between files and the outputs that they create. Maybe a script is named “02_tidy.R” and all of the data files it creates are named “02_…”. Weird or unlikely outputs are easily traced to the scripts that created them because of their shared slug.\n\n\n\nOrganizing Example\nThese tips are all worthwhile but they can feel a little abstract without a set of files firmly in mind. Let’s consider an example synthesis project where we incrementally change the project structure to follow increasing more of the guidelines we suggest above.\n\nVersion 1Version 2Version 3Version 4\n\n\n\n\n\n\n\nPositives\n\nAll project files are in one folder\n\n\n\nAreas for Improvement\n\nNo use of sub-folders to divide logically-linked content\nFile names lack key context (e.g., workflow order, inputs vs. outputs, etc.)\nInconsistent use of delimiters\n\n\n\n\n\n\n\n\n\n\n\nPositives\n\nSub-folders used to divide content\nProject documentation included in top level (README and license files)\n\n\n\nAreas for Improvement\n\nFile names still inconsistent\n\nFile names contain different information in different order\nMixed use of delimiters\nMany file names include spaces\n\n\n\n\n\n\n\n\n\n\n\n\nPositives\n\nMost file names contain context\nStandardized use of casing and–within sub-folder–consistent delimiters used\n\n\n\nAreas for Improvement\n\nWorkflow order “guessable” but not explicit\nUnclear which files are inputs / outputs (and of which scripts)\n\n\n\n\n\n\n\n\n\n\n\nPositives\n\nScripts include zero-padded numbers indicating order of operations\nInputs / outputs share zero padded slug with source script\nReport file names machine sorted from least to most recent (top to bottom)\n\n\n\nAreas for Improvement\n\nDepending on sub-folder complexity, could add sub-folder specific README files\nGraph file names still include spaces\n\n\n\n\n\n\n\n\n\nOrganization Recommendations\nIf you integrate any of the concepts we’ve covered above you will find the reproducibility and transparency of your project will greatly increase. However, if you’d like additional recommendations we’ve assembled a non-exhaustive set of additional “best practices” that you may find helpful.\n\nNever Edit Raw Data\nFirst and foremost, it is critical that you never edit the raw data directly. If you do need to edit the raw data, use a script to make all needed edits and save the output of that script as a separate file. Editing the raw data directly without a script or using a script but overwriting the raw data are both incredibly risky operations because your create a file that “looks” like the raw data (and is likely documented as such) but differs from what others would have if they downloaded the ‘real’ raw data personally.\n\n\nSeparate Raw and Processed Data\nIn the same vein as the previous best practice, we recommend that you separate the raw and processed data into separate folders. This will make it easier to avoid accidental edits to the raw data and will make it clear what data are created by your project’s scripts; even if you choose not to adopt a file naming convention that would make this clear.\n\n\nQuarantine External Outputs\nThis can sound harsh, but it is often a good idea to “quarantine” outputs received from others until they can be carefully vetted. This is not at all to suggest that such contributions might be malicious! As you embrace more of the project organization recommendations we’ve described above outputs from others have more and more opportunities to diverge from the framework you establish. Quarantining inputs from others gives you a chance to rename files to be consistent with the rest of your project as well as make sure that the style and content of the code also match (e.g., use or exclusion of particular packages, comment frequency and content, etc.)",
    "crumbs": [
      "Phase I -- Prepare",
      "Reproducibility"
    ]
  },
  {
    "objectID": "mod_reproducibility.html#reproducible-coding",
    "href": "mod_reproducibility.html#reproducible-coding",
    "title": "Reproducibility Best Practices",
    "section": "Reproducible Coding",
    "text": "Reproducible Coding\nNow that you’ve organized your project in a reasonable way and documented those choices, we can move on to principles of reproducible coding! Doing your data operations with scripts is more reproducible than doing those operations without a programming language (i.e., with Microsoft Excel, Google Sheets, etc.). However, scripts are often written in a way that is not reproducible. A recent study aiming to run 2,000 project’s worth of R code found that 74% of the associated R files failed to complete without error (Trisovic et al. 2022). Many of those errors involve coding practices that hinder reproducibility but are easily preventable by the original code authors.\n\nWhen your scripts are clear and reproducibly-written you will reap the following benefits:\n\nReturning to your code after having set it down for weeks/months is much simpler\nCollaborating with team members requires less verbal explanation\nSharing methods for external result validation is more straightforward\nIn cases where you’re developing a novel method or workflow, structuring your code in this way will increase the odds that someone outside of your team will adopt your strategy\n\n\nCode and the Stages of Data\nYou’ll likely need a number of scripts to accomplish the different stages of preparing a synthesized dataset. All of these scripts together are often called a “workflow.” Each script will meet a specific need and its outputs will be the inputs of the next script. These intermediary data products are sometimes useful in and of themselves and tend to occur and predictable points that exist in most code workflows.\nRaw data will be parsed into cleaned data–often using idiosyncratic or dataset-specific scripts–which is then processed into standardized data which can then be further parsed into published data products. Because this process results in potentially many scripts, coding reproducibly is vital to making this workflow intuitive and easy to maintain.\nYou don’t necessarily need to follow all of the guidelines described below but in general, the more of these guidelines you follow the easier it will be to make needed edits, onboard new teammembers, maintain the workflow in the long term, and generally maximize the value of your work to yourself and others!\n\n\n\nDiagram of data stages from raw data to published products. Credit: Margaret O’Brian & Li Kui\n\n\n\n\nPackages, Namespacing, and Software Versions\nAn under-appreciated facet of reproducible coding is a record of what code packages are used in a particular script and the version number of those packages. Packages evolve over time and code that worked when using one version of a given package may not work for future versions of that same package. Perpetually updating your code to work with the latest package versions is not sustainable but recording key information can help users set up the code environment that does work for your project.\n\nLoad Libraries Explicitly\nIt is important to load libraries at the start of every script. In some languages (like Python) this step is required but in others (like R) this step is technically “optional” but disastrous to skip. It is safe to skip including the installation step in your code because the library step should tell code-literate users which packages they need to install.\nFor instance you might begin each script with something like:\n# Load needed libraries\nlibrary(dplyr); library(magrittr); library(ggplot2)\n\n# Get to actual work\n. . .\nIn R the semicolon allows you to put multiple code operations in the same line of the script. Listing the needed libraries in this way cuts down on the number of lines while still being precise about which packages are needed in the script.\nIf you are feeling generous you could use the librarian R package to install packages that are not yet installed and simultaneously load all needed libraries. Note that users would still need to install librarian itself but this at least limits possible errors to one location. This is done like so:\n# Load `librarian` package\nlibrary(librarian)\n\n# Install missing packages and load needed libraries\nshelf(dplyr, magrittr, ggplot2)\n\n# Get to actual work\n. . .\n\n\nFunction Namespacing\nIt is also strongly recommended to “namespace” functions everywhere you use them. In R this is technically optional but it is a really good practice to adopt, particularly for functions that may appear in multiple packages with the same name but do very different operations depending on their source. In R the ‘namespacing operator’ is two colons.\n# Use the `mutate` function from the `dplyr` package\ndplyr::mutate(. . .)\nAn ancillary benefit of namespacing is that namespaced functions don’t need to have their respective libraries loaded. Still good practice to load the library though!\n\n\nPackage Versions\nWhile working on a project you should use the latest version of every needed package. However, as you prepare to publish or otherwise publicize your code, you’ll need to record package versions. R provides the sessionInfo function (from the utils package included in “base” R) which neatly summarizes some high level facets of your code environment. Note that for this method to work you’ll need to actually run the library-loading steps of your scripts.\nFor more in-depth records of package versions and environment preservation–in R–you might also consider the renv package or the packrat package.\n\n\n\nScript Organization\nEvery change to the data between the initial raw data and the finished product should be scripted. The ideal would be that you could hand someone your code and the starting data and have them be able to perfectly retrace your steps. This is not possible if you make unscripted modifications to the data at any point!\nYou may wish to break your scripted workflow into separate, modular files for ease of maintenance and/or revision. This is a good practice so long as each file fits clearly into a logical/thematic group (e.g., data cleaning versus analysis).\n\n\nFile Paths\nWhen importing inputs or exporting outputs we need to specify “file paths”. These are the set of folders between where your project is ‘looking’ and where the input/output should come from/go. The figure from Trisovic et al. (2022) shows that file path and working directory errors are a substantial barrier to code that can be re-run in clean coding environments. Consider the following ways of specifying file paths from least to most reproducible.\n\nWorstBadBetterBest!\n\n\n\nAbsolute Paths\nThe worst way of specifying a file path is to use the “absolute” file path. This is the path from the root of your computer to a given file. There are many issues here but the primary one is that absolute paths only work for one computer! Given that only one person can even run lines of code that use absolute paths, it’s not really worth specifying the other issues.\n\n\nExample\n# Read in bee community data\nmy_df &lt;- read.csv(file = \"~/Users/lyon/Documents/Grad School/Thesis (Chapter 1)/Data/bees.csv\")\n\n\n\n\nManually Setting the Working Directory\nMarginally better than using the absolute path is to set the working directory to some location. This may look neater than the absolute path option but it actually has the same point of failure: Both methods only work for one computer!\n\n\nExample\n# Set working directory\nsetwd(dir = \"~/Users/lyon/Documents/Grad School/Thesis (Chapter 1)\")\n\n# Read in bee community data\nmy_df &lt;- read.csv(file = \"Data/bees.csv\")\n\n\n\n\nRelative Paths\nInstead of using absolute paths or manually setting the working directory you can use “relative” file paths! Relative paths assume all project content lives in the same folder.\nThis is a safe assumption because it is the most fundamental tenet of reproducible project organization! The strength of relative paths is actually a serious contributing factor for why it is good practice to use a single folder.\n\n\nExample\n# Read in bee community data\n1my_df &lt;- read.csv(file = \"Data/bees.csv\")\n\n1\n\nParts of file path specific to each user are automatically recognized by the computer\n\n\n\n\n\n\nOperating System-Flexible Relative Paths\nThe “better” example is nice but has a serious limitation: it hard coded the type of slash between file path elements. This means that only computers of the same operating system as the code author could run that line.\nWe can use functions to automatically detect and insert the correct slashes though!\n\n\nExample\n# Read in bee community data\nmy_df &lt;- read.csv(file = file.path(\"Data\", \"bees.csv\"))\n\n\n\n\n\nFile Path Exception\nGenerally, the labels of the above tab panels are correct (i.e., it is better to use OS-agnostic relative paths). However, there is an important possible exception: how do you handle file paths when the data can’t live in the project folder? A common example of this is when data are stored in a cloud-based system (e.g., Dropbox, Box, etc.) and accessed via a “synced” folder in each local computer. Downloading files is thus unnecessary but the only way to import data from or export outputs to this folder is to specify an absolute file path unique to each user (even though the folders inside the main synced folder are shared among users).\nThe LTER Scientific Computing team (members here) has created a nice tutorial on this topic but to summarize you should take the following steps:\n\nStore user-specific information in a JSON file\n\nConsider using ltertools::make_json\n\nTell Git to ignore that file\nWrite scripts to read this file and access user-specific information from it\n\nFollowing these steps allows you to use absolute paths to the synced folder while enabling relative paths everywhere else. Because the user-specific information is stored in a file ignored by Git you also don’t have to comment/uncomment your absolute path (or commit that ‘change’).\n\n\n\nCode Style\nWhen it comes to code style, the same ‘rule of thumb’ applies here that applied to project organization: virtually any system will work so long as you (and your team) are consistent! That said, there are a few principles worth adopting if you have not already done so.\n\nUse concise and descriptive object names\n\nIt can be difficult to balance these two imperatives but short object names are easier to re-type and visually track through a script. Descriptive object names on the other hand are useful because they help orient people reading the script to what the object contains.\n\nDon’t be afraid of empty space!\n\nScripts are free to write regardless of the number of lines so do not feel as though there is a strict character limit you need to keep in mind. Cramped code is difficult to read and thus can be challenging to share with others or debug on your own. Inserting an empty line between coding lines can help break up sections of code and putting spaces before and after operators can make reading single lines much simpler.\n\n\n\nCode Comments\nA “comment” in a script is a human readable, non-coding line that helps give context for the code. In R (and Python), comment lines start with a hashtag (#). Including comments is a low effort way of both (A) creating internal documentation for the script and (B) increasing the reproducibility of the script. It is difficult to include “too many” comments, so when in doubt: add more comments!\nThere are two major strategies for comments and either or both might make sense for your project.\n\n“What” Comments\nComments describe what the code is doing.\n\nBenefits: allows team members to understand workflow without code literacy\nRisks: rationale for code not explicit\n\n# Remove all pine trees from dataset\nno_pine_df &lt;- dplyr::filter(full_df, genus != \"Pinus\")\n\n\n“Why” Comments\nComments describe rationale and/or context for code.\n\nBenefits: built-in documentation for team decisions\nRisks: assumes everyone can read code\n\n# Cone-bearing plants are not comparable with other plants in dataset\nno_pine_df &lt;- dplyr::filter(full_df, genus != \"Pinus\")\n\n\n\n\n\n\nDiscussion: Comment on Comments\n\n\n\nWith a partner discuss the following questions:\n\nWhen you write comments, do you focus more on the “what” or the “why”?\nWhat would you estimate is the ratio of code to comment lines in your code?\n\n1:1 being every code line has one comment line\n\nIf you have revisited old code, were your comments helpful?\n\nHow could you make them more helpful?\n\nIn what ways do you think you would need to change your commenting style for a team project?\n\n\n\n\n\n\n\n\n\nActivity: Make Comments\n\n\n\nRevisit a script from an old project (ideally one you haven’t worked on recently). Once you’ve opened the script:\n\nScan through the script\n\nCan you identify the main purpose(s) of the code?\n\nIdentify any areas where you’re not sure either (A) what the code is doing or (B) why that section of code exists\n\nAdd comments to these areas to document what they’re up to\n\nShare the commented version of one of these trouble areas with a partner\n\nDo they understand the what and/or why of your code?\nIf not, revise the comments and repeat\n\n\n\n\n\n\n\nConsider Custom Functions\nIn most cases, duplicating code is not good practice. Such duplication risks introducing a typo in one copy but not the others. Additionally, if a decision is made later on that requires updating this section of code, you must remember to update each copy separately.\nInstead of taking this copy/paste approach, you could consider writing a “custom” function that fits your purposes. All instances where you would have copied the code now invoke this same function. Any error is easily tracked to the single copy of the function and changes to that step of the workflow can be accomplished in a centralized location.\n\nFunction Recommendations\nWe have the following ‘rules of thumb’ for custom function use:\n- If a given operation is duplicated 3 or more times within a project, write a custom function\nFunctions written in this case can be extremely specific and–though documentation is always a good idea–can be a little lighter on documentation. Note that the reason you can reduce the emphasis on documentation is only because of the assumption that you won’t be sharing the function widely. If you do decide the function could be widely valuable you would need to add the needed documentation post hoc.\n- Write functions defensively\nWhen you write custom functions, it is really valuable to take the time to write them defensively. In this context, “defensively” means that you anticipate likely errors and write your own informative/human readable error messages. Let’s consider a simplified version of a function from the ltertools R package for calculating the coefficient of variation (CV).\nThe coefficient of variation is equal to the standard deviation divided by the mean. Fortunately, R provides functions for calculating both of these already and both expect numeric vectors. If either of those functions is given a non-number you get the following warning message: “In mean.default(x =”…“) : argument is not numeric or logical: returning NA”.\nSomeone with experience in R may be able to interpret this error but for many users this error message is completely opaque. In the function included below however we can see that there is a simpler, more human readable version of the error message and the function is stopped before it can ever reach the part of the code that would throw the warning message included above.\ncv &lt;- function(x){\n  \n  # Error out if x is not numeric\n  if(is.numeric(x) != TRUE)\n    stop(\"`x` must be numeric\")\n  \n  # Calculate CV\n  sd(x = x) / mean(x = x)\nThe key to defensive programming is to try to get functions to fail fast and fail informatively as soon as a problem is detected! This is easier to debug and understand for coders with a range of coding expertise and–for complex functions–can save a ton of useless processing time when failure is guaranteed at a later step.\n- If a given operation is duplicated 3 or more times across projects, consider creating an R package\nCreating an R package can definitely seem like a daunting task but duplication across projects carries the same weaknesses of excessive duplication within a project. However, when duplication is across projects, not even writing a custom function saves you because you need to duplicate that function’s script for each project that needs the tool.\nHadley Wickham and Jenny Bryan have written a free digital book on this subject that demystifies a lot of this process and may make you feel more confident to create your own R package if/when one is needed.\nIf you do take this path, you can simply install your package as you would any other in order to have access to the operations rather than creating duplicates by hand.",
    "crumbs": [
      "Phase I -- Prepare",
      "Reproducibility"
    ]
  },
  {
    "objectID": "mod_reproducibility.html#fair-care-data-principles",
    "href": "mod_reproducibility.html#fair-care-data-principles",
    "title": "Reproducibility Best Practices",
    "section": "FAIR & CARE Data Principles",
    "text": "FAIR & CARE Data Principles\nData availability, data size, and demand for transparency by government and funding agencies are all steadily increasing. While ensuring that your project and code practices are reproducible is important, it is also important to consider how open and reproducible your data are as well. Synthesis projects are in a unique position here because synthesis projects use data that may have been previously published on and/or be added to a public data repository by the original data collectors. However, synthesis projects take data from these different sources and wrangle it such that the different data sources are comparable to one another. These ‘synthesis data products’ can be valuable to consider archiving in a public repository to save other researchers from needing to re-run your entire wrangling workflow in order to get the data product. In either primary or synthesis research contexts there are several valuable frameworks to consider as data structure and metadata are being decided. Among these are the FAIR and CARE data principles.\n\nFAIR\nFAIR is an acronym for Findable Accessible Interpoerable and Reusable. Each element of the FAIR principles can be broken into a set of concrete actions that you can take throughout the lifecycle of your project to ensure that your data are open and transparent. Perhaps most importantly, FAIR data are most easily used by other research teams in the future so the future impact of your work is–in some ways–dependent upon how thoroughly you consider these actions.\nConsider the following list of actions you might take to make your data FAIR. Note that not all actions may be appropriate for all types of data (see our discussion of the CARE principles below), but these guidelines are still important to consider–even if you ultimately choose to reject some of them. Virtually all of the guidelines considered below apply to metadata (i.e., the formal documentation describing your data) and the ‘actual’ data but for ease of reference we will call both of these resources “data.”\n\nFindability\n\nEnsure that data have a globally unique and persistent identifier\nCompletely fill out all metadata details\nRegister/index data in a searchable resource\n\nAccessibility\n\nStore data in a file format that is open, free, and universally implementable (e.g., CSV rather than MS Excel, etc.)\nEnsure that metadata will be available even if the data they describe are not\n\nInteroperability\n\nUse formal, shared, and broadly applicable language for knowledge representation\n\nThis can mean using full species names rather than codes or shorthand that may not be widely known\n\nUse vocabularies that are broadly used and that themselves follow FAIR principles\nInclude explicit references to other FAIR data\n\nReusability\n\nDescribe your data with rich detail that covers a plurality of relevant attributes\nAttach a clear data usage license so that secondary data users know how they are allowed to use your data\nInclude detailed provenance information about your data\nEnsure that your data meet discipline-specific community standards\n\n\n\n\n\n\n\nDiscussion: Consider Data FAIRness\n\n\n\nConsider the first data chapter of your thesis or dissertation. On a scale of 1-5, how FAIR do you think your data and metadata are? What actions could you take to make your data more FAIR compliant? If it helps, feel free to rate your (meta)data based on each FAIR criterion separately!\nFeel free to use these questions to guide your thinking\n\nHow are the data for that project stored?\nWhat metadata exists to document those data?\nHow easy would it be for someone in your lab group to pick up and use your data?\nHow easy would it be for someone not in your lab group?\n\n\n\n\n\nCARE\nWhile making data and code more FAIR is often a good ideal the philosophy behind those four criteria come from a perspective that emphasizes data sharing as a good in and of itself. This approach can ignore historical context and contemporary power differentials and thus be insufficient as the sole tool to use in evaluating how data/code are shared and stored. The Global Indigenous Data Alliance (GIDA) created the CARE principles with these ethical considerations explicitly built into their tenets. Before making your data widely available and transparent (ideally before even beginning your research), it is crucial to consider this ethical dimension.\n\nCARE stands for Collective Benefit, Authority to Control, Responsibility, and Ethics. Ensuring that your data meet these criteria helps to advance Indigenous data sovereignty and respects those who have been–and continue to be–collecting knowledge about the world around us for millennia. The following actions are adapted from Jennings et al. 2023 (linked at the bottom of this page).\nCollective Benefit\n\nDemonstrate how your research and its potential results are relevant and of value to the interests of the community at large and its individual members\nInclude and value local community experts in the research team\nUse classifications and categories in ways defined by Indigenous communities and individuals\nDisaggregate large geographic scale data to increase relevance for place-specific Indigenous priorities\nCompensate community experts throughout the research process (proposal development through to community review of pre-publication manuscripts)\n\nAuthority to Control\n\nEstablish institutional principles or protocols that explicitly recognize Indigenous Peoples’ rights to and interests in their knowledges/data\nEnsure data collection and distribution are consistent with individual and community consent provisions and that consent is ongoing (including the right to withdraw or refuse)\nEnsure Indigenous communities have access to the (meta)data in usable format\n\nResponsibility\n\nCreate and expand opportunities for community capacity\nRecord the Traditional Knowledge and biocultural labels of the Local Contexts Hub in metadata\nEnsure review of draft publications before publication\nUse the languages of Indigenous Peoples in the (meta)data\n\nEthics\n\nAccess research using Indigenous ethical frameworks\nUse community-defined review processes with appropriate reviewers for activities delineated in data management plans\nWork to maximize benefits from the perspectives of Indigenous Peoples by clear and transparent dialogue with communities and individuals\nEngage with community guidelines for the use and reuse of data (including facilitating data removal and/or disposal requests from aggregated datasets)",
    "crumbs": [
      "Phase I -- Prepare",
      "Reproducibility"
    ]
  },
  {
    "objectID": "mod_reproducibility.html#reproducibility-best-practices-summary",
    "href": "mod_reproducibility.html#reproducibility-best-practices-summary",
    "title": "Reproducibility Best Practices",
    "section": "Reproducibility Best Practices Summary",
    "text": "Reproducibility Best Practices Summary\nMaking sure that your project is reproducible requires a handful of steps before you begin, some actions during the life of the project, and then a few finishing touches when the project nears its conclusion. The following diagram may prove helpful as a coarse roadmap for how these steps might be followed in a general project setting.",
    "crumbs": [
      "Phase I -- Prepare",
      "Reproducibility"
    ]
  },
  {
    "objectID": "mod_reproducibility.html#additional-resources",
    "href": "mod_reproducibility.html#additional-resources",
    "title": "Reproducibility Best Practices",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\nBritish Ecological Society (BES). Better Science Guides: Reproducible Code. 2024.\nEnglehardt, C. et al. FAIR Teaching Handbook. 2024.\nJennings, L. et al. Applying the ‘CARE Principles for Indigenous Data Governance’ to Ecology and Biodiversity Research. 2023. Nature Ecology & Evolution\nWickham, H. & Bryan, J. R Packages (2nd ed.). 2023.\nTrisovic, A. et al. A Large-Scale Study on Research Code Quality and Execution. 2022. Scientific Data\n\n\n\nWorkshops & Courses\n\nCsik, S. et al. UCSB Master of Environmental Data Science (MEDS) README Guidelines. 2024.\nThe Carpentries. Data Analysis and Visualization in R for Ecologists: Before We Start. 2024.\nThe Carpentries. Introduction to R for Geospatial Data: Project Management with RStudio. 2024.\nNational Center for Ecological Analysis and Synthesis (NCEAS) Learning Hub. coreR: FAIR and CARE Principles. 2023.\nNational Center for Ecological Analysis and Synthesis (NCEAS) Learning Hub. coreR: Reproducibility & Provenance. 2023.\n\n\n\nWebsites\n\nBriney, K. Research Data Management Workbook. 2024.\nGoogle. Style Guide. 2024.\nLTER Scientific Computing Team. Team Coding: 5 Essentials. 2024.\nLowndes, J.S. et al. Documenting Things: Openly for Future Us. 2023. posit::conf(2023)\nWickham, H. Advanced R: Style Guide. (1st ed.). 2019.\nvan Rossum, G. et al. PEP 8: Style Guide for Python Code. 2013. Python Enhancement Proposals",
    "crumbs": [
      "Phase I -- Prepare",
      "Reproducibility"
    ]
  },
  {
    "objectID": "mod_stats.html",
    "href": "mod_stats.html",
    "title": "Analysis & Modeling",
    "section": "",
    "text": "Given the wide range in statistical training in graduate curricula (and corresponding breadth of experience among early career researchers), we’ll be approaching this module in a different way than other modules. This module uses a “flipped approach” where project teams will share and discuss their proposed analyses with one another.\nThat said, one of our goals for these course materials is that they be a valuable resource to those who are not formally enrolled in the course and this flipped approach could result in a lack of information on a critical topic. To reduce this risk, we have created a typical instructional module dedicated to analyses that are more common in–or exclusive to–synthesis research that we hope is helpful.",
    "crumbs": [
      "Phase III -- Execute",
      "Analysis & Modeling"
    ]
  },
  {
    "objectID": "mod_stats.html#overview",
    "href": "mod_stats.html#overview",
    "title": "Analysis & Modeling",
    "section": "",
    "text": "Given the wide range in statistical training in graduate curricula (and corresponding breadth of experience among early career researchers), we’ll be approaching this module in a different way than other modules. This module uses a “flipped approach” where project teams will share and discuss their proposed analyses with one another.\nThat said, one of our goals for these course materials is that they be a valuable resource to those who are not formally enrolled in the course and this flipped approach could result in a lack of information on a critical topic. To reduce this risk, we have created a typical instructional module dedicated to analyses that are more common in–or exclusive to–synthesis research that we hope is helpful.",
    "crumbs": [
      "Phase III -- Execute",
      "Analysis & Modeling"
    ]
  },
  {
    "objectID": "mod_stats.html#learning-objectives",
    "href": "mod_stats.html#learning-objectives",
    "title": "Analysis & Modeling",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nDescribe proposed analytical methods to an interested audience of mixed prior experience\nExplain nuance in interpretation of results of proposed analyses\nIdentify some statistical tests common in synthesis research\nPerform some synthesis-specific analyses",
    "crumbs": [
      "Phase III -- Execute",
      "Analysis & Modeling"
    ]
  },
  {
    "objectID": "mod_stats.html#preparation",
    "href": "mod_stats.html#preparation",
    "title": "Analysis & Modeling",
    "section": "Preparation",
    "text": "Preparation\nEach project group should:\n\nPrepare a presentation (15-30 min) on their (A) synthesis question, (B) the data they are using to address that question, and (C) their proposed analytical methods\n\nThe presentation modality (e.g., slide deck, website, etc.) is up to the group!",
    "crumbs": [
      "Phase III -- Execute",
      "Analysis & Modeling"
    ]
  },
  {
    "objectID": "mod_stats.html#networking-session",
    "href": "mod_stats.html#networking-session",
    "title": "Analysis & Modeling",
    "section": "Networking Session",
    "text": "Networking Session\n\n2024 Guests\n\n\nTo Be Determined!",
    "crumbs": [
      "Phase III -- Execute",
      "Analysis & Modeling"
    ]
  },
  {
    "objectID": "mod_stats.html#team-presentations-of-proposed-methods",
    "href": "mod_stats.html#team-presentations-of-proposed-methods",
    "title": "Analysis & Modeling",
    "section": "Team Presentations of Proposed Methods",
    "text": "Team Presentations of Proposed Methods\nContent produced by project teams during the presentation component of this module may be included in the tab panels below or in the ‘Additional Resources’ section at the bottom of this module at the discretion of each team.\n\n2024 Methods\n\n\nFor more information on project teams, see here.",
    "crumbs": [
      "Phase III -- Execute",
      "Analysis & Modeling"
    ]
  },
  {
    "objectID": "mod_stats.html#needed-packages",
    "href": "mod_stats.html#needed-packages",
    "title": "Analysis & Modeling",
    "section": "Needed Packages",
    "text": "Needed Packages\nIf you’d like to follow along with the code chunks included throughout this module, you’ll need to install the following packages:\n\n# Note that these lines only need to be run once per computer\n## So you can skip this step if you've installed these before\ninstall.packages(\"tidyverse\")\ninstall.packages(\"lmerTest\")\ninstall.packages(\"palmerpenguins\")\ninstall.packages(\"esc\")\n\nWe’ll go ahead and load some of these libraries as well to be able to better demonstrate these concepts.\n\n# Load needed libraries\nlibrary(tidyverse)\nlibrary(lmerTest)\nlibrary(palmerpenguins)",
    "crumbs": [
      "Phase III -- Execute",
      "Analysis & Modeling"
    ]
  },
  {
    "objectID": "mod_stats.html#hypothesis-framing-aside",
    "href": "mod_stats.html#hypothesis-framing-aside",
    "title": "Analysis & Modeling",
    "section": "Hypothesis Framing Aside",
    "text": "Hypothesis Framing Aside\nBefore we dive in, we should discuss two of the ways in which you can frame your hypothesis and the differences in interpretation and appropriate statistical tool(s) that follow from that choice. We’ll restrict our conversation here to two alternate modes of thinking about your hypothesis: frequentist statistics versus multi-model inference.\nNote that this is something of a false dichotomy as tools from both worlds can be/are frequently used to complement one another. However, many graduate students are trained by instructors with strong feelings about one method in opposition to the other so it is worthwhile to consider these two paths separately even if you wind up using components of both in your own work.\n\nFrequentist InferenceMulti-Model Inference\n\n\nHypotheses here are a question of whether a variable has a “significant” effect on another. “Significant” has a very precise meaning in this context that has to do with p-values. Fundamentally, these methods focus on whether the observed relationship in the data is likely to be observed by chance alone or not. Strong effects are less likely–though not impossible–to be observed due to random chance.\nIf your hypothesis can be summarized as something along the lines of ‘we hypothesize that X affects Y’ then frequentist inference may be a more appropriate methodology.\nFor the purposes of SSECR, our discussion of frequentist inference will focus on mixed-effect models.\n\n\nHyoptheses here are a question of which variables explain the most variation in the data. Methods in this framing are unconcerned–or at least less concerned than in frequentist inference–with the probability associated with a particular variable. Intead, these methods focus on which of a set of user-defined candidate models explains most of the noise in the data even when that best model does not necessarily explain much of that variation in absolute terms.\nIf your hypothesis can be summarized as something along the lines of ‘we hypothesize that models including X explain more of the variation in Y than those that do not’ then multi-model inference may be a more appropriate methodology.\nFor the purposes of SSECR, our discussion of multi-model inference will focus on comparing model strengths with AIC.",
    "crumbs": [
      "Phase III -- Execute",
      "Analysis & Modeling"
    ]
  },
  {
    "objectID": "mod_stats.html#mixed-effects-models",
    "href": "mod_stats.html#mixed-effects-models",
    "title": "Analysis & Modeling",
    "section": "Mixed-Effects Models",
    "text": "Mixed-Effects Models\nIn any statistical test there is at least one response variable (a.k.a. “dependent” variable) and some number of explanatory variables (a.k.a. “independent” variables). However, in biology our experiments often involve repeated sampling over time or at the same locations. These variables (time or site) are neither response nor explanatory variables but we might reasonably conclude that they affect our response and/or explanatory variables.\nIn essence we want to use a statistical tool that asks ‘what is the effect of the explanatory variable(s) on the response when the variation due to these non-variable considerations is accounted for?’ Such tests are called mixed-effects models. This name derives from considering explanatory variables “fixed effects” and non-explanatory/response variables as “random effects”. Including both fixed and random effects thus creates a model with “mixed effects.”\n\nTypes of Random Effect\nThere are a few types of random effects but we can limit our conversation here to just two: random intercepts and random slopes.\n\nRandom InterceptsRandom Slopes\n\n\nRandom intercepts should be used when you expect that the average response differs among levels of that variable but not in a way that changes the relationship between each level of this variable and the other variables (either fixed or random). In statistical terms you want to allow the intercept to change with levels of this variable.\nFor example, let’s imagine that we are studying the effect of different organic farming practices on beneficial insect populations. We build relationships with several organic farmers willing to let us conduct this research on their properties and sample the insect communities at each farm over the course of a summer. However, we know that each farm is surrounded by a different habitat type that affects the composition of the local insect community. It is reasonable to expect that even farms where ‘the same’ management method is used are likely to differ because of this difference in landscape context.\nIn cases like this, we don’t want to include a term for ‘site’ as a fixed effect but we do want to account for those differences so that our assessment of the significance of our explanatory variables isn’t limited by the variation due to site.\n\n\nRandom slopes should be used when you expect that the average response differs among levels of that variable in a way that does change with other variables.\nFor example, let’s imagine that we are studying the effect of temperature on avian malaria rates in songbirds. We identify several sites–along a gradient of daytime temperature ranges–where our species of interest can be found, capture them, and measure malaria infection rates. However, unbeknownst to us at the start of our study, our study sites have varying populations of dragonflies which affects local mosquito populations and malaria transmission/infection rates. If we revisit our sites repeatedly for several years is is reasonable to expect that this difference among sites likely affects the relationship between daytime temperatures and songbird malaria rates.\nBy including site as a random slope in this context, we can account for this effect and still analyze our explanatory variables of interest. Note that random slopes are very “data hungry” so you may not be able to use them without very high replication in your study design.\n\n\n\n\n\nNested Random Effects\nTo further complicate matters, we can use nested random effects as well. These can be either random intercepts or random slopes though they are more commonly seen with random intercepts. A nested random effect accounts for the effect of one random variable that is itself affected by another variable! A classic example of this is when a study design uses two (or more) levels of spatial nestedness in their experimentall design.\nFor instance, let’s imagine we were conducting a global study of marine plankton biodiversity. To gether these data we took several cruises (scientific not–exclusively–pleasure) at different places around the world and during each cruise we followed a set of transects. In each transect we did several plankton tows and quantified the diversity of each tow. We can reasonably assume the following:\n\nEach cruise differs from each other cruise (due to any number of climatic/ecological factors)\n\nBut cruises within the same part of the world are still likely to have similar planktonic communities\n\nWithin each cruise, each transect differs from the others (again, due to unpreventable factors)\n\nBut transects within the same cruise are still likely to be more similar to one another than to transects in different cruises (even other ones in the same region!)\n\nWithin each transect, each plankton tow differs from one another!\n\nBut again, more similar to other tows in the same transect than other tows in different transects/cruises\n\n\nIf we put these assumptions together we realize we want to account for the variation of cruise, transect, and tow while still retaining the nestedness of the similarity among samples. A nested random effect where transect is nested inside of cruise and tow is nested inside of transect would capture this effectively!\n\n\nPhilosophical Note: Random vs. Fixed\nDeciding whether a given variable should be a fixed or random effect can be tough. You’ll likely need to rely on your scientific intuition about which feels more appropriate and then be prepared to defend that decision to your committee and/or “reviewer #2”. It may prove helpful though to consider whether you ‘care’ about the effect of that variable.\nIf your hypothesis includes that variable than it should likely be a fixed effect. If the variable is just a facet of your experimental design but isn’t something you’re necessarily interested in testing, then it should likely be a random effect. And, once you’ve made your decision, it is totally okay to change your mind and tweak the structure of your model!\n\n\n\n\n\n\nDiscussion: Random versus Fixed Effects\n\n\n\nWith a small group, decide whether you think the terms in the examples below should be fixed effects or random effects:\n\nYou study small mammal populations in urban settings\n\nShould ‘proportion green space’ be a fixed effect or a random effect?\n\nYou are part of a team studying leopard seal feeding behavior\n\nWhat type of effect should ‘observer’ be?\n\nYou study the gut microbiota of a particular beetle species\n\nShould ‘beetle sex’ be a fixed or a random effect?\nWhat about beetle life stage (e.g., larva versus adult)?\nWhat about the region of the gut from which the samples were taken?\n\nYou study vascular plant chemical defenses against herbivory\n\nShould phylogeny (i.e., evolutionary relatedness) be a fixed or random effect?\nWhat about feeding guild of the herbivore?\n\n\n\n\n\n\nMixed-Effects Case Study\nLet’s imagine we are researching tarantula populations for several years in the Chihuahuan Desert. Our hypothesis is that the number of tarantulas will be greater in sites further from the nearest road. We select ten study sites of varying distances from the nearest road and intensively count our furry friends at three plots within each site for several months. We return to our sites–and their associated plots–and repeat this process each year for three years. In the second year we have help from a new member of our lab but in the third year we’re back to working alone (they had their own project to handle by then). We enter our data and perform careful quality control to get it into a tidy format ready for analyis.\n\n# Read in data\ntarantula_df &lt;- read.csv(file = file.path(\"data\", \"tarantulas.csv\"))\n\n# Check structure\nstr(tarantula_df)\n\n'data.frame':   450 obs. of  6 variables:\n $ year           : int  2022 2022 2022 2022 2022 2022 2022 2022 2022 2022 ...\n $ road.dist_km   : num  64.5 64.5 64.5 64.5 64.5 ...\n $ site           : chr  \"site_A\" \"site_A\" \"site_A\" \"site_A\" ...\n $ plot           : chr  \"plot_a\" \"plot_a\" \"plot_a\" \"plot_a\" ...\n $ site.plot      : chr  \"A_a\" \"A_a\" \"A_a\" \"A_a\" ...\n $ tarantula_count: int  199 220 213 206 220 128 156 121 121 142 ...\n\n\nWith our data in hand, we now want to run some statistical tests and–hopefully–get some endorphine-inducingly small p-values. If we choose to simply ignore our possible random effects, we could fit a linear regression.\n\n# Fit model\n1tarantula_lm &lt;- lm(tarantula_count ~ road.dist_km, data = tarantula_df)\n\n# Extract summary\nsummary(tarantula_lm)\n\n\n1\n\nR syntax for statistical tests is response ~ explanatory a.k.a. Y ~ X\n\n\n\n\n\nCall:\nlm(formula = tarantula_count ~ road.dist_km, data = tarantula_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-216.916  -77.487    8.486   59.913  316.084 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -3.3935    14.8929  -0.228     0.82    \nroad.dist_km   2.8140     0.2775  10.141   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 100.2 on 448 degrees of freedom\nMultiple R-squared:  0.1867,    Adjusted R-squared:  0.1849 \nF-statistic: 102.8 on 1 and 448 DF,  p-value: &lt; 2.2e-16\n\n\nThis naive test seems to support our hypothesis. However, sampling effort differed between the three study years. Not only was there a second person in the second year but we can also reasonably expect that by the third year in this system we had greatly improved our tarantula-finding skills. So, a random effect of year is definitely justified. We are not concerned that the different study years will affect the relationship between tarantula populations and road distance though so a random intercept is fine.\nThere could be an argument for including year as a fixed effect in its own right but some preliminary investigations reveal no significant climatic differences across the region we worked in those three years. So, while we think that years may differ from one another, that difference is not something we care to analyze.\n\n# Fit the new model\ntarantula_mem1 &lt;- lmerTest::lmer(tarantula_count ~ road.dist_km + \n1                               (1|year),\n                             data = tarantula_df)\n\n# Extract summary\nsummary(tarantula_mem1)\n\n\n1\n\nThis is the syntax for specifying a random intercept (random slope variables should be before the | where 1 goes for a random intercept)\n\n\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: tarantula_count ~ road.dist_km + (1 | year)\n   Data: tarantula_df\n\nREML criterion at convergence: 5362.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.6578 -0.7941  0.1653  0.6517  2.9451 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n year     (Intercept) 1940     44.04   \n Residual             8747     93.52   \nNumber of obs: 450, groups:  year, 3\n\nFixed effects:\n             Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)   -3.3935    28.9768   3.1783  -0.117    0.914    \nroad.dist_km   2.8140     0.2589 446.0000  10.868   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nroad.dst_km -0.455\n\n\nBy including that random effect we actually get a slightly stronger effect of road distance (T value of ~12 without versus ~13 with). This is because our new random effect accounts for some of the ‘noise’ between study years. That actually gives us a better picture of the relationship between our response and explanatory variables.\nNow that we’re already using a mixed-effects model, we have little excuse not to account for the other potential random effect: plot! Remember that there were three plots within each site and from our extensive time in the field we have developed a strong intuition that there might be substantial among-plot variation at each site. We can make a quick exploratory graph to facilitate an ‘eyeball test’ of whether the data show what our intuition suggest.\n\nggplot(tarantula_df, aes(y = tarantula_count, x = plot, fill = plot)) +\n1  geom_violin(alpha = 0.5) +\n  geom_jitter(width = 0.25, size = 0.5) +\n  facet_wrap(site ~ .) +\n  theme_bw() +\n2  theme(axis.text.x = element_text(angle = 35, hjust = 1))\n\n\n1\n\nViolin plots are a nice alternative to boxplots because they allow visualizing data distributions directly rather than requiring an intutive grasp of the distribution metrics described by each bit of a boxplot\n\n2\n\nThis is allowing us to ‘tilt’ the X axis tick labels so they don’t overlap with one another\n\n\n\n\n\n\n\n\n\n\n\nThis graph clearly supports our intuition that among-plot variation is dramatic! We could account for this by including plot as a fixed effect but we’ll need to sacrifice a lot of degrees of freedom (can be thought of as “statistical power”) for a variable that we don’t actually care about. Instead, we could include plot as another random effect.\n\n# Fit the new model\ntarantula_mem2 &lt;- lmerTest::lmer(tarantula_count ~ road.dist_km + \n1                               (1|year) + (1|site.plot),\n                             data = tarantula_df)\n\n# Extract summary\nsummary(tarantula_mem2)\n\n\n1\n\nNote that we need to use this column as the random effect because plots are not uniquely named across sites (i.e., all sites have plots “a”, “b”, and “c”). Making the random effect just the ‘plot’ column would fail to reflect how plots are nested within each site\n\n\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: tarantula_count ~ road.dist_km + (1 | year) + (1 | site.plot)\n   Data: tarantula_df\n\nREML criterion at convergence: 4553.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.0902 -0.5352 -0.0117  0.5990  3.9323 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n site.plot (Intercept) 8165     90.36   \n year      (Intercept) 1991     44.62   \n Residual              1058     32.52   \nNumber of obs: 450, groups:  site.plot, 30; year, 3\n\nFixed effects:\n             Estimate Std. Error     df t value Pr(&gt;|t|)   \n(Intercept)    -3.393     58.233 23.593  -0.058  0.95402   \nroad.dist_km    2.814      0.973 28.000   2.892  0.00733 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nroad.dst_km -0.851\n\n\nThis test reveals that while there is a significant relationship between road distance and tarantula population but the effect is not nearly as strong as it was when we let plot-level variation be ignored. This is likely due to high (or low) average populations in a single plot skewing the site-level average. Still, this is a result we can be more confident in because we’ve now accounted for all known sources of variation in our data–either by including them as fixed effects or including them as a random effects.\nWe can create one more graph of our tidy data and use some aesthetic settings to make sure the nested structure of the data is clear to those looking at our work. Note that you could also use predicted values from the model itself though that choice is–arguably–a matter of personal preference.\n\nggplot(tarantula_df, aes(y = tarantula_count, x = road.dist_km)) +\n  geom_point(aes(color = plot, shape = as.factor(year)), size = 2, alpha = 0.5) +\n  geom_smooth(method = \"lm\", formula = \"y ~ x\", se = T, color = \"black\") +\n  labs(y = \"Tarantula Abundance\", x = \"Distance to Nearest Road (km)\") +\n  theme_bw()",
    "crumbs": [
      "Phase III -- Execute",
      "Analysis & Modeling"
    ]
  },
  {
    "objectID": "mod_stats.html#multi-model-inference-1",
    "href": "mod_stats.html#multi-model-inference-1",
    "title": "Analysis & Modeling",
    "section": "Multi-Model Inference",
    "text": "Multi-Model Inference\nRegardless of your choice of statistical test, multi-model inference may be an appropriate method to use to assess your hypothesis. As stated earlier, this frames your research question as a case of which variables best explain the data rather than the likelihood of the observed effect relating to any variable in particular.\nTo begin, it can be helpful to write out all possible “candidate models”. For instance, let’s say that you measured some response variable (Y) and several potential explanatory variables (X, W, and Z). We would then fit the following candidate models:\n\nX alone explains the most variation in Y\nW alone explains the most variation in Y\nZ alone explains the most variation in Y\nX, W, and Z together explain the most variation in Y\n\nWe might also fit other candidate models for pairs of X, W, and Z but for the sake of simplicity in this hypothetical we’ll skip those. Note that for this method to be appropriate you need to fit the same type of model in all cases!\nOnce we’ve fit all of our models and assigned them to objects, we can use the AIC function included in base R to compare the AIC score of each model. “AIC” stands for Akaike (AH-kuh-ee-kay) Information Criterion and is one of several related information criteria for summarizing a model’s explanatory power. Models with more parameters are penalized to make it mathematically possible for a model with fewer explanatory variables to still do a better job capturing the variation in the data.\nThe model with the lowest AIC best explains the data. Technically any difference in AIC indicates model improvement but many scientists use a rule of thumb of a difference of 2. So, if two models have AIC scores that differ by less than 2, you can safely say that they have comparable explanatory power. That is definitely a semi-arbitrary threshold but so is the 0.05 threshold for p-value “significance”.\n\nAIC Case Study\nLet’s check out an example using AIC to compare the strengths of several models. Rather than using simulated data–as we did earlier in the mixed-effect model section–we’ll use some real penguin data included in the palmerpenguins package.\nThis dataset includes annual data on three penguin species spread across several islands. The sex of the penguins was also recorded in addition to the length of their flippers, body mass, and bill length and depth.\nFor the purposes of this example, our research question is as follows: what factors best explain penguin body mass?\n\n# Load the penguins data from the `palmerpenguins` package\ndata(penguins)\n\n# Make a version where no NAs are allowed\n1peng_complete &lt;- penguins[complete.cases(penguins), ]\n\n# Check the structure of it\ndplyr::glimpse(peng_complete)\n\n\n1\n\nThis is a base R way of keeping only rows that have no NA values in any column. It is better to identify and handle NAs more carefully but for this context we just want to have the same number of observations in each model\n\n\n\n\nRows: 333\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 41.1, 38.6…\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 17.6, 21.2…\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, 193, 190, 181, 195, 182, 191, 198, 18…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3200, 3800…\n$ sex               &lt;fct&gt; male, female, female, female, male, female, male, fe…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nWith our data in hand and research question in mind, we can fit several candidate models that our scientific intuition and the published literature support as probable then compare them with AIC.\n\n# Species and sex\nmod_spp &lt;- lm(body_mass_g ~ species + sex, data = peng_complete)\n\n# Island alone\nmod_isl &lt;- lm(body_mass_g ~ island, data = peng_complete)\n\n# Combination of species and island\nmod_eco &lt;- lm(body_mass_g ~ island + species + sex, data = peng_complete)\n\n# Body characteristics alone\nmod_phys &lt;- lm(body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm,\n               data = peng_complete)\n\n# Global model\n1mod_sink &lt;- lm(body_mass_g ~ island + species + sex +\n               flipper_length_mm + bill_length_mm + bill_depth_mm,\n               data = peng_complete)\n\n\n1\n\nWe’ve named the global model “sink” because of the American idiom “everything but the kitchen sink.” It is used in cases where everything that can be included has been\n\n\n\n\nOnce we’ve fit all of these models, we can use the AIC function from base R (technically from the stats package included in base R).\n\n# Compare models\nAIC(mod_spp, mod_isl, mod_eco, mod_phys, mod_sink) %&gt;% \n1  dplyr::arrange(AIC)\n\n\n1\n\nUnfortunately, the AIC function doesn’t sort by AIC score automatically so we’re using the arrange function to make it easier for us to rank models by their AIC scores\n\n\n\n\n         df      AIC\nmod_sink 10 4727.242\nmod_spp   5 4785.594\nmod_eco   7 4789.480\nmod_phys  5 4929.554\nmod_isl   4 5244.224\n\n\nInterestingly, it looks like the best model (i.e., the one that explains most of the data) is the global model that included most of the available variables. As stated earlier, it is not always the case that the model with the most parameters has the lowest AIC so we can be confident this is a “real” result. The difference between that one and the next (incidentally the model where only species and sex are included as explanatory variables) is much larger than 2 so we can be confident that the global model is much better than the next best.\nWith this result your interpretation would be that penguin body mass is better explained by a combination of species, sex, physical characteristics of the individual penguin, and the penguin’s home island than it is by any of the other candidate models. In a publication you’d likely want to report this entire AIC table (either parenthetically or in a table) so that reviewers could evaluate your logic.",
    "crumbs": [
      "Phase III -- Execute",
      "Analysis & Modeling"
    ]
  },
  {
    "objectID": "mod_stats.html#meta-analysis",
    "href": "mod_stats.html#meta-analysis",
    "title": "Analysis & Modeling",
    "section": "Meta-Analysis",
    "text": "Meta-Analysis\nMany synthesis projects are able to find the original data of each study, harmonize that data, and then perform standard analyses on that synthesized data. However, in some cases you may find that the data used in different projects are not directly comparable. For instance, if you want to know what the effect of restoration methods are on forest recovery you might not be able to simply combine data from different studies that use widely different restoration methods, data collection methods, and have different forest community compositions. In such cases you can use meta-analysis to compare the results of different studies rather than using their data. Meta-analysis is named the way it is because it is an analysis of prior analyses.\nTo perform meta-analysis you’ll need to calculate an “effect size” for all studies you’d like to include. An effect size captures the direction and magnitude of the relationship analyzed in each original study. If you use a standard effect size calculation for each stud, you’ll make it possible to directly compare results across these studies (even if context differs among them!). Note that some people disagree with the word “effect” in “effect size” because it suggests a causal relationship; for our purposes, let’s consider ‘effect’ to be inclusive of correlative relationships and ignore the possible implication of causality.\nIn order to calculate these effect sizes you’ll need to extract the following information from each study:\n\nA measure of the ‘central tendency’ of the response\n\nOften the arithmetic mean but can also be a proportion or a correlation\nYou’ll need to do this separately for any groups within the study\n\nA measure of the variation in the response\n\nTypically standard deviation\n\nThe sample size of the response\n\nOnce you have that information, you can calculate effect sizes for the various groups in each study. Note that the importance of this information to meta-analyses should also highlight how vital it is that you report this information in your own research! Doing so will enable future meta-analyses to include your study and increase the scientific impact of your work as well as its professional benefits to you.\nOne such effect size is Cohen’s d and is a reasonable effect size for quantifying the difference in means between two groups. In order to perform this calculation you simply need the mean, standard deviation, and sample size for two groups. Let’s check out an example to demonstrate.\n\n# Load needed library\nlibrary(esc)\n\n# Calculate Cohen's d\ncalc_effect &lt;- esc::esc_mean_sd(grp1m = 50, grp2m = 60, \n                                grp1sd = 10, grp2sd = 10, \n                                grp1n = 50, grp2n = 50)\n\n# Check out output\ncalc_effect\n\n\nEffect Size Calculation for Meta Analysis\n\n     Conversion: mean and sd to effect size d\n    Effect Size:  -1.0000\n Standard Error:   0.2121\n       Variance:   0.0450\n       Lower CI:  -1.4158\n       Upper CI:  -0.5842\n         Weight:  22.2222\n\n\nNote that Cohen’s d is just one effect size available to you and others may be more appropriate in certain contexts. Just like any other metric, which effect size you choose is a mix of your scientific intution and appropriateness for the content of your data. For a deeper dive into the breadth of effect size considerations available to you, see the relevant chapter of the Doing Meta-Analysis in R online book.\nAfter you’ve calculated all relevant effect sizes–using your chosen flavor of effect size–the “actual” meta-analysis is nearly finished. Simply create a graph of the effect sizes with error bars indicating confidence intervals (included by default in most effect size functions). Where the error bars overlap among studies, there is no significant difference between those effect sizes. Conversely, where the error bars do not overlap among studies the effect sizes do significantly differ indicating that the studies results’ differ for the data used to calculate the effect size.",
    "crumbs": [
      "Phase III -- Execute",
      "Analysis & Modeling"
    ]
  },
  {
    "objectID": "mod_stats.html#additional-resources",
    "href": "mod_stats.html#additional-resources",
    "title": "Analysis & Modeling",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\nSpake, R. et al. Understanding ‘It Depends’ in Ecology: A Guide to Hypothesising, Visualising and Interpreting Statistical Interactions. 2023. Biological Reviews\nSpake, R. et al. Improving Quantitative Synthesis to Achieve Generality in Ecology. 2022. Nature Ecology and Evolution\nTredennick, A.T. et al. A Practical Guide to Selecting Models for Exploration, Inference, and Prediction in Ecology. 2021. Ecology\nHarrier, M. et al. Doing Meta-Analysis with R: A Hands-On Guide. 2021.\nZuur, A.F. et al. Mixed Effects Models and Extensions in Ecology with R. 2009.\n\n\n\nWorkshops & Courses\n\nVuorre, M. Bayesian Meta-Analysis with R, Stan, and brms. 2016.\n\n\n\nWebsites\n\nKurz, A. S. Bayesian Meta-Analysis in brms-II. 2022.\nViechtbauer, W. Meta-Analysis with R. 2022.\nViechtbauer, W. The metafor Package: A Meta-Analysis Package for R. 2021.\nKurz, A. S. Bayesian Meta-Analysis in brms. 2020.",
    "crumbs": [
      "Phase III -- Execute",
      "Analysis & Modeling"
    ]
  },
  {
    "objectID": "mod_thinking.html",
    "href": "mod_thinking.html",
    "title": "Supporting Divergent, Emergent, and Convergent Thinking",
    "section": "",
    "text": "As many course participants will have discovered by now, the process of deciding on, refining, and choosing a path forward for data analysis will involve many differing opinions and perspectives. To work effectively as a group, you will need to elicit a broad range of ideas and then combine and focus them. This can be unfamiliar territory for researchers who have mainly pursued individual projects. While the particulars of each project are different, the patterns are often similar and being familiar with a few approaches for eliciting the best from yourself and colleagues can speed the process and improve the experience for all involved.\nBelow you will find support for your group as you move through both exciting, and sometimes uncomfortable, stages of collaborative, participatory decision-making - from divergent to emergent and finally convergent thinking.",
    "crumbs": [
      "Phase II -- Plan",
      "Supporting D.E.C. Thinking"
    ]
  },
  {
    "objectID": "mod_thinking.html#overview",
    "href": "mod_thinking.html#overview",
    "title": "Supporting Divergent, Emergent, and Convergent Thinking",
    "section": "",
    "text": "As many course participants will have discovered by now, the process of deciding on, refining, and choosing a path forward for data analysis will involve many differing opinions and perspectives. To work effectively as a group, you will need to elicit a broad range of ideas and then combine and focus them. This can be unfamiliar territory for researchers who have mainly pursued individual projects. While the particulars of each project are different, the patterns are often similar and being familiar with a few approaches for eliciting the best from yourself and colleagues can speed the process and improve the experience for all involved.\nBelow you will find support for your group as you move through both exciting, and sometimes uncomfortable, stages of collaborative, participatory decision-making - from divergent to emergent and finally convergent thinking.",
    "crumbs": [
      "Phase II -- Plan",
      "Supporting D.E.C. Thinking"
    ]
  },
  {
    "objectID": "mod_thinking.html#learning-objectives",
    "href": "mod_thinking.html#learning-objectives",
    "title": "Supporting Divergent, Emergent, and Convergent Thinking",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nRecognize the stages of a participatory decision making process and the type of thinking required for each stage\nIdentify the value of and potential obstacles to emergent thinking in the “groan zone”\nUnderstand the diamond model of participatory decision making as a framework for group process design\nDevelop strategies to support participation, innovative thinking, integration, and convergence / decision making in a team setting",
    "crumbs": [
      "Phase II -- Plan",
      "Supporting D.E.C. Thinking"
    ]
  },
  {
    "objectID": "mod_thinking.html#preparation",
    "href": "mod_thinking.html#preparation",
    "title": "Supporting Divergent, Emergent, and Convergent Thinking",
    "section": "Preparation",
    "text": "Preparation\nNone required.",
    "crumbs": [
      "Phase II -- Plan",
      "Supporting D.E.C. Thinking"
    ]
  },
  {
    "objectID": "mod_thinking.html#networking-session",
    "href": "mod_thinking.html#networking-session",
    "title": "Supporting Divergent, Emergent, and Convergent Thinking",
    "section": "Networking Session",
    "text": "Networking Session\n\n2024 Guests\n\n\nBala Chaudhary is a soil ecologist specializing in plant-soil-microbial interactions and mycorrhizal symbioses. Research in her lab examines ecosystem-scale questions in mycorrhizal ecology along four main themes: microbial dispersal at the macrosystems scale, trait-based mycorrhizal ecology, synthesis in ecology, and microbial mediation of global change solutions. She uses trait-based approaches to develop predictive frameworks for mycorrhizal dispersal, community assembly, and biogeography, and employs complimentary approaches of macroecological field work, controlled lab experiments, and data synthesis to study multi-scale questions in ecology. Her work spans dozens of ecosystem types and has applications in global change solutions including climate mitigation and adaptation, soil conservation, ecosystem restoration, and sustainable agriculture. She also conducts research on ways to broaden scientific participation and promote racial and ethnic diversity, equity, and inclusion in STEM.",
    "crumbs": [
      "Phase II -- Plan",
      "Supporting D.E.C. Thinking"
    ]
  },
  {
    "objectID": "mod_thinking.html#collaborative-convergence",
    "href": "mod_thinking.html#collaborative-convergence",
    "title": "Supporting Divergent, Emergent, and Convergent Thinking",
    "section": "Collaborative Convergence",
    "text": "Collaborative Convergence\nAt the beginning of a collaborative process, the most important initial outcome is getting convergence or group alignment on a set of shared goals and objectives and a plan for how to achieve them. If your team process is effective, this plan will be an inclusive solution – one that works for everyone in the group. Achieving this shared vision can be more difficult than one might expect. While you may expect that participants have already agreed to the vision in joining the group, agreement does not always equate to alignment. This module focuses on tools and resources to help your group navigate to convergent, inclusive solutions that everyone on the team can align around.",
    "crumbs": [
      "Phase II -- Plan",
      "Supporting D.E.C. Thinking"
    ]
  },
  {
    "objectID": "mod_thinking.html#embracing-divergent-thinking",
    "href": "mod_thinking.html#embracing-divergent-thinking",
    "title": "Supporting Divergent, Emergent, and Convergent Thinking",
    "section": "Embracing Divergent Thinking",
    "text": "Embracing Divergent Thinking\nThe first stage of group decision making is divergent thinking (Kaner et al. 2014). Confronted with a new, complex topic, the group will gradually move from the safe territory of familiar opinions into sharing their diverse perspectives and exploring new ideas. This can feel like the group process is devolving away from what was assumed to be shared agreement, but it is actually a critical part of the collaborative process.\n\n\n\nFigure 1. Drawing by Carrie Kappel, adapted from Sam Kaner’s Facilitator’s Guide to Participatory Decision making\n\n\nWhen a diverse group comes together to work on a complex problem, their views are likely to diverge widely across many dimensions from problem definition to priorities to methods/approaches to the definition of success. But you can tap that divergent thinking to generate entirely new ideas and options that emerge through the group’s productive struggle for mutual understanding.\nWhile your working group is in the divergent thinking stage, it’s critical to foster dialogue to surface different perspectives. Examine hidden assumptions. Create room for disagreement and questioning. Amplify diverse perspectives - and particularly, voices from the edge (e.g., junior members, new collaborators, people from different disciplines, non-scientists who may be affected by the research) - in order to expand the range of possibilities. Mirror and validate what you hear. Invite people who are good at bridging across disciplinary or other differences to help translate and build shared understanding of methods and ways of thinking. Suspend judgment and encourage full participation.\nBeware of the most common pitfall at this stage, which is to converge too quickly on an early conclusion, staying in the safe space of familiar opinions and status quo solutions. You can prepare for this stage and help to avoid that pitfall by reviewing prior work and synthesizing data and knowledge gaps, promising approaches, and critical questions. Your team can use that synthesis of the current state of the science as a jumping off point.",
    "crumbs": [
      "Phase II -- Plan",
      "Supporting D.E.C. Thinking"
    ]
  },
  {
    "objectID": "mod_thinking.html#making-it-through-the-groan-zone",
    "href": "mod_thinking.html#making-it-through-the-groan-zone",
    "title": "Supporting Divergent, Emergent, and Convergent Thinking",
    "section": "Making it Through the Groan Zone",
    "text": "Making it Through the Groan Zone\nIt’s natural for groups to go through a period of confusion and frustration as they struggle to integrate their diverse perspectives into a shared framework of understanding (Kaner et al. 2014). The goal is to get the group across this no man’s land between divergent thinking and convergence known as the “groan zone.” In the groan zone, the group leader or facilitator’s job is to keep the group from getting frustrated and shutting down.\nWhile the groan zone can be challenging, it can also be an extremely fruitful and creative stage. Here in the messy middle of a group process, an open and flexible mindset and a process that invites participants to engage in emergent thinking can enable true innovation. Emergent thinking builds upon ideas generated in the divergent thinking stage, recombining or adapting them in novel ways. It seeks to identify patterns and make meaning in the face of complexity and uncertainty. Done well, emergent thinking enables a group to adapt, sense opportunities, and generate new and exciting ideas.\n\nActivity: Rapid BrainstormCommon barriers to emergent thinking\n\n\nA variety of factors and dynamics can impede emergent thinking and make the groan zone especially challenging. What have you observed?\n\n\n\nDisciplinary differences in epistemology, vocabulary, and methods that impede understanding\nAnalysis paralysis - getting lost in the weeds of endless analysis and detail\nPolarization - opposite camps anchored in\nPower dynamics that squelch creative contributions from the “edges”\nAvoidance of a deeper issue impeding collaboration (e.g. lack of trust)\nTurf wars, competition\nRisk aversion, perception management, fear of failure / getting it wrong\nConfirmation bias and resistance to ideas that challenge group identity and beliefs\n\n\n\n\nSome useful techniques for navigating the groan zone and fostering emergent thinking include:\n\nCultivating presence and patience\nActive listening\nBuilding shared understanding via translation (e.g. across disciplines), metacognition (thinking about how you are thinking), and inquiry\nExploring new data, models, and ways of presenting information\nCreating categories to reveal structure and allow sorting and prioritization of ideas\nCombining or recombining ideas or methods to yield new approaches\nWorking together to separate facts from opinions\nCarefully examining language, e.g. by looking word by word at a key statement or question that is being debated and asking what questions each word raises\nCapturing side issues in writing and reserving time to revisit these – taking the tangents seriously is a critical part of letting participants know you value their contributions\nExamining how proposed ideas might affect each individual in the group\nHonoring objections to the process and asking for suggestions\nAddressing power imbalances and elevating voices from the “edge”\n\nIf you find the conversation getting off track or the dynamics becoming difficult, useful techniques that allow you to remain committed to being supportive and respectful of all group members (including ones you might experience as “difficult”) include:\n\nReminding individuals of the larger purpose of the group and reconnecting them to their own personal reasons for caring about and working on the issue, e.g. by inviting them to take a moment to reflect or to restate what success looks like\nFocusing on common ground and areas of potential alignment\nInviting constructive opposition - ask the critic to say what they can support about a given proposal and what they would like to see changed or discussed further\nSwitching the participation format (e.g., going to breakout groups, brainstorming, a go-around, or individual writing)\nTaking a break\nStepping out of the content and addressing the process\nEducating members about group dynamics and asking them to reflect on how they are showing up\nEncouraging more people to participate\nReframing the discussion, e.g. by surfacing underlying issues, and/or focusing on concrete actions that the group can take to resolve the conflict\n\nDon’t get discouraged by the groan zone. Misunderstanding and miscommunication are normal parts of the process of collaboration. And even more importantly, “the act of working through these misunderstandings is what builds the foundation for sustainable agreements [and]… meaningful collaboration” (Kaner et al. 2014).",
    "crumbs": [
      "Phase II -- Plan",
      "Supporting D.E.C. Thinking"
    ]
  },
  {
    "objectID": "mod_thinking.html#getting-to-convergence",
    "href": "mod_thinking.html#getting-to-convergence",
    "title": "Supporting Divergent, Emergent, and Convergent Thinking",
    "section": "Getting to Convergence",
    "text": "Getting to Convergence\nOnce the group has a strong foundation of shared understanding, things often start to click into place and feel easier and faster as you enter the zone of convergent thinking. At this point, the group is ready to devise inclusive solutions, weigh alternatives and make decisions. As the group leader your role is to help the group devise specific proposals, evaluate and decide among them, refine and synthesize into an overall approach, and lay out a concrete plan. The risk at this stage is that the group never converges on a clear decision or plan, leading the group to spin its wheels in the future.\nDiscuss as a team how you want to make decisions - what is your decision making process going to be? Will it be based on consensus? Majority rules? Or will you delegate the decisionmaking to the team leader or a sub-team who are closest to the decision? What’s your fallback plan if you can’t reach a decision during your time together? The figure below arrays a variety of different decisionmaking processes onto the axes of level of group member involvement and level of group member ownership. Note that the two red circles that are off the graph in the bottom left should definitely be avoided!\n\n\n\nFigure 2. Decision making approaches and fallback options\n\n\nTechniques that are useful in this phase include:\n\nPulling up concrete examples for inspiration\nInviting concrete, written proposals\nClarifying selection criteria and evaluating proposals against them\nCombining the best elements of multiple ideas to support more innovative, inclusive solutions\nDeciding what ideas to pursue and which to keep on the back burner in case the team needs to adapt\nDefining steps and milestones, planning the work flow, and assigning roles and responsibilities\n\nWhile the big work of the initial stage of a synthesis science project is getting to convergence on the overall work plan, you should expect that the group may go through the process of divergence and convergence again at multiple points in the process as you dive into the work and uncover new challenges. But the shared understanding and social rapport that come from successfully struggling together early on will allow the group to more easily and rapidly develop and implement new solutions in subsequent meetings.",
    "crumbs": [
      "Phase II -- Plan",
      "Supporting D.E.C. Thinking"
    ]
  },
  {
    "objectID": "mod_thinking.html#a-framework-for-group-process-design",
    "href": "mod_thinking.html#a-framework-for-group-process-design",
    "title": "Supporting Divergent, Emergent, and Convergent Thinking",
    "section": "A Framework for Group Process Design",
    "text": "A Framework for Group Process Design\nThe diamond model is not only a conceptual model of the common flow of participatory decisionmaking dynamics – it is also a tool you can use to design good group processes.\nHave you ever been in a meeting that generated tons of ideas, but left you wondering whether any of it was going to go anywhere? That group process neglected to engage in convergent thinking. Or have you experienced a leader who hands down dictates with little discussion and expects everyone to just agree? They are missing out on the value of divergent and emergent thinking to generate creativity and buy-in.\nA good group process will move through the diamond model and include all three types of thinking. Your process design, whether it’s for a single meeting, a multi-day workshop, or a longer collaborative process, should start by asking yourself, where are we now? What resources, people, and information do we have as our starting point? And where do we want to get to? What is our target end state – the deliverable or outcome we are trying to achieve?\nBetween those two points - your initial state and your target state - you can use one or multiple diamond shaped activities to productively move your group along. Each microstructure or game you use to engage the group can help to generate new ideas, explore them together, and converge on new insights or decisions. Those insights or decisions can then become the input for the next diamond, until you eventually reach your goal.\nThose diamonds can vary in scale from mini-exercises that only take a few minutes to multi-day or longer processes. Once you start to recognize it, you’ll see the diamond model everywhere (or miss it when it’s lacking!).\n\n\n\n\n\n\nSmall Group Exercise #1 - Where are we in the diamond?\n\n\n\nPurpose: Identify where your group is in the diamond model right now and what stage you need to move through next.\nPart 1 (~2 mins)\nIndividual reflection\n\nWhere would you say your group is right now in the diamond model? Have you been leveraging the relevant thinking for the stage you are in (divergent, emergent, or convergent)?\nThink about the next group work or decisionmaking task that lies ahead - what stage(s) of the diamond will that work take you through and what kind of thinking will be needed?\n\nPart 2 (15 mins)\nProject team discussion\n\nIdentify a reporter, notetaker, and facilitator/timekeeper\nGo around and share your answers to the prompt (1 min each)\nDiscuss and try to reach consensus about which stage(s) of the diamond model best match the next chunk of work you have ahead of you.\n\nPart 3 (~5 mins)\nShare with the whole group",
    "crumbs": [
      "Phase II -- Plan",
      "Supporting D.E.C. Thinking"
    ]
  },
  {
    "objectID": "mod_thinking.html#tools-to-support-the-thinking-you-need",
    "href": "mod_thinking.html#tools-to-support-the-thinking-you-need",
    "title": "Supporting Divergent, Emergent, and Convergent Thinking",
    "section": "Tools to Support the Thinking You Need",
    "text": "Tools to Support the Thinking You Need\n(D=divergent, E=emergent, C=convergent)\n\n\n\nMicrostructure\nStage\nPurpose\nHow It Works\nTips & Traps\n\n\n\n\nBrainwriting or brainstorming\nD\nSurface and elaborate ideas\nBrainstorm ideas in a google doc or virtual whiteboard (or on index cards or sticky notes in person); read and add to each other’s ideas; discuss\nFollow up with affinity mapping as you move into emergent thinking\n\n\nRotating Stations\nD\nSpread good ideas and make informal connections\nSet up stations with experts or innovators who can present information and engage discussion; group members circulate to learn, ask questions, or provide feedback\nKeep the overall session short so that it’s not too fatiguing for presenters who will have to repeat their spiel many times\n\n\nUser Experience Fishbowl (a riff on panel discussion)\nDE\nDraw out and contrast different perspectives from experts or interested parties\nExperts with direct experience of the challenge at hand are invited to engage each other in conversation at the center of a circle; the rest of the group are audience to their conversation; audience and facilitator can suggest things for them to discuss\nAvoid falling back on traditional formats where panelists talk to the audience rather than each other - the power of the fishbowl is the deep way in which the experts can engage each other\n\n\nHorizon scanning / futures thinking\nDE\nDetect emerging trends, issues, or research opportunities\nDefine your scope; consult outside sources of data and expertise or brainstorm within the group to identify emerging trends, events, and weak signals of future change that may affect the question / topic you have defined\nSet a timeframe that’s far enough out to encompass important uncertainties but not so far that forecasting becomes overly speculative\n\n\nAffinity Map (clustering)\nE\nSurface ideas, detect patterns, and analyze\nBrainstorm ideas using sticky notes on a wall or virtual whiteboard; cluster into categories\nFollow up with prioritization of ideas within clusters as you move into convergent thinking\n\n\nConceptual models\nE\nBuild a shared representation of the system\nCo-develop a figure or diagram that encapsulates your collective understanding of the focal problem or system\nConsider mind maps, flow charts, system diagrams; Consider having several small groups attempt this in parallel and compare results\n\n\nWorld Cafe conversations\nE\nEngage everyone in making sense of profound challenges\nAsk for a volunteer to host each table; use a talking object; Go-around 1: share what you are thinking, feeling, or doing about the theme or topic; Go-around 2: share your thoughts and feelings after having listened to others; Open conversation; Go-around 4: “takeaways”\nStart with a clear question or prompt for discussion; Share the agreements and ask hosts to gently facilitate adherence\n\n\nWhat, So What, Now What\nEC\nMake sense of past progress or experiences and decide on future actions\nWhat - As a group, compile the facts and observations relevant to the context; So What - Reflect on the facts and their implications, identify patterns, generate hypotheses; Now What - Draw conclusions - What actions make sense?\nBe firm in calling out opinions being passed off as facts in the What stage. Stick to what is observable.\n\n\nPolling\nEC\nRank alternatives\nDecide how many votes per person; In person - use sticky dots; Virtually - use +1s in a google doc or a digital polling tool (e.g., Zoom, Mural, slido)\nBefore you start - clarify how you will use the results - are you gathering information or taking a vote to make a decision?\n\n\nFeasibility x impact matrix\nC\nCompare alternatives\nDiscuss and agree on definitions for two criteria for evaluating ideas: feasibility of implementation and impact potential; Rate each idea against these two axes and map onto 2x2 grid\nHow you define the axes must be clear and agreed upon by everyone before you start\n\n\nFist to Five / Gradient of Agreement\nC\nAssess degree of consensus; seek closure\nUse when ready to close a discussion or make a decision; Invite participants to rate their level of agreement with a proposal on a scale of 0-5; Five fingers means “absolute, total agreement or support” and a fist means “complete opposition”\nIf you have some 1s and 2s, more discussion is needed - ask them to explain their concerns or questions\n\n\nOpen discussion\nDEC\nGroup inquiry, sensemaking, and/or decisionmaking\nClearly define the scope; set agreements for inclusive discussion; invite discussion about the topic at hand; capture ideas and questions; listen for when the group is ready to converge\nKeep track of side topics (e.g. in a “bike rack”) and make time to come back to them, but don’t let them derail\n\n\nBreakout groups\nDEC\nEngage everyone deeply; avoid groupthink\nDefine the scope and intended outcomes; set a time limit; assign roles (facilitator, notetaker, reporter); model what you want in the report out\nConsider whether all groups should work with the same prompts or different aspects of the problem\n\n\nChart writing or whiteboarding\nDEC\nReflect participant’s viewpoints back to the group\nOn a virtual or physical chart or whiteboard, capture key points in the discussion so everyone can see them\nUse speakers’ own words; if the comment is long or complex, ask the speaker to give you a headline you can capture\n\n\n1,2,4,all\nDEC\nEngage everyone in generating questions, ideas, and suggestions\nIndividual reflection; Pair share; Two pairs combine and share as a group of 4; Small groups share highlights with whole group\nEmphasize novel ideas and distinctions for divergence; common themes and emerging insights for emergence and convergence\n\n\nRound robin / go around\nDEC\nHear from everyone; get starting positions on the table\nEveryone answers the same prompt. Alternatives to going in order: each speaker calls on someone else after they have shared; popcorn-style - people share in the order that they feel moved to speak\nDo not allow discussion until everyone has responded to the initial prompt\n\n\n\n\n\n\n\n\n\nActivity: Small Group Exercise #2 - Emergent thinking practice\n\n\n\nPart 1 (25 mins)\nTeam discussion and co-production\n\nAssign a facilitator, notetaker and reporter\nRead through the list of options and begin with the step that seems most useful for your team’s current needs. Work your way through as many of the steps as you can during the allotted time.\nIf your group doesn’t yet have a conceptual figure that captures how you are thinking about the system you are investigating, design one.\nIf you already have one, revisit it and refine it as needed.\nOnce you are satisfied with your conceptual figure, outline your key questions and map your planned analyses to them. How are you going to answer each question with data? If you were working with the tools we suggested last time to start outlining your analysis, this should build from there.\nWhat are the ≤3 key figures that would best illustrate your results?\n\nPart 2 (2-3 mins / team)\nReport Out\nShare what you developed with the whole group After all groups have shared, offer suggestions to other groups and/or ask questions about the module.",
    "crumbs": [
      "Phase II -- Plan",
      "Supporting D.E.C. Thinking"
    ]
  },
  {
    "objectID": "mod_thinking.html#additional-resources",
    "href": "mod_thinking.html#additional-resources",
    "title": "Supporting Divergent, Emergent, and Convergent Thinking",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\nCravens, A.E., et al., Science facilitation: navigating the intersection of intellectual and interpersonal expertise in scientific collaboration. 2022.\nLipmanowicz & McCandless. The Surprising Power of Liberating Structures: Simple Rules to Unleash a Culture of Innovation. Liberating Structures Press. 2014.\nKaner, S. Facilitator’s Guide to Participatory Decision-Making (Revised). 2014.\nGray, D. et al., Gamestorming: A Playbook for Innovators, Rulebreakers, and Changemakers. O’Reilly Media. 2010.\nBohm, D. On Dialogue. Routledge Classics. 2004.\n\n\n\nWorkshops & Courses\n\nIAF Endorsed™ Facilitation Training Programmes\nFacilitation Training Programs recommended by the Facilitator School\n\n\n\nWebsites\n\nLiberating Structures: Including and Unleashing Everyone\nGamestorming\nKappel, Carrie. Collaboration: From groan zone to growth zone. Integration and Implementation Insights.. 2019.",
    "crumbs": [
      "Phase II -- Plan",
      "Supporting D.E.C. Thinking"
    ]
  },
  {
    "objectID": "mod_wrangle.html",
    "href": "mod_wrangle.html",
    "title": "Data Harmonization & Wrangling",
    "section": "",
    "text": "Now that we have covered how to find data and use data visualization methods to explore it, we can move on to combining separate data files and preparing that combined data file for analysis. For the purposes of this module we’re adopting a very narrow view of harmonization and a very broad view of wrangling but this distinction aligns well with two discrete philosophical/practical arenas. To make those definitions explicit:\n\n“Harmonization” = process of combining separate primary data objects into one object. This includes things like synonymizing columns, or changing data format to support combination. This excludes quality control steps–even those that are undertaken before harmonization begins.\n“Wrangling” = all modifications to data meant to create an analysis-ready ‘tidy’ data object. This includes quality control, unit conversions, and data ‘shape’ changes to name a few. Note that attaching ancillary data to your primary data object (e.g., attaching temperature data to a dataset on plant species composition) also falls into this category!",
    "crumbs": [
      "Phase III -- Execute",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "mod_wrangle.html#overview",
    "href": "mod_wrangle.html#overview",
    "title": "Data Harmonization & Wrangling",
    "section": "",
    "text": "Now that we have covered how to find data and use data visualization methods to explore it, we can move on to combining separate data files and preparing that combined data file for analysis. For the purposes of this module we’re adopting a very narrow view of harmonization and a very broad view of wrangling but this distinction aligns well with two discrete philosophical/practical arenas. To make those definitions explicit:\n\n“Harmonization” = process of combining separate primary data objects into one object. This includes things like synonymizing columns, or changing data format to support combination. This excludes quality control steps–even those that are undertaken before harmonization begins.\n“Wrangling” = all modifications to data meant to create an analysis-ready ‘tidy’ data object. This includes quality control, unit conversions, and data ‘shape’ changes to name a few. Note that attaching ancillary data to your primary data object (e.g., attaching temperature data to a dataset on plant species composition) also falls into this category!",
    "crumbs": [
      "Phase III -- Execute",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "mod_wrangle.html#learning-objectives",
    "href": "mod_wrangle.html#learning-objectives",
    "title": "Data Harmonization & Wrangling",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nIdentify typical steps in data harmonization and wrangling workflows\nCreate a harmonization workflow\nDefine quality control\nSummarize typical operations in a quality control workflow\nUse regular expressions to perform flexible text operations\nWrite custom functions to reduce code duplication\nIdentify value of and typical obstacles to data ‘joining’\nExplain benefits and drawbacks of using data shape to streamline code\nDesign a complete data wrangling workflow",
    "crumbs": [
      "Phase III -- Execute",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "mod_wrangle.html#preparation",
    "href": "mod_wrangle.html#preparation",
    "title": "Data Harmonization & Wrangling",
    "section": "Preparation",
    "text": "Preparation\n\nIn project teams, draft your strategy for wrangling data\n\nWhat needs to happen to the datasets in order for them to be usable in answering your question(s)?\nI.e., what quality control, structural changes, or formatting edits must be made?\n\nIf you are an R user, run the following code:\n\n\ninstall.packages(\"librarian\")\nlibrarian::shelf(ltertools, lterdatasampler, psych, supportR, tidyverse)",
    "crumbs": [
      "Phase III -- Execute",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "mod_wrangle.html#networking-session",
    "href": "mod_wrangle.html#networking-session",
    "title": "Data Harmonization & Wrangling",
    "section": "Networking Session",
    "text": "Networking Session\n\n2025 Guests\n\n\n\nStevan Earl, Information Manager, Central Arizona-Phoenix Long Term Ecological Research and Soil Organic Matter Synthesis Working Group\nCove Sturtevant, Research Scientist, National Ecological Observatory Network and the Flux Gradient Project Synthesis Working Group",
    "crumbs": [
      "Phase III -- Execute",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "mod_wrangle.html#making-a-wrangling-plan",
    "href": "mod_wrangle.html#making-a-wrangling-plan",
    "title": "Data Harmonization & Wrangling",
    "section": "Making a Wrangling Plan",
    "text": "Making a Wrangling Plan\nBefore you start writing your data harmonization and wrangling code, it is a good idea to develop a plan for what data manipulation needs to be done. Just like with visualization, it can be helpful to literally sketch out this plan so that you think through the major points in your data pipeline before beginning to write code that turns out to not be directly related to your core priorities. Consider the discussion below for some leading questions that may help you articulate your group’s plan for your data.\n\n\n\n\n\n\nDiscussion: Wrangling Plan\n\n\n\nWith your project groups discuss the following questions:\n\nWhat harmonization needs to be done to make your starting data files comparable?\n\nConsider units, taxonomic resolution, spatial/temporal granularity, etc.\n\nOnce you have harmonized your data, what wrangling needs to be done?\n\nDo you need to calculate index metrics? Change the ‘shape’ of the data? Aggregate to a particular spatial/temporal resolution or sampling intensity?\n\nHow can you make your workflow less repetitive / more efficient?\n\nReducing redundancy makes integrating new ideas dramatially easier\n\nHow easy will it be to maintain or revisit this code workflow?\nHow can you change the workflow now to make life easier for ‘future you’?",
    "crumbs": [
      "Phase III -- Execute",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "mod_wrangle.html#harmonizing-data",
    "href": "mod_wrangle.html#harmonizing-data",
    "title": "Data Harmonization & Wrangling",
    "section": "Harmonizing Data",
    "text": "Harmonizing Data\nData harmonization is an interesting topic in that it is vital for synthesis projects but only very rarely relevant for primary research. Synthesis projects must reckon with the data choices made by each team of original data collectors. These collectors may or may not have recorded their judgement calls (or indeed, any metadata) but before synthesis work can be meaningfully done these independent datasets must be made comparable to one another and combined.\nFor tabular data, we recommend using the ltertools R package to perform any needed harmonization. This package relies on a “column key” to translate the original column names into equivalents that apply across all datasets. Users can generate this column key however they would like but Google Sheets is a strong option as it allows multiple synthesis team members to simultaneously work on filling in the needed bits of the key. If you already have a set of files locally, ltertools does offer a begin_key function that creates the first two required columns in the column key.\nThe column key requires three columns:\n\n“source” – Name of the raw file\n“raw_name” – Name of all raw columns in that file to be synonymized\n“tidy_name” – New name for each raw column that should be carried to the harmonized data\n\nNote that any raw names either not included in the column key or that lack a tidy name equivalent will be excluded from the final data object. For more information, consult the ltertools package vignette. For convenience, we’re attaching the visual diagram of this method of harmonization from the package vignette.",
    "crumbs": [
      "Phase III -- Execute",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "mod_wrangle.html#wrangling-data",
    "href": "mod_wrangle.html#wrangling-data",
    "title": "Data Harmonization & Wrangling",
    "section": "Wrangling Data",
    "text": "Wrangling Data\nData wrangling is a huge subject that covers a wide range of topics. In this part of the module, we’ll attempt to touch on a wide range of tools that may prove valuable to your data wrangling efforts. This is certainly non-exhaustive and you’ll likely find new tools that fit your coding style and professional intuition better. However, hopefully the topics covered below provide a nice ‘jumping off’ point to reproducibly prepare your data for analysis and visualization work later in the lifecycle of the project.\nTo begin, we’ll load the Plum Island Ecosystems fiddler crab dataset we’ve used in other modules.\n\n# Load the lterdatasampler package\nlibrary(lterdatasampler)\n\n# Load the fiddler crab dataset\ndata(pie_crab)\n\n\nExploring the Data\nBefore beginning any code operations, it’s important to get a sense for the data. Characteristics like the dimensions of the dataset, the column names, and the type of information stored in each column are all crucial pre-requisites to knowing what tools can or should be used on the data.\nChecking the data structure is one way of getting a lot of this high-level information.\n\n# Check dataset structure\nstr(pie_crab)\n\nClasses 'tbl_df', 'tbl' and 'data.frame':   392 obs. of  9 variables:\n $ date         : Date, format: \"2016-07-24\" \"2016-07-24\" ...\n $ latitude     : num  30 30 30 30 30 30 30 30 30 30 ...\n $ site         : chr  \"GTM\" \"GTM\" \"GTM\" \"GTM\" ...\n $ size         : num  12.4 14.2 14.5 12.9 12.4 ...\n $ air_temp     : num  21.8 21.8 21.8 21.8 21.8 ...\n $ air_temp_sd  : num  6.39 6.39 6.39 6.39 6.39 ...\n $ water_temp   : num  24.5 24.5 24.5 24.5 24.5 ...\n $ water_temp_sd: num  6.12 6.12 6.12 6.12 6.12 ...\n $ name         : chr  \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" ...\n\n\nFor data that are primarily numeric, you may find data summary functions to be valuable. Note that most functions of this type do not provide useful information on text columns so you’ll need to find that information elsewhere.\n\n# Get a simple summary of the data\nsummary(pie_crab)\n\n      date               latitude         site                size      \n Min.   :2016-07-24   Min.   :30.00   Length:392         Min.   : 6.64  \n 1st Qu.:2016-07-28   1st Qu.:34.00   Class :character   1st Qu.:12.02  \n Median :2016-08-01   Median :39.10   Mode  :character   Median :14.44  \n Mean   :2016-08-02   Mean   :37.69                      Mean   :14.66  \n 3rd Qu.:2016-08-09   3rd Qu.:41.60                      3rd Qu.:17.34  \n Max.   :2016-08-13   Max.   :42.70                      Max.   :23.43  \n    air_temp      air_temp_sd      water_temp    water_temp_sd  \n Min.   :10.29   Min.   :6.391   Min.   :13.98   Min.   :4.838  \n 1st Qu.:12.05   1st Qu.:8.110   1st Qu.:14.33   1st Qu.:6.567  \n Median :13.93   Median :8.410   Median :17.50   Median :6.998  \n Mean   :15.20   Mean   :8.654   Mean   :17.65   Mean   :7.252  \n 3rd Qu.:18.63   3rd Qu.:9.483   3rd Qu.:20.54   3rd Qu.:7.865  \n Max.   :21.79   Max.   :9.965   Max.   :24.50   Max.   :9.121  \n     name          \n Length:392        \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n\nFor text columns it can sometimes be useful to simply look at the unique entries in a given column and sort them alphabetically for ease of parsing.\n\n# Look at the sites included in the data\nsort(unique(pie_crab$site))\n\n [1] \"BC\"  \"CC\"  \"CT\"  \"DB\"  \"GTM\" \"JC\"  \"NB\"  \"NIB\" \"PIE\" \"RC\"  \"SI\"  \"VCR\"\n[13] \"ZI\" \n\n\nFor those of you who think more visually, a histogram can be a nice way of examining numeric data. There are simple histogram functions in the ‘base’ packages of most programming languages but it can sometimes be worth it to use those from special libraries because they can often convey additional detail.\n\n# Load the psych library\nlibrary(psych)\n\n# Get the histogram of crab \"size\" (carapace width in mm)\npsych::multi.hist(pie_crab$size)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussion: Data Exploration Tools\n\n\n\nWith a group of 4-5 others discuss the following questions:\n\nWhat tools do you use when exploring new data?\nDo you already use any of these tools to explore your data?\n\nIf you do, why do you use them?\nIf not, where do you think they might be valuable to include?\n\nWhat value–if any–do you see in including these exploratory efforts in your code workflow?\n\n\n\n\n\nQuality Control\nYou may have encountered the phrase “QA/QC” (Quality Assurance / Quality Control) in relation to data cleaning. Technically, quality assurance only encapsulates preventative measures for reducing errors. One example of QA would be using a template for field datasheets because using standard fields reduces the risk that data are recorded inconsistently and/or incompletely. Quality control on the other hand refers to all steps taken to resolve errors after data are collected. Any code that you write to fix typos or remove outliers from a dataset falls under the umbrella of QC.\nIn synthesis work, QA is only very rarely an option. You’ll be working with datasets that have already been collected and attempting to handle any issues post hoc which means the vast majority of data wrangling operations will be quality control methods. These QC efforts can be incredibly time-consuming so using a programming language (like R or Python) is a dramatic improvement over manually looking through the data using Microsoft Excel or other programs like it.\n\nQC Considerations\nThe datasets you gather for your synthesis project will likely have a multitude of issues you’ll need to resolve before the data are ready for visualization or analysis. Some of these issues may be clearly identified in that datasets’ metadata or apply to all datasets but it is good practice to make a thorough QC effort as early as is feasible. Keep the following data issues and/or checks in mind as we cover code tools that may be useful in this context later in the module.\n\nVerify taxonomic classificiations against authorities\n\nITIS, GBIF, and WoRMS are all examples of taxonomic authorities\nNote that many of these authorities have R or Python libraries that can make this verification step scripted rather than dependent on manual searches\n\nHandle missing data\n\nSome datasets will use a code to indicate missing values (likely identified in their metadata) while others will just have empty cells\n\nCheck for unreasonable values / outliers\n\nCan use conditionals to create “flags” for these values or just filter them out\n\nCheck geographic coordinates’ reasonability\n\nE.g., western hemisphere coordinates may lack the minus sign\n\nCheck date formatting\n\nI.e., if all sampling is done in the first week of each month it can be difficult to say whether a given date is formatted as MM/DD/YY or DD/MM/YY\n\nConsider spatial and temporal granularity among datasets\n\nYou may need to aggregate data from separate studies in different ways to ensure that the data are directly comparable across all of the data you gather\n\nHandle duplicate data / rows\n\n\n\nNumber Checking\nWhen you read in a dataset and a column that should be numeric is instead read in as a character, it can be a sign that there are malformed numbers lurking in the background. Checking for and resolving these non-numbers is preferable to simply coercing the column into being numeric because the latter method typically changes those values to ‘NA’ where a human might be able to deduce the true number each value ‘should be.’\n\n# Load the supportR package\nlibrary(supportR)\n\n\n# Create an example dataset with non-numbers in ideally numeric columns\nfish_ct &lt;- data.frame(\"species\" = c(\"salmon\", \"bass\", \"halibut\", \"moray eel\"),\n                      \"count\" = c(12, \"14x\", \"_23\", 1))\n\n# Check for malformed numbers in column(s) that should be numeric\nbad_nums &lt;- supportR::num_check(data = fish_ct, col = \"count\")\n\nFor 'count', 2 non-numbers identified: '14x' | '_23'\n\n\nIn the above example, “14x” would be coerced to NA if you simply force the column without checking but you could drop the “x” with text replacing methods once you use tools like this one to flag it for your attention.\n\n\nText Replacement\nOne of the simpler ways of handling text issues is just to replace a string with another string. Most programming languages support this functionality.\n\n# Use pattern match/replace to simplify problem entries\nfish_ct$count &lt;- gsub(pattern = \"x|_\", replacement = \"\", x = fish_ct$count)\n\n# Check that they are fixed\nbad_nums &lt;- supportR::num_check(data = fish_ct, col = \"count\")\n\nFor 'count', no non-numeric values identified.\n\n\nThe vertical line in the gsub example above lets us search for (and replace) multiple patterns. Note however that while you can search for many patterns at once, only a single replacement value can be provided with this function.\n\n\nRegular Expressions\nYou may sometimes want to perform more generic string matching where you don’t necessarily know–or want to list–all possible strings to find and replace. For instance, you may want remove any letter in a numeric column or find and replace numbers with some sort of text note. “Regular expressions” are how programmers specify these generic matches and using them can be a nice way of streamlining code.\n\n# Make a test vector\nregex_vec &lt;- c(\"hello\", \"123\", \"goodbye\", \"456\")\n\n# Find all numbers and replace with the letter X\ngsub(pattern = \"[[:digit:]]\", replacement = \"x\", x = regex_vec)\n\n[1] \"hello\"   \"xxx\"     \"goodbye\" \"xxx\"    \n\n# Replace any number of letters with only a single 0\ngsub(pattern = \"[[:alpha:]]+\", replacement = \"0\", x = regex_vec)\n\n[1] \"0\"   \"123\" \"0\"   \"456\"\n\n\nThe stringr package cheatsheet has a really nice list of regular expression options that you may find valuable if you want to delve deeper on this topic. Scroll to the second page of the PDF to see the most relevant parts.\n\n\n\nConditionals\nRather than finding and replacing content, you may want to create a new column based on the contents of a different column. In plain language you might phrase this as ‘if column X has [some values] then column Y should have [other values]’. These operations are called conditionals and are an important part of data wrangling.\nIf you only want your conditional to support two outcomes (as in an either/or statement) there are useful functions that support this. Let’s return to our Plum Island Ecosystems crab dataset for an example.\n\n# Load tidyverse\nlibrary(tidyverse)\n\n# Make a new colum with an either/or conditional\npie_crab_v2 &lt;- pie_crab %&gt;% \n1  dplyr::mutate(size_category = ifelse(test = (size &gt;= 15),\n                                       yes = \"big\",\n                                       no = \"small\"),\n                .after = size) \n\n# Count the number of crabs in each category\npie_crab_v2 %&gt;% \n  dplyr::group_by(size_category) %&gt;% \n  dplyr::summarize(crab_ct = dplyr::n())\n\n\n1\n\nmutate makes a new column, ifelse is actually doing the conditional\n\n\n\n\n# A tibble: 2 × 2\n  size_category crab_ct\n  &lt;chr&gt;           &lt;int&gt;\n1 big               179\n2 small             213\n\n\nIf you have multiple different conditions you can just stack these either/or conditionals together but this gets cumbersome quickly. It is preferable to instead use a function that supports as many alternates as you want!\n\n# Make a new column with several conditionals\npie_crab_v2 &lt;- pie_crab %&gt;% \n  dplyr::mutate(size_category = dplyr::case_when( \n1    size &lt;= 10 ~ \"tiny\",\n    size &gt; 10 & size &lt;= 15 ~ \"small\",\n    size &gt; 15 & size &lt;= 20 ~ \"big\",\n    size &gt; 20 ~ \"huge\",\n2    TRUE ~ \"uncategorized\"),\n                .after = size)\n\n# Count the number of crabs in each category\npie_crab_v2 %&gt;% \n  dplyr::group_by(size_category) %&gt;% \n  dplyr::summarize(crab_ct = dplyr::n())\n\n\n1\n\nSyntax is ‘test ~ what to do when true’\n\n2\n\nThis line is a catch-all for any rows that don’t meet previous conditions\n\n\n\n\n# A tibble: 4 × 2\n  size_category crab_ct\n  &lt;chr&gt;           &lt;int&gt;\n1 big               150\n2 huge               28\n3 small             178\n4 tiny               36\n\n\nNote that you can use functions like this one when you do have an either/or conditional if you prefer this format.\n\n\n\n\n\n\nActivity: Conditionals\n\n\n\nIn a script, attempt the following with the PIE crab data:\n\nCreate a column indicating when air temperature is above or below 13° Fahrenheit\nCreate a column indicating whether water temperature is lower than the first quartile, between the first quartile and the median water temp, between the median and the third quartile or greater than the third quartile\n\nHint: consult the summary function output!\n\n\n\n\n\n\nUniting / Separating Columns\nSometimes one column has multiple pieces of information that you’d like to consider separately. A date column is a common example of this because particular months are always in a given season regardless of the specific day or year. So, it can be useful to break a complete date (i.e., year/month/day) into its component bits to be better able to access those pieces of information.\n\n# Split date into each piece of temporal info\npie_crab_v3 &lt;- pie_crab_v2 %&gt;% \n  tidyr::separate_wider_delim(cols = date, \n1                              delim = \"-\",\n                              names = c(\"year\", \"month\", \"day\"),\n2                              cols_remove = TRUE)\n\n# Check that out\nstr(pie_crab_v3)\n\n\n1\n\n‘delim’ is short for “delimiter” which we covered in the Reproducibility module\n\n2\n\nThis argument specifies whether to remove the original column when making the new columns\n\n\n\n\ntibble [392 × 12] (S3: tbl_df/tbl/data.frame)\n $ year         : chr [1:392] \"2016\" \"2016\" \"2016\" \"2016\" ...\n $ month        : chr [1:392] \"07\" \"07\" \"07\" \"07\" ...\n $ day          : chr [1:392] \"24\" \"24\" \"24\" \"24\" ...\n $ latitude     : num [1:392] 30 30 30 30 30 30 30 30 30 30 ...\n $ site         : chr [1:392] \"GTM\" \"GTM\" \"GTM\" \"GTM\" ...\n $ size         : num [1:392] 12.4 14.2 14.5 12.9 12.4 ...\n $ size_category: chr [1:392] \"small\" \"small\" \"small\" \"small\" ...\n $ air_temp     : num [1:392] 21.8 21.8 21.8 21.8 21.8 ...\n $ air_temp_sd  : num [1:392] 6.39 6.39 6.39 6.39 6.39 ...\n $ water_temp   : num [1:392] 24.5 24.5 24.5 24.5 24.5 ...\n $ water_temp_sd: num [1:392] 6.12 6.12 6.12 6.12 6.12 ...\n $ name         : chr [1:392] \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" ...\n\n\nWhile breaking apart a column’s contents can be useful, it can also be helpful to combine the contents of several columns!\n\n# Re-combine data information back into date\npie_crab_v4 &lt;- pie_crab_v3 %&gt;% \n  tidyr::unite(col = \"date\",\n1               sep = \"/\",\n               year:day, \n2               remove = FALSE)\n\n# Structure check\nstr(pie_crab_v4)\n\n\n1\n\nThis is equivalent to the ‘delim’ argument in the previous function\n\n2\n\nComparable to the ‘cols_remove’ argument in the previous function\n\n\n\n\ntibble [392 × 13] (S3: tbl_df/tbl/data.frame)\n $ date         : chr [1:392] \"2016/07/24\" \"2016/07/24\" \"2016/07/24\" \"2016/07/24\" ...\n $ year         : chr [1:392] \"2016\" \"2016\" \"2016\" \"2016\" ...\n $ month        : chr [1:392] \"07\" \"07\" \"07\" \"07\" ...\n $ day          : chr [1:392] \"24\" \"24\" \"24\" \"24\" ...\n $ latitude     : num [1:392] 30 30 30 30 30 30 30 30 30 30 ...\n $ site         : chr [1:392] \"GTM\" \"GTM\" \"GTM\" \"GTM\" ...\n $ size         : num [1:392] 12.4 14.2 14.5 12.9 12.4 ...\n $ size_category: chr [1:392] \"small\" \"small\" \"small\" \"small\" ...\n $ air_temp     : num [1:392] 21.8 21.8 21.8 21.8 21.8 ...\n $ air_temp_sd  : num [1:392] 6.39 6.39 6.39 6.39 6.39 ...\n $ water_temp   : num [1:392] 24.5 24.5 24.5 24.5 24.5 ...\n $ water_temp_sd: num [1:392] 6.12 6.12 6.12 6.12 6.12 ...\n $ name         : chr [1:392] \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" ...\n\n\nNote in this output how despite re-combining data information the column is listed as a character column! Simply combining or separating data is not always enough so you need to really lean into frequent data structure checks to be sure that your data are structured in the way that you want.\n\n\nJoining Data\nOften the early steps of a synthesis project involve combining the data tables horizontally. You might imagine that you have two groups’ data on sea star abundance and–once you’ve synonymized the column names–you can simply ‘stack’ the tables on top of one another. Slightly trickier but no less common is combining tables by the contents of a shared column (or columns). Cases like this include wanting to combine your sea star table with ocean temperature data from the region of each group’s research. You can’t simply attach the columns because that assumes that the row order is identical between the two data tables (and indeed, that there are the same number of rows in both to begin with!). In this case, if both data tables shared some columns (perhaps “site” and coordinate columns) you can use joins to let your computer match these key columns and make sure that only appropriate rows are combined.\nBecause joins are completely dependent upon the value in both columns being an exact match, it is a good idea to carefully check the contents of those columns before attempting a join to make sure that the join will be successful.\n\n# Create a fish taxonomy dataframe that corresponds with the earlier fish dataframe\nfish_tax &lt;- data.frame(\"species\" = c(\"salmon\", \"bass\", \"halibut\", \"eel\"),\n                       \"family\" = c(\"Salmonidae\", \"Serranidae\", \"Pleuronectidae\", \"Muraenidae\"))\n\n# Check to make sure that the 'species' column matches between both tables\nsupportR::diff_check(old = fish_ct$species, new = fish_tax$species) \n\nFollowing element(s) found in old object but not new: \n\n\n[1] \"moray eel\"\n\n\nFollowing element(s) found in new object but not old: \n\n\n[1] \"eel\"\n\n\n\n# Use text replacement methods to fix that mistake in one table\nfish_tax_v2 &lt;- fish_tax %&gt;% \n1  dplyr::mutate(species = gsub(pattern = \"^eel$\",\n                               replacement = \"moray eel\", \n                               x = species))\n\n# Re-check to make sure that fixed it\nsupportR::diff_check(old = fish_ct$species, new = fish_tax_v2$species)\n\n\n1\n\nThe symbols around “eel” mean that we’re only finding/replacing exact matches. It doesn’t matter in this context but often replacing a partial match would result in more problems. For example, replacing “eel” with “moray eel” could make “electric eel” into “electric moray eel”.\n\n\n\n\nAll elements of old object found in new\n\n\nAll elements of new object found in old\n\n\nNow that the shared column matches between the two two dataframes we can use a join to combine them! There are four types of join:\n\nleft/right join\nfull join (a.k.a. outer join)\ninner join\nanti join\n\nYou can learn more about the types of join here or here but the quick explanation is that the name of the join indicates whether the rows of the “left” and/or the “right” table are retained in the combined table. In synthesis work a left join or full join is most common (where you have your primary data in the left position and some ancillary/supplementary dataset in the right position).\n\n# Let's combine the fish count and fish taxonomy information\nfish_df &lt;- fish_ct %&gt;% \n  # Actual join step\n1  dplyr::left_join(y = fish_tax_v2, by = \"species\") %&gt;%\n  # Move 'family' column to the left of all other columns\n  dplyr::relocate(family, .before = dplyr::everything())\n\n# Look at the result of that\nfish_df\n\n\n1\n\nThe ‘by’ argument accepts a vector of column names found in both data tables\n\n\n\n\n          family   species count\n1     Salmonidae    salmon    12\n2     Serranidae      bass    14\n3 Pleuronectidae   halibut    23\n4     Muraenidae moray eel     1\n\n\n\n\n\n\n\n\nActivity: Separating Columns & Joining Data\n\n\n\nIn a script, attempt the following with the PIE crab data:\n\nCreate a data frame where you bin months into seasons (i.e., winter, spring, summer, fall)\n\nUse your judgement on which month(s) should fall into each given PIE’s latitude/location\n\nJoin your season table to the PIE crab data based on month\n\nHint: you may need to modify the PIE dataset to ensure both data tables share at least one column upon which they can be joined\n\nCalculate the average size of crabs in each season in order to identify which season correlates with the largest crabs\n\n\n\n\n\nLeveraging Data Shape\nYou may already be familiar with data shape but fewer people recognize how playing with the shape of data can make certain operations dramatically more efficient. If you haven’t encountered it before, any data table can be said to have one of two ‘shapes’: either long or wide. Wide data have all measured variables from a single observation in one row (typically resulting in more columns than rows or “wider” data tables). Long data usually have one observation split into many rows (typically resulting in more rows than columns or “longer” data tables).\nData shape is often important for statistical analysis or visualization but it has an under-appreciated role to play in quality control efforts as well. If many columns have the shared criteria for what constitutes “tidy”, you can reshape the data to get all of those values into a single column (i.e., reshape longer), perform any needed wrangling, then–when you’re finished–reshape back into the original data shape (i.e., reshape wider). As opposed to applying the same operations repeatedly to each column individually.\nLet’s consider an example to help clarify this. We’ll simulate a butterfly dataset where both the number of different species and their sex were recorded in the same column. This makes the column not technically numeric and therefore unusable in analysis or visualization.\n\n# Generate a butterfly dataframe\nbfly_v1 &lt;- data.frame(\"pasture\" = c(\"PNW\", \"PNW\", \"RCS\", \"RCS\"),\n                      \"monarch\" = c(\"14m\", \"10f\", \"7m\", \"16f\"),\n                      \"melissa_blue\" = c(\"32m\", \"2f\", \"6m\", \"0f\"),\n                      \"swallowtail\" = c(\"1m\", \"3f\", \"0m\", \"5f\"))\n\n# First we'll reshape this into long format\nbfly_v2 &lt;- bfly_v1 %&gt;% \n  tidyr::pivot_longer(cols = -pasture, \n                      names_to = \"butterfly_sp\", \n                      values_to = \"count_sex\")\n\n# Check what that leaves us with\nhead(bfly_v2, n = 4)\n\n# A tibble: 4 × 3\n  pasture butterfly_sp count_sex\n  &lt;chr&gt;   &lt;chr&gt;        &lt;chr&gt;    \n1 PNW     monarch      14m      \n2 PNW     melissa_blue 32m      \n3 PNW     swallowtail  1m       \n4 PNW     monarch      10f      \n\n# Let's separate count from sex to be more usable later\nbfly_v3 &lt;- bfly_v2 %&gt;% \n  tidyr::separate_wider_regex(cols = count_sex, \n                              c(count = \"[[:digit:]]+\", sex = \"[[:alpha:]]\")) %&gt;% \n  # Make the 'count' column a real number now\n  dplyr::mutate(count = as.numeric(count))\n\n# Re-check output\nhead(bfly_v3, n = 4)\n\n# A tibble: 4 × 4\n  pasture butterfly_sp count sex  \n  &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;\n1 PNW     monarch         14 m    \n2 PNW     melissa_blue    32 m    \n3 PNW     swallowtail      1 m    \n4 PNW     monarch         10 f    \n\n# Reshape back into wide-ish format\nbfly_v4 &lt;- bfly_v3 %&gt;% \n  tidyr::pivot_wider(names_from = \"butterfly_sp\", values_from = count)\n\n# Re-re-check output\nhead(bfly_v4)\n\n# A tibble: 4 × 5\n  pasture sex   monarch melissa_blue swallowtail\n  &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n1 PNW     m          14           32           1\n2 PNW     f          10            2           3\n3 RCS     m           7            6           0\n4 RCS     f          16            0           5\n\n\nWhile we absolutely could have used the same function to break apart count and butterfly sex data it would have involved copy/pasting the same information repeatedly. By pivoting to long format first, we can greatly streamline our code. This can also be advantageous for unit conversions, applying data transformations, or checking text column contents among many other possible applications.\n\n\nLoops\nAnother way of simplfying repetitive operations is to use a “for loop” (often called simply “loops”). Loops allow you to iterate across a piece of code for a set number of times. Loops require you to define an “index” object that will change itself at the end of each iteration of the loop before beginning the next iteration. This index object’s identity will be determined by whatever set of values you define at the top of the loop.\nHere’s a very bare bones example to demonstrate the fundamentals.\n\n# Loop across each number between 2 and 4\n1for(k in 2:4){\n  \n  # Square the number\n  result &lt;- k^2\n  \n  # Message that outside of the loop\n  message(k, \" squared is \", result)\n2}\n\n\n1\n\n‘k’ is our index object in this loop\n\n2\n\nNote that the operations to iterate across are wrapped in curly braces ({...})\n\n\n\n\n2 squared is 4\n\n\n3 squared is 9\n\n\n4 squared is 16\n\n\nOnce you get the hang of loops, they can be a nice way of simplifying your code in a relatively human-readable way! Let’s return to our Plum Island Ecosystems crab dataset for a more nuanced example.\n\n# Create an empty list\ncrab_list &lt;- list()\n\n# Let's loop across size categories of crab\n1for(focal_size in unique(pie_crab_v4$size_category)){\n  \n  # Subset the data to just this size category\n  focal_df &lt;- pie_crab_v4 %&gt;% \n    dplyr::filter(size_category == focal_size)\n  \n  # Calculate average and standard deviation of size within this category\n  size_avg &lt;- mean(focal_df$size, na.rm = T) \n  size_dev &lt;- sd(focal_df$size, na.rm = T) \n  \n  # Assemble this into a data table and add to our list\n  crab_list[[focal_size]] &lt;- data.frame(\"size_category\" = focal_size,\n                                        \"size_mean\" = size_avg,\n                                        \"size_sd\" = size_dev)\n} # Close loop\n\n# Unlist the outputs into a dataframe\n2crab_df &lt;- purrr::list_rbind(x = crab_list)\n\n# Check out the resulting data table\ncrab_df\n\n\n1\n\nNote that this is not the most efficient way of doing group-wise summarization but is–hopefully–a nice demonstration of loops!\n\n2\n\nWhen all elements of your list have the same column names, list_rbind efficiently stacks those elements into one longer data table.\n\n\n\n\n  size_category size_mean   size_sd\n1         small 12.624270 1.3827471\n2          tiny  8.876944 0.9112686\n3           big 17.238267 1.3650173\n4          huge 21.196786 0.8276744\n\n\n\n\nCustom Functions\nFinally, writing your own, customized functions can be really useful particularly when doing synthesis work. Custom functions are generally useful for reducing duplication and increasing ease of maintenance (see the note on custom functions in the SSECR Reproducibility module) and also can be useful end products of synthesis work in and of themselves.\nIf one of your group’s outputs is a new standard data format or analytical workflow, the functions that you develop to aid yourself become valuable to anyone who adopts your synthesis project’s findings into their own workflows. If you get enough functions you can even release a package that others can install and use on their own computers. Such packages are a valuable product of synthesis efforts and can be a nice addition to a robust scientific resume/CV.\n\n# Define custom function\ncrab_hist &lt;- function(df, size_cat){\n  \n  # Subset data to the desired category\n  data_sub &lt;- dplyr::filter(.data = df, size_category == size_cat)\n  \n  # Create a histogram\n  p &lt;- psych::multi.hist(x = data_sub$size)\n}\n\n# Invoke function\ncrab_hist(df = pie_crab_v4, size_cat = \"tiny\")\n\n\n\n\n\n\n\n\nWhen writing your own functions it can also be useful to program defensively. This involves anticipating likely errors and writing your own error messages that are more informative to the user than whatever machine-generated error would otherwise get generated\n\n# Define custom function\n1crab_hist &lt;- function(df, size_cat = \"small\"){\n  \n  # Error out if 'df' isn't the right format\n2  if(is.data.frame(df) != TRUE)\n    stop(\"'df' must be provided as a data frame\")\n  \n  # Error out if the data doesn't have the right columns\n3  if(all(c(\"size_category\", \"size\") %in% names(df)) != TRUE)\n    stop(\"'df' must include a 'size' and 'size_category' column\")\n  \n  # Error out for unsupported size category values\n  if(size_cat %in% unique(df$size_category) != TRUE)\n    stop(\"Specified 'size_cat' not found in provided data\")\n  \n  # Subset data to the desired category\n  data_sub &lt;- dplyr::filter(.data = df, size_category == size_cat)\n  \n  # Create a histogram\n  p &lt;- psych::multi.hist(x = data_sub$size)\n}\n\n# Invoke new-and-improved function\n4crab_hist(df = pie_crab_v4)\n\n\n1\n\nThe default category is now set to “small”\n\n2\n\nWe recommend phrasing your error checks with this format (i.e., ’if &lt;some condition&gt; is not true, then &lt;informative error/warning message&gt;)\n\n3\n\nThe %in% operator lets you check whether one value matches any element of a set of accepted values. Very useful in contexts like this because the alternative would be a lot of separate “or” conditionals\n\n4\n\nWe don’t need to specify the ‘size_cat’ argument because we can rely on the default\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nActivity: Custom Functions\n\n\n\nIn a script, attempt the following on the PIE crab data:\n\nWrite a function that:\n(A) calculates the median of the user-supplied column\n\n\ndetermines whether each value is above, equal to, or below the median\n\n\nmakes a column indicating the results of step B\n\n\nUse the function on the standard deviation of water temperature\nUse it again on the standard deviation of air temperature\nRevisit your function and identify 2-3 likely errors\nWrite custom checks (and error messages) for the set of likely issues you just identified",
    "crumbs": [
      "Phase III -- Execute",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "mod_wrangle.html#additional-resources",
    "href": "mod_wrangle.html#additional-resources",
    "title": "Data Harmonization & Wrangling",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\nTodd-Brown, K.E.O. et al. Reviews and Syntheses: The Promise of Big Diverse Soil Data, Moving Current Practices Towards Future Potential. 2022. Biogeosciences\nElgarby, O. The Ultimate Guide to Data Cleaning. 2019. Medium\nBorer, E. et al. Some Simple Guidelines for Effective Data Management. 2009. Ecological Society of America Bulletin\n\n\n\nWorkshops & Courses\n\nThe Carpentries. Data Analysis and Visualization in R for Ecologists: Working with Data. 2024.\nLTER Scientific Computing Team. Coding in the Tidyverse. 2023.\nNational Center for Ecological Analysis and Synthesis (NCEAS) Learning Hub coreR: Cleaning & Wrangling Data. 2023.\nNCEAS Learning Hub coreR: Writing Functions & Packages. 2023.\nDelta Science Program Data Munging / QA / QC / Cleaning. 2019.\n\n\n\nWebsites\n\nFox, J. Ten Commandments for Good Data Management. 2016. Dynamic Ecology",
    "crumbs": [
      "Phase III -- Execute",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "policy/attendance.html",
    "href": "policy/attendance.html",
    "title": "Synthesis Skills for Early Career Researchers",
    "section": "",
    "text": "If you get sick, observe a religious holiday unaccounted for by the SSECR schedule, have to miss class for an interview, or simply don’t think you can handle class on a given day, please email the course instructors as early as possible to let us know that you won’t be in class with a (brief) explanation. This will help us to share resources we’ll cover in class with you and plan for a smaller in-class community while you are out. Our hope is that this course will be somewhere you want to attend, but we totally understand that you have many demands on your time and sometimes life happens!\nPlease keep in mind that your presence in and contributions to class are important both to your understanding of the material and the creation and maintenance of an in-class community."
  },
  {
    "objectID": "policy/pronouns.html",
    "href": "policy/pronouns.html",
    "title": "Synthesis Skills for Early Career Researchers",
    "section": "",
    "text": "You provided a name when you first applied to be a part of the course but we will gladly honor your request to be addressed by an alternate name. We will also use whichever pronouns you identify with. Please advise us of your pronouns and/or chosen name early in the course so that we can ensure that we treat you respectfully throughout the course."
  },
  {
    "objectID": "proj_milestones.html",
    "href": "proj_milestones.html",
    "title": "Project Milestones",
    "section": "",
    "text": "This page is meant to provide the project teams with rough progress estimates in relation to the course’s instructional modules. Your group may at times be ahead of these milestones or behind at others so please use this resource as general guidance rather than specific timing requirements."
  },
  {
    "objectID": "proj_milestones.html#overview",
    "href": "proj_milestones.html#overview",
    "title": "Project Milestones",
    "section": "",
    "text": "This page is meant to provide the project teams with rough progress estimates in relation to the course’s instructional modules. Your group may at times be ahead of these milestones or behind at others so please use this resource as general guidance rather than specific timing requirements."
  },
  {
    "objectID": "proj_milestones.html#milestones",
    "href": "proj_milestones.html#milestones",
    "title": "Project Milestones",
    "section": "Milestones",
    "text": "Milestones\n\nMonths 1-2 (Sep./Oct.)\n\nLiterature review well underway and/or complete\nGood progress finding data (50-75% known needed data found)\nCollaboratively identified ground rules, communication and coordination plan, and project roles\n\n\n\nMonth 3 (Nov.)\n\nDraft a plan for your analyses\n\nLooking for sufficient detail that you can articulate what harmonized data might be needed\n\nLargely finished finding data\nExploratory visualization\n\n\n\nMonth 4 (Dec.)\n\nStrong progress harmonizing your data\nInternal project status check-in and team dynamics assessment\n\n\n\nMonth 5 (Jan.)\n\nFinish harmonizing data\nRevize planned analyses\n\nHarmonizing the data will help inform what questions you want to (or are able to) ask\n\n\n\n\nMonth 6 (Feb.)\n\nRun and interpret preliminary analyses\nConsider drafting an ‘extended figures document’ that summarizes key messages and includes rough draft figures)\nCreate an intellectual credit argeement as a team\n\n\n\nMonth 7 (Mar.)\n\nDraft outline for the product that your team aims to create\nBegin generating publication-quality figures\nPrepare datasets for publication (if relevant)\n\nConsider FAIR and CARE data practices (see Reproducibility module)\n\n\n\n\nMonth 8-9 (Apr./May)\n\nDraft publishable dataset\nTeam product ready to report on (internally) to SSECR fellows & instructors\nIntellectual credit contributions table completed"
  }
]