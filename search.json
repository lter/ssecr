[
  {
    "objectID": "mod_thinking.html",
    "href": "mod_thinking.html",
    "title": "Supporting Divergent, Emergent, and Convergent Thinking",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase III -- Execute",
      "Supporting D.E.C. Thinking"
    ]
  },
  {
    "objectID": "mod_thinking.html#overview",
    "href": "mod_thinking.html#overview",
    "title": "Supporting Divergent, Emergent, and Convergent Thinking",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase III -- Execute",
      "Supporting D.E.C. Thinking"
    ]
  },
  {
    "objectID": "mod_thinking.html#learning-objectives",
    "href": "mod_thinking.html#learning-objectives",
    "title": "Supporting Divergent, Emergent, and Convergent Thinking",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nDevelop strategies to support participation, innovative thinking, integration, and convergence / decision making in a team setting",
    "crumbs": [
      "Phase III -- Execute",
      "Supporting D.E.C. Thinking"
    ]
  },
  {
    "objectID": "mod_thinking.html#module-content",
    "href": "mod_thinking.html#module-content",
    "title": "Supporting Divergent, Emergent, and Convergent Thinking",
    "section": "Module Content",
    "text": "Module Content",
    "crumbs": [
      "Phase III -- Execute",
      "Supporting D.E.C. Thinking"
    ]
  },
  {
    "objectID": "mod_thinking.html#additional-resources",
    "href": "mod_thinking.html#additional-resources",
    "title": "Supporting Divergent, Emergent, and Convergent Thinking",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\nFacilitator’s Guide to Participatory Decision-Making by Sam Kaner\n\n\n\nWorkshops & Courses\n\n\n\n\n\nWebsites\n\nLiberating Structures website",
    "crumbs": [
      "Phase III -- Execute",
      "Supporting D.E.C. Thinking"
    ]
  },
  {
    "objectID": "mod_project-mgmt.html",
    "href": "mod_project-mgmt.html",
    "title": "Project Management & Logic Models",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase II -- Plan",
      "Project Management"
    ]
  },
  {
    "objectID": "mod_project-mgmt.html#overview",
    "href": "mod_project-mgmt.html#overview",
    "title": "Project Management & Logic Models",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase II -- Plan",
      "Project Management"
    ]
  },
  {
    "objectID": "mod_project-mgmt.html#learning-objectives",
    "href": "mod_project-mgmt.html#learning-objectives",
    "title": "Project Management & Logic Models",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nArticulate key principles of project management\nDevelop (or refine) the project management framework for your team project\nDefine common approaches for logic models\nIdentify and make explicit internal logical leaps",
    "crumbs": [
      "Phase II -- Plan",
      "Project Management"
    ]
  },
  {
    "objectID": "mod_project-mgmt.html#module-content",
    "href": "mod_project-mgmt.html#module-content",
    "title": "Project Management & Logic Models",
    "section": "Module Content",
    "text": "Module Content",
    "crumbs": [
      "Phase II -- Plan",
      "Project Management"
    ]
  },
  {
    "objectID": "mod_project-mgmt.html#additional-resources",
    "href": "mod_project-mgmt.html#additional-resources",
    "title": "Project Management & Logic Models",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\n\n\n\n\nWorkshops & Courses\n\nNCEAS Scientific Computing team’s Collaborative Coding with GitHub workshop project management modules\n\nGitHub Issues\nGitHub Projects\n\nOpen Science Synthesis for the Delta Science Program’s Logic Models and Synthesis Development\n\n\n\nWebsites",
    "crumbs": [
      "Phase II -- Plan",
      "Project Management"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "SSECR Contributing Guidelines",
    "section": "",
    "text": "We are using a GitHub Team (see here for more information on GitHub Teams) to simplify access protocols to SSECR GitHub materials.\nContact Marty Downs and/or Nick Lyon to be added to the SSECR GitHub Team. This will give you write-level access to (A) the SSECR GitHub repository and (B) the SSECR GitHub Project that we’re using for task management.\n\n\n\nContact Marty Downs and/or Nick Lyon for access to the Google Drive. The Shared Drive is named “LTER-Grad-Course”.\n\n\n\nCommunicating via email is fine though we also have a channel (#lter-grad-course) in NCEAS’ Slack organization if that is preferable.\n\n\n\n\n\nIndividual tasks should be tracked as GitHub Issues\n\nBe sure that each task is SMART (i.e., specific, measurable, achievable, relevant, and time-bound)\n\nPlease use the issue template\n\nWhen you select “New Issue” you will be prompted to use this template automatically\n\nTry to document task progress within the dedicated issue for that task (for posterity)\nStrategic planning (i.e., project management across tasks) should use the SSECR GitHub Project\n\nTask lifecycle can be tracked by dragging an issue’s “card” among columns that correspond to major steps in task completion\n\n\n\n\n\nAs much as possible, use snake case (i.e., all_lowercase_separated_by_underscores). When in doubt, try to maintain consistency with the naming convention and internal structure of other files in the same directory/repository."
  },
  {
    "objectID": "CONTRIBUTING.html#accessing-course-materials",
    "href": "CONTRIBUTING.html#accessing-course-materials",
    "title": "SSECR Contributing Guidelines",
    "section": "",
    "text": "We are using a GitHub Team (see here for more information on GitHub Teams) to simplify access protocols to SSECR GitHub materials.\nContact Marty Downs and/or Nick Lyon to be added to the SSECR GitHub Team. This will give you write-level access to (A) the SSECR GitHub repository and (B) the SSECR GitHub Project that we’re using for task management.\n\n\n\nContact Marty Downs and/or Nick Lyon for access to the Google Drive. The Shared Drive is named “LTER-Grad-Course”.\n\n\n\nCommunicating via email is fine though we also have a channel (#lter-grad-course) in NCEAS’ Slack organization if that is preferable."
  },
  {
    "objectID": "CONTRIBUTING.html#project-management",
    "href": "CONTRIBUTING.html#project-management",
    "title": "SSECR Contributing Guidelines",
    "section": "",
    "text": "Individual tasks should be tracked as GitHub Issues\n\nBe sure that each task is SMART (i.e., specific, measurable, achievable, relevant, and time-bound)\n\nPlease use the issue template\n\nWhen you select “New Issue” you will be prompted to use this template automatically\n\nTry to document task progress within the dedicated issue for that task (for posterity)\nStrategic planning (i.e., project management across tasks) should use the SSECR GitHub Project\n\nTask lifecycle can be tracked by dragging an issue’s “card” among columns that correspond to major steps in task completion"
  },
  {
    "objectID": "CONTRIBUTING.html#style-guide",
    "href": "CONTRIBUTING.html#style-guide",
    "title": "SSECR Contributing Guidelines",
    "section": "",
    "text": "As much as possible, use snake case (i.e., all_lowercase_separated_by_underscores). When in doubt, try to maintain consistency with the naming convention and internal structure of other files in the same directory/repository."
  },
  {
    "objectID": "mod_data-disc.html",
    "href": "mod_data-disc.html",
    "title": "Data Discovery & Management",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase I -- Prepare",
      "Data Discovery"
    ]
  },
  {
    "objectID": "mod_data-disc.html#overview",
    "href": "mod_data-disc.html#overview",
    "title": "Data Discovery & Management",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase I -- Prepare",
      "Data Discovery"
    ]
  },
  {
    "objectID": "mod_data-disc.html#learning-objectives",
    "href": "mod_data-disc.html#learning-objectives",
    "title": "Data Discovery & Management",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nExplore common data repositories\nUse Google search operators to more effectively perform general searches\nDefine fundamental properties of effective data management plans\nCreate a data management plan\nDescribe best practices for documenting data for archiving",
    "crumbs": [
      "Phase I -- Prepare",
      "Data Discovery"
    ]
  },
  {
    "objectID": "mod_data-disc.html#module-content",
    "href": "mod_data-disc.html#module-content",
    "title": "Data Discovery & Management",
    "section": "Module Content",
    "text": "Module Content",
    "crumbs": [
      "Phase I -- Prepare",
      "Data Discovery"
    ]
  },
  {
    "objectID": "mod_data-disc.html#additional-resources",
    "href": "mod_data-disc.html#additional-resources",
    "title": "Data Discovery & Management",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\nThe British Ecological Society’s Better Science Guides – Data Management Guide\n\n\n\nWorkshops & Courses\n\nNCEAS coreR Data Management Essentials lesson\nOpen Science Synthesis for the Delta Science Program’s Data Management Essentials and the FAIR & CARE Principles\nOpen Science Synthesis for the Delta Science Program’s Writing Data Management Plans\nNCEAS Scientific Computing team’s Data Acquisition instructions\n\n\n\nWebsites\n\nEnvironmental Data Initiative (EDI) Data Portal\nDataONE Data Catalog\nOcean Observatories Initiative (OOI) Data Explorer",
    "crumbs": [
      "Phase I -- Prepare",
      "Data Discovery"
    ]
  },
  {
    "objectID": "mod_team-sci.html",
    "href": "mod_team-sci.html",
    "title": "Team Science Practices",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase I -- Prepare",
      "Team Science"
    ]
  },
  {
    "objectID": "mod_team-sci.html#overview",
    "href": "mod_team-sci.html#overview",
    "title": "Team Science Practices",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase I -- Prepare",
      "Team Science"
    ]
  },
  {
    "objectID": "mod_team-sci.html#learning-objectives",
    "href": "mod_team-sci.html#learning-objectives",
    "title": "Team Science Practices",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nInterpret and enact current best practices in team science\nIdentify different interaction styles and the effect they have on group dynamics\nIdentify benefits (and potential costs of) diverse teams\nDescribe ways to mitigate costs of diverse teams\nExplain methods for improving the experience of virtual participants on hybrid teams ## Module Content",
    "crumbs": [
      "Phase I -- Prepare",
      "Team Science"
    ]
  },
  {
    "objectID": "mod_team-sci.html#additional-resources",
    "href": "mod_team-sci.html#additional-resources",
    "title": "Team Science Practices",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\nBates et al., Overcome Imposter Syndrome: Contribute to Working Groups and Build Strong Networks. 2024. Biological Conservation\nFarrell et al., Training Macrosystems Scientists Requires Both Interpersonal and Technical Skills. 2021. Frontiers in Ecology and the Environment\nHampton & Parker, Collaboration and Productivity in Scientific Synthesis. 2011. BioScience\n\n\n\nWorkshops & Courses\n\nAmelia Liberatore’s Developing a Successful Team workshop slides\nOpen Science Synthesis for the Delta Science Program’s Team Science for Synthesis\nOpen Science Synthesis for the Delta Science Program’s Thinking Preferences\n\n\n\nWebsites",
    "crumbs": [
      "Phase I -- Prepare",
      "Team Science"
    ]
  },
  {
    "objectID": "mod_spatial.html",
    "href": "mod_spatial.html",
    "title": "Working with Spatial Data",
    "section": "",
    "text": "Under Construction",
    "crumbs": [
      "Bonus Modules",
      "Spatial Data"
    ]
  },
  {
    "objectID": "mod_spatial.html#overview",
    "href": "mod_spatial.html#overview",
    "title": "Working with Spatial Data",
    "section": "",
    "text": "Under Construction",
    "crumbs": [
      "Bonus Modules",
      "Spatial Data"
    ]
  },
  {
    "objectID": "mod_spatial.html#learning-objectives",
    "href": "mod_spatial.html#learning-objectives",
    "title": "Working with Spatial Data",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this topic you will be able to:\n\nDefine characteristics of common types of spatial data\nManipulate spatial data with R\nIntegrate spatial data with tabular data",
    "crumbs": [
      "Bonus Modules",
      "Spatial Data"
    ]
  },
  {
    "objectID": "mod_spatial.html#needed-packages",
    "href": "mod_spatial.html#needed-packages",
    "title": "Working with Spatial Data",
    "section": "Needed Packages",
    "text": "Needed Packages\nIf you’d like to follow along with the code chunks included throughout this module, you’ll need to install the following packages:\n\n# Note that these lines only need to be run once per computer\n## So you can skip this step if you've installed these before\ninstall.packages(\"tidyverse\")",
    "crumbs": [
      "Bonus Modules",
      "Spatial Data"
    ]
  },
  {
    "objectID": "mod_spatial.html#module-content",
    "href": "mod_spatial.html#module-content",
    "title": "Working with Spatial Data",
    "section": "Module Content",
    "text": "Module Content",
    "crumbs": [
      "Bonus Modules",
      "Spatial Data"
    ]
  },
  {
    "objectID": "mod_spatial.html#additional-spatial-resources",
    "href": "mod_spatial.html#additional-spatial-resources",
    "title": "Working with Spatial Data",
    "section": "Additional Spatial Resources",
    "text": "Additional Spatial Resources\n\nPapers & Documents\n\n\n\n\n\nWorkshops & Courses\n\nThe Carpentries’ Introduction to Geospatial Raster & Vector Data with R\nThe Carpentries’ Introduction to R for Geospatial Data\nArctic Data Center’s Spatial and Image Data Using GeoPandas chapter of their Scalable Computing course\nJason Flower’s (UC Santa Barbara) Introduction to rasters with terra\n\n\n\nWebsites\n\nNASA’s Application for Extracting and Exploring Analysis Ready Samples (AppEEARS) Portal",
    "crumbs": [
      "Bonus Modules",
      "Spatial Data"
    ]
  },
  {
    "objectID": "mod_stats.html",
    "href": "mod_stats.html",
    "title": "Analysis & Modeling",
    "section": "",
    "text": "Given the wide range in statistical training in graduate curricula (and corresponding breadth of experience among early career researchers), we’ll be approaching this module in a different way than the others. One half of the module will use a “flipped approach” where project teams will share their proposed analyses with one another. The other half of the module will be dedicated to analyses that are more common in–or exclusive to–synthesis research.",
    "crumbs": [
      "Phase III -- Execute",
      "Analysis & Modeling"
    ]
  },
  {
    "objectID": "mod_stats.html#overview",
    "href": "mod_stats.html#overview",
    "title": "Analysis & Modeling",
    "section": "",
    "text": "Given the wide range in statistical training in graduate curricula (and corresponding breadth of experience among early career researchers), we’ll be approaching this module in a different way than the others. One half of the module will use a “flipped approach” where project teams will share their proposed analyses with one another. The other half of the module will be dedicated to analyses that are more common in–or exclusive to–synthesis research.",
    "crumbs": [
      "Phase III -- Execute",
      "Analysis & Modeling"
    ]
  },
  {
    "objectID": "mod_stats.html#learning-objectives",
    "href": "mod_stats.html#learning-objectives",
    "title": "Analysis & Modeling",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nDescribe proposed analytical methods to an interested audience of mixed prior experience\nExplain nuance in interpretation of results of proposed analyses\nCompare and contrast interpretation of results in synthesis work versus primary research\nIdentify statistical tests common in synthesis research\nPerform some synthesis-specific analyses",
    "crumbs": [
      "Phase III -- Execute",
      "Analysis & Modeling"
    ]
  },
  {
    "objectID": "mod_stats.html#needed-packages",
    "href": "mod_stats.html#needed-packages",
    "title": "Analysis & Modeling",
    "section": "Needed Packages",
    "text": "Needed Packages\nIf you’d like to follow along with the code chunks included throughout this module, you’ll need to install the following packages:\n\n# Note that these lines only need to be run once per computer\n## So you can skip this step if you've installed these before\ninstall.packages(\"tidyverse\")",
    "crumbs": [
      "Phase III -- Execute",
      "Analysis & Modeling"
    ]
  },
  {
    "objectID": "mod_stats.html#mixed-effects-models",
    "href": "mod_stats.html#mixed-effects-models",
    "title": "Analysis & Modeling",
    "section": "Mixed-Effects Models",
    "text": "Mixed-Effects Models",
    "crumbs": [
      "Phase III -- Execute",
      "Analysis & Modeling"
    ]
  },
  {
    "objectID": "mod_stats.html#multi-model-inference",
    "href": "mod_stats.html#multi-model-inference",
    "title": "Analysis & Modeling",
    "section": "Multi-Model Inference",
    "text": "Multi-Model Inference",
    "crumbs": [
      "Phase III -- Execute",
      "Analysis & Modeling"
    ]
  },
  {
    "objectID": "mod_stats.html#meta-analysis",
    "href": "mod_stats.html#meta-analysis",
    "title": "Analysis & Modeling",
    "section": "Meta-Analysis",
    "text": "Meta-Analysis",
    "crumbs": [
      "Phase III -- Execute",
      "Analysis & Modeling"
    ]
  },
  {
    "objectID": "mod_stats.html#additional-resources",
    "href": "mod_stats.html#additional-resources",
    "title": "Analysis & Modeling",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\nUnderstanding ‘It Depends’ in Ecology: A Guide to Hypothesising, Visualising and Interpreting Statistical Interactions. Spake et al., 2023. Biological Reviews\nImproving Quantitative Synthesis to Achieve Generality in Ecology. Spake et al., 2022.Nature Ecology and Evolution\nDoing Meta-Analysis with R: A Hands-On Guide\n\n\n\nWorkshops & Courses\n\nMatt Vuorre’s Bayesian Meta-Analysis with R, Stan, and brms\n\n\n\nWebsites",
    "crumbs": [
      "Phase III -- Execute",
      "Analysis & Modeling"
    ]
  },
  {
    "objectID": "mod_reproducibility.html",
    "href": "mod_reproducibility.html",
    "title": "Reproducibility Best Practices",
    "section": "",
    "text": "As we set out to engage with the synthesis skills this course aims to offer, it will be helpful to begin with a careful consideration of “reproducibility.” Because synthesis projects draw data from many sources and typically involve many researchers working in concert, reproducibility is particularly important. In individual projects, adhering to reproducibility best practices is certainly a good goal but failing to do so for synthesis projects can severely limit the work in a more significant way than for those individual projects. “Reproducibility” is a wide sphere encompassing many different–albeit related–topics so it can be challenging to feel well-equipped to evaluate how well we are following these guidelines in our own work. In this module, we will cover a few fundamental facets of reproducibility and point to some considerations that may encourage you to push yourself to elevate your practices to the next level.",
    "crumbs": [
      "Phase I -- Prepare",
      "Reproducibility"
    ]
  },
  {
    "objectID": "mod_reproducibility.html#overview",
    "href": "mod_reproducibility.html#overview",
    "title": "Reproducibility Best Practices",
    "section": "",
    "text": "As we set out to engage with the synthesis skills this course aims to offer, it will be helpful to begin with a careful consideration of “reproducibility.” Because synthesis projects draw data from many sources and typically involve many researchers working in concert, reproducibility is particularly important. In individual projects, adhering to reproducibility best practices is certainly a good goal but failing to do so for synthesis projects can severely limit the work in a more significant way than for those individual projects. “Reproducibility” is a wide sphere encompassing many different–albeit related–topics so it can be challenging to feel well-equipped to evaluate how well we are following these guidelines in our own work. In this module, we will cover a few fundamental facets of reproducibility and point to some considerations that may encourage you to push yourself to elevate your practices to the next level.",
    "crumbs": [
      "Phase I -- Prepare",
      "Reproducibility"
    ]
  },
  {
    "objectID": "mod_reproducibility.html#learning-objectives",
    "href": "mod_reproducibility.html#learning-objectives",
    "title": "Reproducibility Best Practices",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nIdentify core tenets of reproducibility best practices\nCreate robust workflow documentation\nImplement reproducible project organization strategies\nDiscuss methods for improving the reproducibility of your code products\nSummarize FAIR and CARE data principles\nEvaluate the FAIR/CAREness of your work",
    "crumbs": [
      "Phase I -- Prepare",
      "Reproducibility"
    ]
  },
  {
    "objectID": "mod_reproducibility.html#lego-activity",
    "href": "mod_reproducibility.html#lego-activity",
    "title": "Reproducibility Best Practices",
    "section": "Lego Activity",
    "text": "Lego Activity\nBefore we dive into the world of reproducibility for synthesis projects, we thought it would be fun (and informative!) to begin with an activity that is a useful analogy for the importance of some of the concepts we’ll cover today. The LEGO activity was designed by Mary Donaldson and Matt Mahon at the University of Glasgow. The full materials can be accessed here.",
    "crumbs": [
      "Phase I -- Prepare",
      "Reproducibility"
    ]
  },
  {
    "objectID": "mod_reproducibility.html#project-documentation-organization",
    "href": "mod_reproducibility.html#project-documentation-organization",
    "title": "Reproducibility Best Practices",
    "section": "Project Documentation & Organization",
    "text": "Project Documentation & Organization\nMuch of the popular conversation around reproducibility centers on reproducibility as it pertains to code. That is definitely an important facet but before we write even a single line it is vital to consider project-wide reproducibility. “Perfect” code in a project that isn’t structured thoughtfully can still result in a project that isn’t reproducible. On the other hand, “bad” code can be made more intelligible when it is placed in a well-documented/organized project!\n\nDocumentation\nDocumenting a project can feel daunting but it is often not as hard as one might imagine and always well worth the effort! One simple practice you can adopt to dramatically improve the reproducibility of your project is to create a “README” file in the top-level of your project’s folder system. This file can be formatted however you’d like but generally READMEs should include:\n\nProject overview written in plain language\nBasic table of contents for the primary folders in your project folder\nBrief description of the file naming scheme you’ve adopted for this project.\n\nYour project’s README becomes the ‘landing page’ for those navigating your repository and makes it easy for team members to know where documentation should go (in the README!). You may also choose to create a README file for some of the sub-folders of your project. This can be particularly valuable for your “data” folder(s) as it is an easy place to store data source/provenance information that might be overwhelming to include in the project-level README file.\nFinally, you should choose a place to keep track of ideas, conversations, and decisions about the project. While you can take notes on these topics on a piece of paper, adopting a digital equivalent is often helpful because you can much more easily search a lengthy document when it is machine readable. We will discuss GitHub during the Version Control module but GitHub offers something called Issues that can be a really effective place to record some of this information.\n\n\n\n\n\n\nActivity: Create a README\n\n\n\nCreate a draft README for one of your research projects. If all of your projects already have READMEs (very impressive!) revisit the one with the least detail.\n\nInclude a 2-4 sentence description of the project objectives / hypotheses\nIdentify and describe (in 1 sentence) the primary sub-folders in the project\nIf your chosen project includes scripts, summarize each and indicate which script(s) they depend on and which depend on them\n\nFeel free to put your personal flair on the README! If there is other information you feel would be relevant to an outsider looking at your project, you can definitely add that.\n\n\n\n\nFundamental Structure\n\nThe simplest way of beginning a reproducible project is adopting a good file organization system. There is no single “best” way of organizing your projects’ files as long as you are consistent. Consistency will make your system–whatever that consists of–understandable to others.\nHere are some rules to keep in mind as you decide how to organize your project:\n\nUse one folder per project\n\nKeeping all inputs, outputs, and documentation in a single folder makes it easier to collaborate and share all project materials. Also, most programming applications (RStudio, VS Code, etc.) work best when all needed files are in the same folder.\n\nOrganize content with sub-folders\n\nPutting files that share a purpose or source into logical sub-folders is a great idea! This makes it easy to figure out where to put new content and reduces the effort of documenting project organization. Don’t feel like you need to use an intricate web of sub-folders either! Just one level of sub-folders is enough for many projects.\n\nCraft informative file names\n\nAn ideal file name should give some information about the file’s contents, purpose, and relation to other project files. Some of that may be reinforced by folder names, but the file name itself should be inherently meaningful. This lets you change folder names without fear that files would also need to be re-named.\n\n\n\n\n\n\nDiscussion: Project Structure\n\n\n\nWith a partner discuss (some of) the following questions:\n\nHow do you typically organize your projects’ files?\nWhat benefits do you see of your current approach?\nWhat–if any–limitations to your system have you experienced?\nDo you think your structure would work well in a team environment?\n\nIf not, what changes might you make to better fit that context?\n\n\n\n\n\nNaming Tips\nWe’ve brought up the importance of naming several times already but haven’t actually discussed the specifics of what makes a “good” name for a file or folder. Consider the adopting some (or all!) of the file name tips we outline below.\n\nNames should be sorted by a computer and human in the same way\n\nComputers sort files/folders alphabetically and numerically. Sorting alphabetically rarely matches the order scripts in a workflow should be run. If you add step numbers to the start of each file name the computer will sort the files in an order that makes sense for the project. You may also want to “zero pad” numbers so that all numbers have the same number of digits (e.g., “01” and “10” vs. “1” and “10”).\n\nNames should avoid spaces and special characters\n\nSpaces and special characters (e.g., é, ü, etc.) cause errors in some computers (particularly Windows operating systems). You can replace spaces with underscores or hyphens to increase machine readability. Avoid using special characters as much as possible. You should also be consistent about casing (i.e., lower vs. uppercase).\n\nNames should use consistent delimiters\n\nDelimiters are characters used to separate pieces of information in otherwise plain text. Underscores are a commonly used example of this. If a file/folder name has multiple pieces of information, you can separate these with a delimiter to make them more readable to people and machines. For example, you could name a folder “coral_reef_data” which would be more readable than “coralreefdata”.\nYou may also want to use multiple delimiters to indicate different things. For instance, you could use underscores to differentiate categories and then use hyphens instead of spaces between words.\n\nNames should use “slugs” to connect inputs and outputs\n\nSlugs are human-readable, unique pieces of file names that are shared between files and the outputs that they create. Maybe a script is named “02_tidy.R” and all of the data files it creates are named “02_…”. Weird or unlikely outputs are easily traced to the scripts that created them because of their shared slug.\n\n\n\nOrganizing Example\nThese tips are all worthwhile but they can feel a little abstract without a set of files firmly in mind. Let’s consider an example synthesis project where we incrementally change the project structure to follow increasing more of the guidelines we suggest above.\n\nVersion 1Version 2Version 3Version 4\n\n\n\n\n\n\n\nPositives\n\nAll project files are in one folder\n\n\n\nAreas for Improvement\n\nNo use of sub-folders to divide logically-linked content\nFile names lack key context (e.g., workflow order, inputs vs. outputs, etc.)\nInconsistent use of delimiters\n\n\n\n\n\n\n\n\n\n\n\nPositives\n\nSub-folders used to divide content\nProject documentation included in top level (README and license files)\n\n\n\nAreas for Improvement\n\nFile names still inconsistent\n\nFile names contain different information in different order\nMixed use of delimiters\nMany file names include spaces\n\n\n\n\n\n\n\n\n\n\n\n\nPositives\n\nMost file names contain context\nStandardized use of casing and–within sub-folder–consistent delimiters used\n\n\n\nAreas for Improvement\n\nWorkflow order “guessable” but not explicit\nUnclear which files are inputs / outputs (and of which scripts)\n\n\n\n\n\n\n\n\n\n\n\nPositives\n\nScripts include zero-padded numbers indicating order of operations\nInputs / outputs share zero padded slug with source script\nReport file names machine sorted from least to most recent (top to bottom)\n\n\n\nAreas for Improvement\n\nDepending on sub-folder complexity, could add sub-folder specific README files\nGraph file names still include spaces\n\n\n\n\n\n\n\n\n\nOrganization Recommendations\nIf you integrate any of the concepts we’ve covered above you will find the reproducibility and transparency of your project will greatly increase. However, if you’d like additional recommendations we’ve assembled a non-exhaustive set of additional “best practices” that you may find helpful.\n\nNever Edit Raw Data\nFirst and foremost, it is critical that you never edit the raw data directly. If you do need to edit the raw data, use a script to make all needed edits and save the output of that script as a separate file. Editing the raw data directly without a script or using a script but overwriting the raw data are both incredibly risky operations because your create a file that “looks” like the raw data (and is likely documented as such) but differs from what others would have if they downloaded the ‘real’ raw data personally.\n\n\nSeparate Raw and Processed Data\nIn the same vein as the previous best practice, we recommend that you separate the raw and processed data into separate folders. This will make it easier to avoid accidental edits to the raw data and will make it clear what data are created by your project’s scripts; even if you choose not to adopt a file naming convention that would make this clear.\n\n\nQuarantine External Outputs\nThis can sound harsh, but it is often a good idea to “quarantine” outputs received from others until they can be carefully vetted. This is not at all to suggest that such contributions might be malicious! As you embrace more of the project organization recommendations we’ve described above outputs from others have more and more opportunities to diverge from the framework you establish. Quarantining inputs from others gives you a chance to rename files to be consistent with the rest of your project as well as make sure that the style and content of the code also match (e.g., use or exclusion of particular packages, comment frequency and content, etc.)",
    "crumbs": [
      "Phase I -- Prepare",
      "Reproducibility"
    ]
  },
  {
    "objectID": "mod_reproducibility.html#reproducible-coding",
    "href": "mod_reproducibility.html#reproducible-coding",
    "title": "Reproducibility Best Practices",
    "section": "Reproducible Coding",
    "text": "Reproducible Coding\nNow that you’ve organized your project in a reasonable way and documented those choices, we can move on to principles of reproducible coding! Doing your data operations with scripts is more reproducible than doing those operations without a programming language (i.e., with Microsoft Excel, Google Sheets, etc.). However, scripts are often written in a way that is not reproducible. A recent study aiming to run 2,000 project’s worth of R code found that 74% of the associated R files failed to complete without error (Trisovic et al. 2022). Many of those errors involve coding practices that hinder reproducibility but are easily preventable by the original code authors.\n\nWhen your scripts are clear and reproducibly-written you will reap the following benefits:\n\nReturning to your code after having set it down for weeks/months is much simpler\nCollaborating with team members requires less verbal explanation\nSharing methods for external result validation is more straightforward\nIn cases where you’re developing a novel method or workflow, structuring your code in this way will increase the odds that someone outside of your team will adopt your strategy\n\n\nPackages, Namespacing, and Software Versions\nAn under-appreciated facet of reproducible coding is a record of what code packages are used in a particular script and the version number of those packages. Packages evolve over time and code that worked when using one version of a given package may not work for future versions of that same package. Perpetually updating your code to work with the latest package versions is not sustainable but recording key information can help users set up the code environment that does work for your project.\n\nLoad Libraries Explicitly\nIt is important to load libraries at the start of every script. In some languages (like Python) this step is required but in others (like R) this step is technically “optional” but disastrous to skip. It is safe to skip including the installation step in your code because the library step should tell code-literate users which packages they need to install.\nFor instance you might begin each script with something like:\n# Load needed libraries\nlibrary(dplyr); library(magrittr); library(ggplot2)\n\n# Get to actual work\n. . .\nIn R the semicolon allows you to put multiple code operations in the same line of the script. Listing the needed libraries in this way thus lets everyone reading the code know exactly which packages they will need to have installed.\nIf you are feeling generous you could use the librarian R package to install packages that are not yet installed and simultaneously load all needed libraries. Note that users would still need to install librarian itself but this at least limits possible errors to one location. This is done like so:\n# Load `librarian` package\nlibrary(librarian)\n\n# Install missing packages and load needed libraries\nshelf(dplyr, magrittr, ggplot2)\n\n# Get to actual work\n. . .\n\n\nFunction Namespacing\nIt is also strongly recommended to “namespace” functions everywhere you use them. In R this is technically optional but it is a really good practice to adopt, particularly for functions that may appear in multiple packages with the same name but do very different operations depending on their source. In R the ‘namespacing operator’ is two colons.\n# Use the `mutate` function from the `dplyr` package\ndplyr::mutate(. . .)\nAn ancillary benefit of namespacing is that namespaced functions don’t need to have their respective libraries loaded. Still good practice to load the library though!\n\n\nPackage Versions\nWhile working on a project you should use the latest version of every needed package. However, as you prepare to publish or otherwise publicize your code, you’ll need to record package versions. R provides the sessionInfo function (from the utils package included in “base” R) which neatly summarizes some high level facets of your code environment. Note that for this method to work you’ll need to actually run the library-loading steps of your scripts.\nFor more in-depth records of package versions and environment preservation–in R–you might also consider the renv package or the packrat package.\n\n\n\nScript Organization\nEvery change to the data between the initial raw data and the finished product should be scripted. The ideal would be that you could hand someone your code and the starting data and have them be able to perfectly retrace your steps. This is not possible if you make unscripted modifications to the data at any point!\nYou may wish to break your scripted workflow into separate, modular files for ease of maintenance and/or revision. This is a good practice so long as each file fits clearly into a logical/thematic group (e.g., data cleaning versus analysis).\n\n\nFile Paths\nWhen importing inputs or exporting outputs we need to specify “file paths”. These are the set of folders between where your project is ‘looking’ and where the input/output should come from/go. The figure from Trisovic et al. (2022) shows that file path and working directory errors are a substantial barrier to code that can be re-run in clean coding environments. Consider the following ways of specifying file paths from least to most reproducible.\n\nWorstBadBetterBest!\n\n\n\nAbsolute Paths\nThe worst way of specifying a file path is to use the “absolute” file path. This is the path from the root of your computer to a given file. There are many issues here but the primary one is that absolute paths only work for one computer! Given that only one person can even run lines of code that use absolute paths, it’s not really worth specifying the other issues.\n\n\nExample\n# Read in bee community data\nmy_df &lt;- read.csv(file = \"~/Users/lyon/Documents/Grad School/Thesis (Chapter 1)/Data/bees.csv\")\n\n\n\n\nManually Setting the Working Directory\nMarginally better than using the absolute path is to set the working directory to some location. This may look neater than the absolute path option but it actually has the same point of failure: Both methods only work for one computer!\n\n\nExample\n# Set working directory\nsetwd(dir = \"~/Users/lyon/Documents/Grad School/Thesis (Chapter 1)\")\n\n# Read in bee community data\nmy_df &lt;- read.csv(file = \"Data/bees.csv\")\n\n\n\n\nRelative Paths\nInstead of using absolute paths or manually setting the working directory you can use “relative” file paths! Relative paths assume all project content lives in the same folder.\nThis is a safe assumption because it is the most fundamental tenet of reproducible project organization! The strength of relative paths is actually a serious contributing factor for why it is good practice to use a single folder.\n\n\nExample\n# Read in bee community data\n1my_df &lt;- read.csv(file = \"Data/bees.csv\")\n\n1\n\nParts of file path specific to each user are automatically recognized by the computer\n\n\n\n\n\n\nOperating System-Flexible Relative Paths\nThe “better” example is nice but has a serious limitation: it hard coded the type of slash between file path elements. This means that only computers of the same operating system as the code author could run that line.\nWe can use functions to automatically detect and insert the correct slashes though!\n\n\nExample\n# Read in bee community data\nmy_df &lt;- read.csv(file = file.path(\"Data\", \"bees.csv\"))\n\n\n\n\n\n\nCode Style\nWhen it comes to code style, the same ‘rule of thumb’ applies here that applied to project organization: virtually any system will work so long as you (and your team) are consistent! That said, there are a few principles worth adopting if you have not already done so.\n\nUse concise and descriptive object names\n\nIt can be difficult to balance these two imperatives but short object names are easier to re-type and visually track through a script. Descriptive object names on the other hand are useful because they help orient people reading the script to what the object contains.\n\nDon’t be afraid of empty space!\n\nScripts are free to write regardless of the number of lines so do not feel as though there is a strict character limit you need to keep in mind. Cramped code is difficult to read and thus can be challenging to share with others or debug on your own. Inserting an empty line between coding lines can help break up sections of code and putting spaces before and after operators can make reading single lines much simpler.\n\n\n\nCode Comments\nA “comment” in a script is a human readable, non-coding line that helps give context for the code. In R (and Python), comment lines start with a hashtag (#). Including comments is a low effort way of both (A) creating internal documentation for the script and (B) increasing the reproducibility of the script. It is difficult to include “too many” comments, so when in doubt: add more comments!\nThere are two major strategies for comments and either or both might make sense for your project.\n\n“What” Comments\nComments describe what the code is doing.\n\nBenefits: allows team members to understand workflow without code literacy\nRisks: rationale for code not explicit\n\n# Remove all pine trees from dataset\nno_pine_df &lt;- dplyr::filter(full_df, genus != \"Pinus\")\n\n\n“Why” Comments\nComments describe rationale and/or context for code.\n\nBenefits: built-in documentation for team decisions\nRisks: assumes everyone can read code\n\n# Cone-bearing plants are not comparable with other plants in dataset\nno_pine_df &lt;- dplyr::filter(full_df, genus != \"Pinus\")\n\n\n\n\n\n\nDiscussion: Comment on Comments\n\n\n\nWith a partner discuss the following questions:\n\nWhen you write comments, do you focus more on the “what” or the “why”?\nWhat would you estimate is the ratio of code to comment lines in your code?\n\n1:1 being every code line has one comment line\n\nIf you have revisited old code, were your comments helpful?\n\nHow could you make them more helpful?\n\nIn what ways do you think you would need to change your commenting style for a team project?\n\n\n\n\n\n\n\n\n\nActivity: Make Comments\n\n\n\nRevisit a script from an old project (ideally one you haven’t worked on recently). Once you’ve opened the script:\n\nScan through the script\n\nCan you identify the main purpose(s) of the code?\n\nIdentify any areas where you’re not sure either (A) what the code is doing or (B) why that section of code exists\n\nAdd comments to these areas to document what they’re up to\n\nShare the commented version of one of these trouble areas with a partner\n\nDo they understand the what and/or why of your code?\nIf not, revise the comments and repeat\n\n\n\n\n\n\n\nConsider Custom Functions\nIn most cases, duplicating code is not good practice. Such duplication risks introducing a typo in one copy but not the others. Additionally, if a decision is made later on that requires updating this section of code, you must remember to update each copy separately.\nInstead of taking this copy/paste approach, you could consider writing a “custom” function that fits your purposes. All instances where you would have copied the code now invoke this same function. Any error is easily tracked to the single copy of the function and changes to that step of the workflow can be accomplished in a centralized location.\n\nFunction Recommendations\nWe have the following ‘rules of thumb’ for custom function use:\n- If a given operation is duplicated 3 or more times within a project, write a custom function\nFunctions written in this case can be extremely specific and–though documentation is always a good idea–can be a little lighter on documentation. Note that the reason you can reduce the emphasis on documentation is only because of the assumption that you won’t be sharing the function widely. If you do decide the function could be widely valuable you would need to add the needed documentation post hoc.\n- Write functions defensively\nWhen you write custom functions, it is really valuable to take the time to write them defensively. In this context, “defensively” means that you anticipate likely errors and write your own informative/human readable error messages. Let’s consider a simplified version of a function from the ltertools R package for calculating the coefficient of variation (CV).\nThe coefficient of variation is equal to the standard deviation divided by the mean. Fortunately, R provides functions for calculating both of these already and both expect numeric vectors. If either of those functions is given a non-number you get the following warning message: “In mean.default(x =”…“) : argument is not numeric or logical: returning NA”.\nSomeone with experience in R may be able to interpret this error but for many users this error message is completely opaque. In the function included below however we can see that there is a simpler, more human readable version of the error message and the function is stopped before it can ever reach the part of the code that would throw the warning message included above.\ncv &lt;- function(x){\n  \n  # Error out if x is not numeric\n  if(is.numeric(x) != TRUE)\n    stop(\"`x` must be numeric\")\n  \n  # Calculate CV\n  sd(x = x) / mean(x = x)\nThe key to defensive programming is to try to get functions to fail fast and fail informatively as soon as a problem is detected! This is easier to debug and understand for coders with a range of coding expertise and–for complex functions–can save a ton of useless processing time when failure is guaranteed at a later step.\n- If a given operation is duplicated 3 or more times across projects, consider creating an R package\nCreating an R package can definitely seem like a daunting task but duplication across projects carries the same weaknesses of excessive duplication within a project. However, when duplication is across projects, not even writing a custom function saves you because you need to duplicate that function’s script for each project that needs the tool.\nHadley Wickham and Jenny Bryan have written a free digital book on this subject that demystifies a lot of this process and may make you feel more confident to create your own R package if/when one is needed.\nIf you do take this path, you can simply install your package as you would any other in order to have access to the operations rather than creating duplicates by hand.",
    "crumbs": [
      "Phase I -- Prepare",
      "Reproducibility"
    ]
  },
  {
    "objectID": "mod_reproducibility.html#fair-care-data-principles",
    "href": "mod_reproducibility.html#fair-care-data-principles",
    "title": "Reproducibility Best Practices",
    "section": "FAIR & CARE Data Principles",
    "text": "FAIR & CARE Data Principles\nData availability, data size, and demand for transparency by government and funding agencies are all steadily increasing. While ensuring that your project and code practices are reproducible is important, it is also important to consider how open and reproducible your data are as well. Synthesis projects are in a unique position here because synthesis projects use data that may have been previously published on and/or be added to a public data repository by the original data collectors. However, synthesis projects take data from these different sources and wrangle it such that the different data sources are comparable to one another. These ‘synthesis data products’ can be valuable to consider archiving in a public repository to save other researchers from needing to re-run your entire wrangling workflow in order to get the data product. In either primary or synthesis research contexts there are several valuable frameworks to consider as data structure and metadata are being decided. Among these are the FAIR and CARE data principles.\n\nFAIR\nFAIR is an acronym for Findable Accessible Interpoerable and Reusable. Each element of the FAIR principles can be broken into a set of concrete actions that you can take throughout the lifecycle of your project to ensure that your data are open and transparent. Perhaps most importantly, FAIR data are most easily used by other research teams in the future so the future impact of your work is–in some ways–dependent upon how thoroughly you consider these actions.\nConsider the following list of actions you might take to make your data FAIR. Note that not all actions may be appropriate for all types of data (see our discussion of the CARE principles below), but these guidelines are still important to consider–even if you ultimately choose to reject some of them. Virtually all of the guidelines considered below apply to metadata (i.e., the formal documentation describing your data) and the ‘actual’ data but for ease of reference we will call both of these resources “data.”\n\nFindability\n\nEnsure that data have a globally unique and persistent identifier\nCompletely fill out all metadata details\nRegister/index data in a searchable resource\n\nAccessibility\n\nStore data in a file format that is open, free, and universally implementable (e.g., CSV rather than MS Excel, etc.)\nEnsure that metadata will be available even if the data they describe are not\n\nInteroperability\n\nUse formal, shared, and broadly applicable language for knowledge representation\n\nThis can mean using full species names rather than codes or shorthand that may not be widely known\n\nUse vocabularies that are broadly used and that themselves follow FAIR principles\nInclude explicit references to other FAIR data\n\nReusability\n\nDescribe your data with rich detail that covers a plurality of relevant attributes\nAttach a clear data usage license so that secondary data users know how they are allowed to use your data\nInclude detailed provenance information about your data\nEnsure that your data meet discipline-specific community standards\n\n\n\n\n\n\n\nDiscussion: Consider Data FAIRness\n\n\n\nConsider the first data chapter of your thesis or dissertation. On a scale of 1-5, how FAIR do you think your data and metadata are? What actions could you take to make your data more FAIR compliant? If it helps, feel free to rate your (meta)data based on each FAIR criterion separately!\nFeel free to use these questions to guide your thinking\n\nHow are the data for that project stored?\nWhat metadata exists to document those data?\nHow easy would it be for someone in your lab group to pick up and use your data?\nHow easy would it be for someone not in your lab group?\n\n\n\n\n\nCARE\nWhile making data and code more FAIR is often a good ideal the philosophy behind those four criteria come from a perspective that emphasizes data sharing as a good in and of itself. This approach can ignore historical context and contemporary power differentials and thus be insufficient as the sole tool to use in evaluating how data/code are shared and stored. The Global Indigenous Data Alliance (GIDA) created the CARE principles with these ethical considerations explicitly built into their tenets. Before making your data widely available and transparent (ideally before even beginning your research), it is crucial to consider this ethical dimension.\n\nCARE stands for Collective Benefit, Authority to Control, Responsibility, and Ethics. Ensuring that your data meet these criteria helps to advance Indigenous data sovereignty and respects those who have been–and continue to be–collecting knowledge about the world around us for millennia. The following actions are adapted from Jennings et al. 2023 (linked at the bottom of this page).\nCollective Benefit\n\nDemonstrate how your research and its potential results are relevant and of value to the interests of the community at large and its individual members\nInclude and value local community experts in the research team\nUse classifications and categories in ways defined by Indigenous communities and individuals\nDisaggregate large geographic scale data to increase relevance for place-specific Indigenous priorities\nCompensate community experts throughout the research process (proposal development through to community review of pre-publication manuscripts)\n\nAuthority to Control\n\nEstablish institutional principles or protocols that explicitly recognize Indigenous Peoples’ rights to and interests in their knowledges/data\nEnsure data collection and distribution are consistent with individual and community consent provisions and that consent is ongoing (including the right to withdraw or refuse)\nEnsure Indigenous communities have access to the (meta)data in usable format\n\nResponsibility\n\nCreate and expand opportunities for community capacity\nRecord the Traditional Knowledge and biocultural labels of the Local Contexts Hub in metadata\nEnsure review of draft publications before publication\nUse the languages of Indigenous Peoples in the (meta)data\n\nEthics\n\nAccess research using Indigenous ethical frameworks\nUse community-defined review processes with appropriate reviewers for activities delineated in data management plans\nWork to maximize benefits from the perspectives of Indigenous Peoples by clear and transparent dialogue with communities and individuals\nEngage with community guidelines for the use and reuse of data (including facilitating data removal and/or disposal requests from aggregated datasets)",
    "crumbs": [
      "Phase I -- Prepare",
      "Reproducibility"
    ]
  },
  {
    "objectID": "mod_reproducibility.html#reproducibility-best-practices-summary",
    "href": "mod_reproducibility.html#reproducibility-best-practices-summary",
    "title": "Reproducibility Best Practices",
    "section": "Reproducibility Best Practices Summary",
    "text": "Reproducibility Best Practices Summary\nMaking sure that your project is reproducible requires a handful of steps before you begin, some actions during the life of the project, and then a few finishing touches when the project nears its conclusion. The following diagram may prove helpful as a coarse roadmap for how these steps might be followed in a general project setting.",
    "crumbs": [
      "Phase I -- Prepare",
      "Reproducibility"
    ]
  },
  {
    "objectID": "mod_reproducibility.html#additional-resources",
    "href": "mod_reproducibility.html#additional-resources",
    "title": "Reproducibility Best Practices",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\nA Large-Scale Study on Research Code Quality and Execution. Trisovic et al., 2022. Scientific Data\nApplying the ‘CARE Principles for Indigenous Data Governance’ to Ecology and Biodiversity Research. Jennings et al., 2023. Nature Ecology & Evolution\nGuides to Better Science - Reproducible Code. The British Ecological Society, 2024.\nFAIR Teaching Handbook. Englehardt et al., 2024.\nR Packages (2nd ed.). Wickham & Bryan.\n\n\n\nWorkshops & Courses\n\nData Analysis and Visualization in R for Ecologists, Episode 1: Before We Start. The Carpentries\nIntroduction to R for Geospatial Data, Episode 2: Project Management with RStudio. The Carpentries\ncoreR Course, Chapter 5: FAIR and CARE Principles. National Center for Ecological Analysis and Synthesis (NCEAS) Learning Hub, 2023.\ncoreR Course, Chapter 18: Reproducibility & Provenance. NCEAS Learning Hub, 2023.\n\n\n\nWebsites\n\nCoding Tips. NCEAS Scientific Computing Team, 2024.\nTeam Coding: 5 Essentials. NCEAS Scientific Computing Team, 2024.\nAdvanced R (1st ed.) Style Guide. Wickham\nPEP 8 Style Guide for Python Code. van Rossum et al. 2013.\nGoogle Style Guides",
    "crumbs": [
      "Phase I -- Prepare",
      "Reproducibility"
    ]
  },
  {
    "objectID": "mod_facilitation.html",
    "href": "mod_facilitation.html",
    "title": "Inclusive Facilitation",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase II -- Plan",
      "Inclusive Facilitation"
    ]
  },
  {
    "objectID": "mod_facilitation.html#overview",
    "href": "mod_facilitation.html#overview",
    "title": "Inclusive Facilitation",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase II -- Plan",
      "Inclusive Facilitation"
    ]
  },
  {
    "objectID": "mod_facilitation.html#learning-objectives",
    "href": "mod_facilitation.html#learning-objectives",
    "title": "Inclusive Facilitation",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nDescribe benefits of encouraging full, thoughtful, engaged participation\nIdentify methods for ensuring equitable access to participation in a team setting\nIdentify one activity that privileges each thinking style",
    "crumbs": [
      "Phase II -- Plan",
      "Inclusive Facilitation"
    ]
  },
  {
    "objectID": "mod_facilitation.html#module-content",
    "href": "mod_facilitation.html#module-content",
    "title": "Inclusive Facilitation",
    "section": "Module Content",
    "text": "Module Content",
    "crumbs": [
      "Phase II -- Plan",
      "Inclusive Facilitation"
    ]
  },
  {
    "objectID": "mod_facilitation.html#additional-resources",
    "href": "mod_facilitation.html#additional-resources",
    "title": "Inclusive Facilitation",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\nFacilitator’s Guide to Participatory Decision-Making by Sam Kaner\n\n\n\nWorkshops & Courses\n\n\n\n\n\nWebsites\n\nLiberating Structures website",
    "crumbs": [
      "Phase II -- Plan",
      "Inclusive Facilitation"
    ]
  },
  {
    "objectID": "mod_version-control.html",
    "href": "mod_version-control.html",
    "title": "Version Control",
    "section": "",
    "text": "Synthesis projects require an intensely collaborative team spirit complemented by an emphasis on quantitative rigor. While other modules focus on elements of each of these, version control is a useful tool that lives at their intersection. We will shortly embark on a deep dive into version control but broadly it is a method of tracking changes to files (especially code “scripts”) that lends itself remarkably well to coding in groups. The National Center for Ecological Analysis and Synthesis (NCEAS) has a Scientific Computing Team that developed a robust workshop-style set of materials aimed at teaching version control to LTER-funded synthesis working groups. Rather than reinvent these materials for the purposes of SSECR, we will work through parts of these workshop materials.",
    "crumbs": [
      "Phase I -- Prepare",
      "Version Control"
    ]
  },
  {
    "objectID": "mod_version-control.html#overview",
    "href": "mod_version-control.html#overview",
    "title": "Version Control",
    "section": "",
    "text": "Synthesis projects require an intensely collaborative team spirit complemented by an emphasis on quantitative rigor. While other modules focus on elements of each of these, version control is a useful tool that lives at their intersection. We will shortly embark on a deep dive into version control but broadly it is a method of tracking changes to files (especially code “scripts”) that lends itself remarkably well to coding in groups. The National Center for Ecological Analysis and Synthesis (NCEAS) has a Scientific Computing Team that developed a robust workshop-style set of materials aimed at teaching version control to LTER-funded synthesis working groups. Rather than reinvent these materials for the purposes of SSECR, we will work through parts of these workshop materials.",
    "crumbs": [
      "Phase I -- Prepare",
      "Version Control"
    ]
  },
  {
    "objectID": "mod_version-control.html#learning-objectives",
    "href": "mod_version-control.html#learning-objectives",
    "title": "Version Control",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nDescribe how version control facilitates code collaboration\nNavigate GitHub via a web browser\nCreate and edit a repository through GitHub\nDefine fundamental git vocabulary\nSketch the RStudio-to-GitHub order of operations\nUse RStudio, Git, and GitHub to collaborate with version control",
    "crumbs": [
      "Phase I -- Prepare",
      "Version Control"
    ]
  },
  {
    "objectID": "mod_version-control.html#module-content",
    "href": "mod_version-control.html#module-content",
    "title": "Version Control",
    "section": "Module Content",
    "text": "Module Content\nThe workshop materials we will be working through live here but for convenience we have also embedded the workshop directly into the SSECR course website (see below).",
    "crumbs": [
      "Phase I -- Prepare",
      "Version Control"
    ]
  },
  {
    "objectID": "mod_version-control.html#additional-resources",
    "href": "mod_version-control.html#additional-resources",
    "title": "Version Control",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\nNot Just for Programmers: How GitHub can Accelerate Collaborative and Reproducible Research in Ecology and Evolution. Pereira Braga et al., 2023. Methods in Ecology and Evolution\nGit Cheat Sheet. GitHub\n\n\n\nWorkshops & Courses\n\nHappy Git and GitHub for the useR. Bryan et al., 2024.\ncoreR Course, Chapter 6: Intro to Git and GitHub. NCEAS Learning Hub, 2023.",
    "crumbs": [
      "Phase I -- Prepare",
      "Version Control"
    ]
  },
  {
    "objectID": "policy_ai.html",
    "href": "policy_ai.html",
    "title": "Synthesis Skills for Early Career Researchers",
    "section": "",
    "text": "Artificial intelligence (AI) tools are increasingly well-known and widely discussed in the context of data science. AI products can increase the efficiency of code writing and are becoming a common part of the data science landscape. For the purposes of this course, we strongly recommend that you do not use AI tools. There is an under-discussed ethical consideration to the use and training of these tools in addition to their known practical limitations. However, the main reason we suggest you not use them for this class though is that leaning too heavily upon AI tools is likely to negatively impact your learning and skill acquisition.\nYou may have prior experience with some of the quantitative skills this course aims to teach but others are likely new to you. During the first steps of learning any new skill, it can be really helpful to struggle a bit in solving problems. Your efforts now will help refine your troubleshooting skills and will likely make it easier to remember how you solved a given problem the next time it arises. Over-use of AI tools can short circuit this pathway to mastery. Once you have become a proficient coder, you will be better able to identify and avoid any distortions or assumptions introduced by relying on AI.\nAI Resources\n\nPratim Ray, P. ChatGPT: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope. 2023. Internet of Things and Cyber-Physical Systems\nTrust, T. ChatGPT & Education Slide Deck. 2023. National Teaching Repository\nCsik, S. Teach Me How to Google. University of California, Santa Barbara (UCSB) Master of Environmental Data Science (MEDS) Program."
  },
  {
    "objectID": "mod_next-steps.html",
    "href": "mod_next-steps.html",
    "title": "Next Steps & Proposal Writing",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase IV -- Magnify",
      "Next Steps & Proposals"
    ]
  },
  {
    "objectID": "mod_next-steps.html#overview",
    "href": "mod_next-steps.html#overview",
    "title": "Next Steps & Proposal Writing",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase IV -- Magnify",
      "Next Steps & Proposals"
    ]
  },
  {
    "objectID": "mod_next-steps.html#learning-objectives",
    "href": "mod_next-steps.html#learning-objectives",
    "title": "Next Steps & Proposal Writing",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nIdentify needed data and target audience(s)\nArticulate connection(s) between proposed investigation and beneficial outcome\nWrite polished, funding-worthy proposals",
    "crumbs": [
      "Phase IV -- Magnify",
      "Next Steps & Proposals"
    ]
  },
  {
    "objectID": "mod_next-steps.html#module-content",
    "href": "mod_next-steps.html#module-content",
    "title": "Next Steps & Proposal Writing",
    "section": "Module Content",
    "text": "Module Content",
    "crumbs": [
      "Phase IV -- Magnify",
      "Next Steps & Proposals"
    ]
  },
  {
    "objectID": "mod_next-steps.html#additional-resources",
    "href": "mod_next-steps.html#additional-resources",
    "title": "Next Steps & Proposal Writing",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\nDeutsch et al., Leading Inter- and Transdisciplinary Research: Lessons from Applying Theories of Change to a Strategic Research Program. 2021. Environmental Science & Policy\n\n\n\nWorkshops & Courses\n\n\n\n\n\nWebsites",
    "crumbs": [
      "Phase IV -- Magnify",
      "Next Steps & Proposals"
    ]
  },
  {
    "objectID": "mod_data-viz.html",
    "href": "mod_data-viz.html",
    "title": "Data Visualization & Exploration",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase II -- Plan",
      "Data Visualization"
    ]
  },
  {
    "objectID": "mod_data-viz.html#overview",
    "href": "mod_data-viz.html#overview",
    "title": "Data Visualization & Exploration",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase II -- Plan",
      "Data Visualization"
    ]
  },
  {
    "objectID": "mod_data-viz.html#learning-objectives",
    "href": "mod_data-viz.html#learning-objectives",
    "title": "Data Visualization & Exploration",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nIdentify appropriate graph types for given data type/distribution\nExplain how visualization can be a tool for quality control\nUse data visualization as an exploratory tool\nDefine fundamental ggplot2 vocabulary\nCreate publication-quality graphs with ggplot2",
    "crumbs": [
      "Phase II -- Plan",
      "Data Visualization"
    ]
  },
  {
    "objectID": "mod_data-viz.html#needed-packages",
    "href": "mod_data-viz.html#needed-packages",
    "title": "Data Visualization & Exploration",
    "section": "Needed Packages",
    "text": "Needed Packages\nIf you’d like to follow along with the code chunks included throughout this module, you’ll need to install the following packages:\n\n# Note that these lines only need to be run once per computer\n## So you can skip this step if you've installed these before\ninstall.packages(\"ggplot2\")",
    "crumbs": [
      "Phase II -- Plan",
      "Data Visualization"
    ]
  },
  {
    "objectID": "mod_data-viz.html#module-content",
    "href": "mod_data-viz.html#module-content",
    "title": "Data Visualization & Exploration",
    "section": "Module Content",
    "text": "Module Content",
    "crumbs": [
      "Phase II -- Plan",
      "Data Visualization"
    ]
  },
  {
    "objectID": "mod_data-viz.html#additional-resources",
    "href": "mod_data-viz.html#additional-resources",
    "title": "Data Visualization & Exploration",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\n\n\n\n\nWorkshops & Courses\n\nNCEAS Scientific Computing team’s Coding in the Tidyverse workshop ggplot2 module\nThe Carpentries’ Data Analysis and Visualization in R for Ecologists ggplot2 episode\n\n\n\nWebsites",
    "crumbs": [
      "Phase II -- Plan",
      "Data Visualization"
    ]
  },
  {
    "objectID": "mod_findings.html",
    "href": "mod_findings.html",
    "title": "Communicating Findings",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase V -- Share",
      "Communicating Findings"
    ]
  },
  {
    "objectID": "mod_findings.html#overview",
    "href": "mod_findings.html#overview",
    "title": "Communicating Findings",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase V -- Share",
      "Communicating Findings"
    ]
  },
  {
    "objectID": "mod_findings.html#learning-objectives",
    "href": "mod_findings.html#learning-objectives",
    "title": "Communicating Findings",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nDefine elements of (in)effective communication\nIdentify relevant audiences for a particular work\nDetermine audience motivations and interest\nTranslate communication into various formats based on efficacy with target group",
    "crumbs": [
      "Phase V -- Share",
      "Communicating Findings"
    ]
  },
  {
    "objectID": "mod_findings.html#module-content",
    "href": "mod_findings.html#module-content",
    "title": "Communicating Findings",
    "section": "Module Content",
    "text": "Module Content",
    "crumbs": [
      "Phase V -- Share",
      "Communicating Findings"
    ]
  },
  {
    "objectID": "mod_findings.html#additional-resources",
    "href": "mod_findings.html#additional-resources",
    "title": "Communicating Findings",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\n\n\n\n\nWorkshops & Courses\n\nOpen Science Synthesis for the Delta Science Program’s Communicating Your Science\n\n\n\nWebsites\n\nCompass’ The Message Box",
    "crumbs": [
      "Phase V -- Share",
      "Communicating Findings"
    ]
  },
  {
    "objectID": "mod_interactivity.html",
    "href": "mod_interactivity.html",
    "title": "Creating Interactive Apps",
    "section": "",
    "text": "Shiny is a popular tool that allows users to build interactive web applications without the normally pre-requisite web development expertise. In addition to Shiny apps being simpler to build for the programmer they are often used to allow visitors to perform coding tasks without ever actually writing code. These are huge advantages because they reduce or eliminate significant technical barriers in developing truly interactive applications.\nIn synthesis contexts, Shiny can be used for a variety of valuable purposes. You can use it to develop dashboards for sharing data with related communities, allow your team to quickly “play with” exploratory graphs, or even to create a data submission portal (as is the case with some Research Coordination Networks or “RCNs”).\nNote that Shiny can be built in either R or Python ‘under the hood’ but for the purposes of this module we’ll focus on R.",
    "crumbs": [
      "Bonus Modules",
      "Interactivity"
    ]
  },
  {
    "objectID": "mod_interactivity.html#overview",
    "href": "mod_interactivity.html#overview",
    "title": "Creating Interactive Apps",
    "section": "",
    "text": "Shiny is a popular tool that allows users to build interactive web applications without the normally pre-requisite web development expertise. In addition to Shiny apps being simpler to build for the programmer they are often used to allow visitors to perform coding tasks without ever actually writing code. These are huge advantages because they reduce or eliminate significant technical barriers in developing truly interactive applications.\nIn synthesis contexts, Shiny can be used for a variety of valuable purposes. You can use it to develop dashboards for sharing data with related communities, allow your team to quickly “play with” exploratory graphs, or even to create a data submission portal (as is the case with some Research Coordination Networks or “RCNs”).\nNote that Shiny can be built in either R or Python ‘under the hood’ but for the purposes of this module we’ll focus on R.",
    "crumbs": [
      "Bonus Modules",
      "Interactivity"
    ]
  },
  {
    "objectID": "mod_interactivity.html#learning-objectives",
    "href": "mod_interactivity.html#learning-objectives",
    "title": "Creating Interactive Apps",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this topic you will be able to:\n\nDefine the three fundamental components of a Shiny app\nExplain benefits and limitations of interactive approaches to data exploration\nGenerate an interactive app with Shiny\nUse text formatting methods in a Shiny app\nExplore available Shiny layout options\nCreate a Shiny app\nDescribe (briefly) the purpose of deploying a Shiny app",
    "crumbs": [
      "Bonus Modules",
      "Interactivity"
    ]
  },
  {
    "objectID": "mod_interactivity.html#needed-packages",
    "href": "mod_interactivity.html#needed-packages",
    "title": "Creating Interactive Apps",
    "section": "Needed Packages",
    "text": "Needed Packages\nIf you’d like to follow along with the code chunks included throughout this module, you’ll need to install the following packages:\n\n# Note that these lines only need to be run once per computer\n## So you can skip this step if you've installed these before\ninstall.packages(\"tidyverse\")\ninstall.packages(\"shiny\")\ninstall.packages(\"htmltools\")\ninstall.packages(\"lterdatasampler\")\n\nWe’ll load the Tidyverse meta-package here to have access to many of its useful tools when we need them later as well as the shiny package.\n\n# Load needed libraries\nlibrary(tidyverse); library(shiny)",
    "crumbs": [
      "Bonus Modules",
      "Interactivity"
    ]
  },
  {
    "objectID": "mod_interactivity.html#shiny-fundamentals",
    "href": "mod_interactivity.html#shiny-fundamentals",
    "title": "Creating Interactive Apps",
    "section": "Shiny Fundamentals",
    "text": "Shiny Fundamentals\nAll Shiny apps are composed of three pieces: a user interface (UI), a server, and a call to the shinyApp function. The user interface includes everything that the user sees and can interact with; note that this includes both inputs and outputs. The server is responsible for all code operations performed on user inputs in order to generate outputs specified in the UI. The server is not available to the user. Finally, the shinyApp function simply binds together the UI and server and creates a living app. The app appears either in your RStudio or in a new tab on a web browser depending on your settings.\nFor those of you who write your own functions, you may notice that the syntax of Shiny is very similar to the syntax of functions. If you have not already, your quality of life will benefit greatly if you turn on “rainbow parentheses” in RStudio (Tools  Global Options  Code  Display  Check “Use rainbow parentheses” box).\nLet’s consider an artificially simple Shiny app so you can get a sense for the fundamental architecture of this tool.\n\n# Define the UI\n1basic_ui &lt;- shiny::fluidPage(\n  \"Hello there!\"\n)\n\n# Define the server\n2basic_server &lt;- function(input, output){ }\n\n# Generate the app\nshiny::shinyApp(ui = basic_ui, server = basic_server)\n\n\n1\n\nThe fluidPage function is important for leaving flexibility in UI layout which we’ll explore later in the module\n\n2\n\nBecause this app has no inputs or outputs, it doesn’t need anything in the ‘server’ component (though it still does require an empty server!)\n\n\n\n\nIf you copy and run the above code, you should see an app that is a blank white page with “Hello there!” written in the top left in plain text. Congratulations, you have now made your first Shiny app! Now, your reason for exploring this module likely involves an app that actually does something but the fundamental structure of all apps–even skeletal apps like this one–is the same. More complicated apps will certainly have more content in the UI and server sections but all Shiny apps will have this tripartite structure.",
    "crumbs": [
      "Bonus Modules",
      "Interactivity"
    ]
  },
  {
    "objectID": "mod_interactivity.html#interactive-apps",
    "href": "mod_interactivity.html#interactive-apps",
    "title": "Creating Interactive Apps",
    "section": "Interactive Apps",
    "text": "Interactive Apps\nNow that we’ve covered non-reactive apps, let’s create an interactive one! It is important to remember that the user interface needs to contain both the inputs the user can make and the outputs determined by those inputs. The server will be responsible for turning the inputs into outputs but if you want your interactive app to actually show the user the interactivity you need to be careful to include the outputs in the UI.\nEssentially all Shiny UI functions use the same syntax of &lt;value class&gt;Input or &lt;value class&gt;Output. So, determining how you want the user to engage with your app is sometimes as straightforward as identifying the class of the value you want them to interact with. Shiny calls these helper functions “widgets”.\nLet’s consider an app that accepts a single number and returns the square root of that number.\n\n# Define the UI ---- \nreactive_ui &lt;- shiny::fluidPage(\n  \n  # Create input\n1  shiny::numericInput(inputId = \"num_in\",\n                      label = \"Type a number\",\n                      value = 16),\n  \n  # Include some plain text for contextualizing the output\n2  \"Square root is: \",\n  \n  # Create output\n  shiny::textOutput(outputId = \"num_out\")\n  \n) # Close UI\n\n# Define server ----\nreactive_server &lt;- function(input, output){\n  \n  # Reactively accept the input and take the square root of it\n3  root &lt;- shiny::reactive({\n4    sqrt(x = input$num_in)\n  })\n  \n  # Make that value an output of the server/app\n5  output$num_out &lt;- shiny::renderText(\n6    expr = root()\n  ) \n  \n} # Close server\n\n# Generate the app ----\nshiny::shinyApp(ui = reactive_ui, server = reactive_server)\n\n\n1\n\nNote that the argument name is capital “I” but lowercase “d”. Typing inputID is a common and frustrating source of error for Shiny app developers\n\n2\n\nEvery element of the UI–except the last one–needs to end with a comma\n\n3\n\nAll reactive elements (i.e., those that change as soon as the user changes an input) need to be specified inside of reactive with both parentheses and curly braces\n\n4\n\nThe name of this input exactly matches the inputId we defined in the UI. That it is an input is defined by our use of the numericInput widget\n\n5\n\nThe name of this output exactly matches the outputId we told the UI to expect.\n\n6\n\nReactive elements essentially become functions in their own right! So, when we want to use them, we need to include empty parentheses next to their name\n\n\n\n\nWe included a lot of footnote annotations in that code chunk to help provide context but there are a few small comments that are worthwhile to bring up at this stage.\n\nUI outputs and server renders must match\n\nThe widget you use in the UI to return an output must correspond to the function used in the server to generate that output. In this example, we use textOutput in the UI so in the server we use renderText. Essentially all widgets in Shiny use this &lt;class&gt;Output versus render&lt;Class&gt; syntax which can be a big help to visual checks that your app is written correctly. You will need to be sure that whatever the ‘class’ is, it is lowercase in the UI but title case in the server (i.e., only first letter capitalized).\n\nUse section header format\n\nThis app is relatively short but we think effectively hints at how long and convoluted purpose-built Shiny apps can easily become. So, we recommend using section headers in your Shiny app code. You can do this by putting either four hyphens or four hashtags at the end of a comment line (e.g., # Section 1 #### or # My header ----). Headings defined in this way will appear in the bottom left of the “Source” pane of RStudio next to a light orange hashtag symbol. Clicking the text in that area will open a drop-down menu showing all headings in your current file and clicking one of the other headings will instantly jump you to that heading. This can be incredibly convenient when you’re trying to navigate a several hundred line long Shiny app. While rainbow parentheses can be useful for avoiding typos within a section, section headers make it much easier to avoid typos across sections.\nIf you don’t use headings already (or your cursor is on a line before the first heading), the relevant bit of the “Source” pane will just say “(Top Level)” and will not have the golden hashtag symbol.",
    "crumbs": [
      "Bonus Modules",
      "Interactivity"
    ]
  },
  {
    "objectID": "mod_interactivity.html#including-data",
    "href": "mod_interactivity.html#including-data",
    "title": "Creating Interactive Apps",
    "section": "Including Data",
    "text": "Including Data\nYou can also use your Shiny app to work with a full data table! When running your app locally, you only need to read in the data as you normally would then run the app. By having read in the data you will ensure the object is in your environment and accessible to the app. However, keep in mind this will only work in “local” (i.e., non-deployed) contexts. See our–admittedly brief–discussion of deployment at the end of this module.\nLet’s explore an example using data about fiddler crabs (Minuca pugnax) at the Plum Island Ecosystems (PIE) LTER site from the lterdatasampler R package. The app we’re about to create will make a graph between any two (numeric) columns.\n\n# Load lterdatasampler package\nlibrary(lterdatasampler)\n\n# Load fiddler crab data\n1data(pie_crab)\n\n# Define the UI ---- \ndata_ui &lt;- shiny::fluidPage(\n  \n  # Let the user choose which the X axis\n  shiny::selectInput(inputId = \"x_vals\",\n              label = \"Choose the X-axis\",\n2              choices = setdiff(x = names(pie_crab),\n                                y = c(\"date\", \"site\", \"name\")),\n              selected = \"latitude\"),\n  \n  # Also the Y axis\n  shiny::selectInput(inputId = \"y_vals\",\n              label = \"Choose the Y-axis\",\n              choices = setdiff(x = names(pie_crab),\n                                y = c(\"date\", \"site\", \"name\")),\n              selected = \"size\"),\n  \n  # Return the desired plot\n  shiny::plotOutput(outputId = \"crab_graph\")\n              \n) # Close UI\n\n# Define the server ----\ndata_server &lt;- function(input, output){\n  \n  # Reactively identify X & Y axess\n3  picked_x &lt;- shiny::reactive({ input$x_vals })\n  picked_y &lt;- shiny::reactive({ input$y_vals })\n  \n  # Create the desired graph\n  output$crab_graph &lt;- shiny::renderPlot(\n    \n4    ggplot(pie_crab, aes(x = .data[[picked_x()]], y = .data[[picked_y()]])) +\n      geom_point(aes(fill = .data[[picked_x()]]), pch = 21, size = 2.5) +\n      labs(x = stringr::str_to_title(picked_x()),\n           y = stringr::str_to_title(picked_y())) +\n      theme_bw()\n    \n  ) # Close plot rendering\n  \n} # Close server\n\n# Generate the app ----\nshiny::shinyApp(ui = data_ui, server = data_server)\n\n\n1\n\nNote the loading of the data is done outside of the app! You can have the app load its own data but that is more complicated than this example needs to be.\n\n2\n\nTo make our life easier in the server we can exclude non-number columns\n\n3\n\nSee how we’re reactively grabbing both axes?\n\n4\n\nggplot2 requires special syntax to specify axes with quoted column names (which is how reactive Shiny elements from that widget are returned)",
    "crumbs": [
      "Bonus Modules",
      "Interactivity"
    ]
  },
  {
    "objectID": "mod_interactivity.html#layouts",
    "href": "mod_interactivity.html#layouts",
    "title": "Creating Interactive Apps",
    "section": "Layouts",
    "text": "Layouts\nExperimenting with different app layouts can be a fun step in the process of making an app that is as effective as possible! We do recommend that during app development you stick with a very simple user interface because it’ll be easier to make sure your inputs and outputs work as desired. Once you are satisfied with those elements you can relatively easily chengs the UI to help guide users through your app.\nAs implied by that preface, layouts are exclusively an element of the user interface! This is great when you have an app with a complicated server component because you won’t need to mess with that at all to get the UI looking perfect. In the examples below, we’ll generate a non-interactive app so that we can really emphasize the ‘how to’ perspective of using different layouts.\n\nSidebar\nOne of the more common Shiny UI choices is to use a sidebar. The sidebar typically takes up about one third of the width of the app while the remaining two thirds is taken up by the main panel. The sidebar can be nice place to put all the user inputs and have the outputs display in the main panel. This format allows for really clear visual separation between where you want the user to interact with the app versus where the results of their choices can be viewed.\n\n# Define the UI\nsidebar_ui &lt;- shiny::fluidPage(\n  \n  # Define the layout type\n1  shiny::sidebarLayout(\n  \n    # Define what goes in the eponymous sidebar\n    shiny::sidebarPanel(\n      \"Hello from the sidebar!\"\n2      ),\n    \n    # Define what goes in the main panel\n    shiny::mainPanel(\n      \"Hello from the main panel!\"\n3      ) ) )\n\n# Define the server\nbasic_server &lt;- function(input, output){ }\n\n# Generate the app\nshiny::shinyApp(ui = sidebar_ui, server = basic_server)\n\n\n1\n\nNotice that everything else in the UI is wrapped inside this function. If you want something above/below the sidebar vs. main panel you’ll need to put that content outside of this function’s parentheses but still in the fluidPage parentheses\n\n2\n\nBe careful not to forget this comma separating the sidebarPanel and mainPanel functions!\n\n3\n\nThree closing parentheses are needed to close the UI elements. This is why it’s really helpful to use rainbow parentheses in your coding environment!\n\n\n\n\n\n\nTab Panels\nIf you feel that your app is better represented in separate pages, tab panels may be a better layout choice! The result of this layout is a series of discrete tabs along the top of your app. If the user clicks one of them they’ll be able to look at a separate chunk of your app. Inputs in any tab are available to the app’s server and can be outputs in any tab (remember that their is a shared server so it is impossible for it to be otherwise!). Generally it may be a good idea to have inputs and outputs in the same tab so that users can see the interactive app responding to their inputs rather than needing to click back and forth among tabs to see the results of their inputs. For example, you could have an app where users choose what goes on either axis of several graph types and put each graph type on its own tab of the larger Shiny app.\n\n# Define the UI\ntabs_ui &lt;- shiny::fluidPage(\n  \n# Define the layout type\n1  shiny::tabsetPanel(\n  \n    # Define what goes in the first tab\n    shiny::tabPanel(title = \"Tab 1\",\n                    \"Hello from the first tab!\"\n2                    ),\n    \n    # And in the second\n    shiny::tabPanel(title = \"Tab 2\",\n                    \"Welcome to the second tab!\"\n                    ),\n    \n    # And so on\n    shiny::tabPanel(title = \"Tab 3\",\n                    \"Hello yet again!\"\n3                    ) ) )\n\n# Define the server\nbasic_server &lt;- function(input, output){ }\n\n# Generate the app\nshiny::shinyApp(ui = tabs_ui, server = basic_server)\n\n\n1\n\nThis function is comparable to sidebarLayout in that if you want stuff above/below the tab panel area you’ll need to be outside of this function’s parentheses but still in the fluidPage parentheses\n\n2\n\nAgain, just like the sidebarLayout subfunctions, you’ll need a comma after each UI element except the last one\n\n3\n\nHere we’re closing all of the nested UI functions\n\n\n\n\n\n\nOther Layouts\nWe just briefly covered two layout options but hopefully this is a nice indication for the kind of flexibility in user interface that you can expect of Shiny apps! For more information, check out Posit’s Shiny Application Layout Guide. That resource has some really nice examples of these and other layout options that will be well worth checking out as you begin your journey into Shiny.",
    "crumbs": [
      "Bonus Modules",
      "Interactivity"
    ]
  },
  {
    "objectID": "mod_interactivity.html#text-formatting",
    "href": "mod_interactivity.html#text-formatting",
    "title": "Creating Interactive Apps",
    "section": "Text Formatting",
    "text": "Text Formatting\nBeyond making your app have an intuitive layout it can be really helpful to be able to do even simple text formatting to assist your app’s users. For instance, you may want to use sub-headings within the same UI layout component but still want to draw a distinction between two sets of inputs. Additionally you may want to emphasize some tips for best results or hyperlink to your group’s other products. All of these can be accomplished using text formatting tools that are readily available within Shiny.\n\n# Load the `htmltools` library\nlibrary(htmltools)\n\n# Define the UI\ntext_ui &lt;- shiny::fluidPage(\n  \n  # Let's make some headings\n1  htmltools::h1(\"This is a Big Heading\"),\n  htmltools::h3(\"Smaller heading\"),\n  htmltools::h5(\"Even smaller heading!\"), \n  \n  # Now we'll format more text in various (non-heading) ways\n  htmltools::strong(\"Bold text\"),\n  \n2  htmltools::br(),\n\n  htmltools::a(href = \"https://lter.github.io/ssecr/mod_interactivity.html\",\n               \"This text is hyperlinked\",\n3               target = \"_blank\"),\n  \n  htmltools::br(),\n  \n4  htmltools::code(\"This is 'code' text\") )\n\n# Define the server\nbasic_server &lt;- function(input, output){ }\n\n# Generate the app\nshiny::shinyApp(ui = text_ui, server = basic_server)\n\n\n1\n\nHeadings (of any size) automatically include a line break after the heading text\n\n2\n\nThe br function creates a line break\n\n3\n\nWhen the target argument is set to “_blank” it will open a new tab when users click the hyperlinked text. This is ideal because if a user left your app to visit the new site they would lose all of their inputs\n\n4\n\nCode text looks like this",
    "crumbs": [
      "Bonus Modules",
      "Interactivity"
    ]
  },
  {
    "objectID": "mod_interactivity.html#deployment",
    "href": "mod_interactivity.html#deployment",
    "title": "Creating Interactive Apps",
    "section": "Deployment",
    "text": "Deployment\nWhen Shiny apps are only being used by those in your team, keeping them as a code script works well. However, if you’d like those outside of your team to be able to find your app as they would any other website you’ll need to deploy your Shiny app. This process is outside of the scope of this module but is often the end goal of Shiny app development.\nTake a look at Posit’s instructions for deployment for more details but essentially “deployment” is the process of getting your local app hosted on shinyapps.io which gives it a link that anyone can use to access/run your app on their web browser of choice.",
    "crumbs": [
      "Bonus Modules",
      "Interactivity"
    ]
  },
  {
    "objectID": "mod_interactivity.html#additional-interactivity-resources",
    "href": "mod_interactivity.html#additional-interactivity-resources",
    "title": "Creating Interactive Apps",
    "section": "Additional Interactivity Resources",
    "text": "Additional Interactivity Resources\n\nPapers & Documents\n\n\n\n\n\nWorkshops & Courses\n\nPosit’s Welcome to Shiny (for R coders)\n2022 All Scientists’ Meeting Shiny Apps for Sharing Science workshop\n\n\n\nWebsites\n\nPosit’s Shiny website",
    "crumbs": [
      "Bonus Modules",
      "Interactivity"
    ]
  },
  {
    "objectID": "mod_credit.html",
    "href": "mod_credit.html",
    "title": "Authorship & Intellectual Credit",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase III -- Execute",
      "Intellectual Credit"
    ]
  },
  {
    "objectID": "mod_credit.html#overview",
    "href": "mod_credit.html#overview",
    "title": "Authorship & Intellectual Credit",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase III -- Execute",
      "Intellectual Credit"
    ]
  },
  {
    "objectID": "mod_credit.html#learning-objectives",
    "href": "mod_credit.html#learning-objectives",
    "title": "Authorship & Intellectual Credit",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nDefine types of intellectual contributions to a synthesis project\nDescribe some common frameworks for equitable authorship decisions\nExplain benefits (or avoided costs) of making authorship decisions both collaboratively and transparently\nCreate a draft intellectual credit plan for your team",
    "crumbs": [
      "Phase III -- Execute",
      "Intellectual Credit"
    ]
  },
  {
    "objectID": "mod_credit.html#module-content",
    "href": "mod_credit.html#module-content",
    "title": "Authorship & Intellectual Credit",
    "section": "Module Content",
    "text": "Module Content",
    "crumbs": [
      "Phase III -- Execute",
      "Intellectual Credit"
    ]
  },
  {
    "objectID": "mod_credit.html#additional-resources",
    "href": "mod_credit.html#additional-resources",
    "title": "Authorship & Intellectual Credit",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\nDahlin et al., Hear Every Voice, Working Groups that Work. 2019. Frontiers in Ecology and the Environment\nAllen et al., How Can We Ensure Visibility and Diversity in Research Contributions? How the Contributor Role Taxonomy (CReDiT) is Helping the Shift from Authorship to Contributorship. 2018. Learned Publishing\n\n\n\nWorkshops & Courses\n\n\n\n\n\nWebsites\n\nNutrient Network (NutNet) authorship policy\nHerbivory Variability Network (HerbVar) participation guidelines",
    "crumbs": [
      "Phase III -- Execute",
      "Intellectual Credit"
    ]
  },
  {
    "objectID": "mod_wrangle.html",
    "href": "mod_wrangle.html",
    "title": "Data Harmonization & Wrangling",
    "section": "",
    "text": "Now that we have covered how to find data and use data visualization methods to explore it, we can move on to combining separate data files and preparing that combined data file for analysis. For the purposes of this module we’re adopting a very narrow view of harmonization and a very broad view of wrangling but this distinction aligns well with two discrete philosophical/practical arenas. To make those definitions explicit:\n\n“Harmonization” = process of combining separate primary data objects into one object. This includes things like synonymizing columns, or changing data format to support combination. This excludes quality control steps–even those that are undertaken before harmonization begins.\n“Wrangling” = all modifications to data meant to create an analysis-ready ‘tidy’ data object. This includes quality control, unit conversions, and data ‘shape’ changes to name a few. Note that attaching ancillary data to your primary data object (e.g., attaching temperature data to a dataset on plant species composition) also falls into this category!",
    "crumbs": [
      "Phase III -- Execute",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "mod_wrangle.html#overview",
    "href": "mod_wrangle.html#overview",
    "title": "Data Harmonization & Wrangling",
    "section": "",
    "text": "Now that we have covered how to find data and use data visualization methods to explore it, we can move on to combining separate data files and preparing that combined data file for analysis. For the purposes of this module we’re adopting a very narrow view of harmonization and a very broad view of wrangling but this distinction aligns well with two discrete philosophical/practical arenas. To make those definitions explicit:\n\n“Harmonization” = process of combining separate primary data objects into one object. This includes things like synonymizing columns, or changing data format to support combination. This excludes quality control steps–even those that are undertaken before harmonization begins.\n“Wrangling” = all modifications to data meant to create an analysis-ready ‘tidy’ data object. This includes quality control, unit conversions, and data ‘shape’ changes to name a few. Note that attaching ancillary data to your primary data object (e.g., attaching temperature data to a dataset on plant species composition) also falls into this category!",
    "crumbs": [
      "Phase III -- Execute",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "mod_wrangle.html#learning-objectives",
    "href": "mod_wrangle.html#learning-objectives",
    "title": "Data Harmonization & Wrangling",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nIdentify typical steps in data harmonization and wrangling workflows\nCreate a harmonization workflow\nDefine quality control\nSummarize typical operations in a quality control workflow\nUse regular expressions to perform flexible text operations\nWrite custom functions to reduce code duplication\nIdentify value of and typical obstacles to data ‘joining’\nExplain benefits and drawbacks of using data shape to streamline code\nDesign a complete data wrangling workflow",
    "crumbs": [
      "Phase III -- Execute",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "mod_wrangle.html#needed-packages",
    "href": "mod_wrangle.html#needed-packages",
    "title": "Data Harmonization & Wrangling",
    "section": "Needed Packages",
    "text": "Needed Packages\nIf you’d like to follow along with the code chunks included throughout this module, you’ll need to install the following packages:\n\n# Note that these lines only need to be run once per computer\n## So you can skip this step if you've installed these before\ninstall.packages(\"ltertools\")\ninstall.packages(\"lterdatasampler\")\ninstall.packages(\"psych\")\ninstall.packages(\"supportR\")\ninstall.packages(\"tidyverse\")\n\nWe’ll load the Tidyverse meta-package here to have access to many of its useful tools when we need them later.\n\n# Load tidyverse\nlibrary(tidyverse)",
    "crumbs": [
      "Phase III -- Execute",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "mod_wrangle.html#harmonizing-data",
    "href": "mod_wrangle.html#harmonizing-data",
    "title": "Data Harmonization & Wrangling",
    "section": "Harmonizing Data",
    "text": "Harmonizing Data\nData harmonization is an interesting topic in that it is vital for synthesis projects but only very rarely relevant for primary research. Synthesis projects must reckon with the data choices made by each team of original data collectors. These collectors may or may not have recorded their judgement calls (or indeed, any metadata) but before synthesis work can be meaningfully done these independent datasets must be made comparable to one another and combined.\nFor tabular data, we recommend using the ltertools R package to perform any needed harmonization. This package relies on a “column key” to translate the original column names into equivalents that apply across all datasets. Users can generate this column key however they would like but Google Sheets is a strong option as it allows multiple synthesis team members to simultaneously work on filling in the needed bits of the key. If you already have a set of files locally, ltertools does offer a begin_key function that creates the first two required columns in the column key.\nThe column key requires three columns:\n\n“source” – Name of the raw file\n“raw_name” – Name of all raw columns in that file to be synonymized\n“tidy_name” – New name for each raw column that should be carried to the harmonized data\n\nNote that any raw names either not included in the column key or that lack a tidy name equivalent will be excluded from the final data object. For more information, consult the ltertools package vignette. For convenience, we’re attaching the visual diagram of this method of harmonization from the package vignette.",
    "crumbs": [
      "Phase III -- Execute",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "mod_wrangle.html#wrangling-data",
    "href": "mod_wrangle.html#wrangling-data",
    "title": "Data Harmonization & Wrangling",
    "section": "Wrangling Data",
    "text": "Wrangling Data\nData wrangling is a huge subject that covers a wide range of topics. In this part of the module, we’ll attempt to touch on a wide range of tools that may prove valuable to your data wrangling efforts. This is certainly non-exhaustive and you’ll likely find new tools that fit your coding style and professional intuition better. However, hopefully the topics covered below provide a nice ‘jumping off’ point to reproducibly prepare your data for analysis and visualization work later in the lifecycle of the project.\nThis module will use example data to demonstrate these tools but as we work through these topics you should feel free to substitute a dataset of your choosing! If you don’t have one in mind, you can use the example dataset shown in the code chunks throughout this module.\nThis dataset comes from the lterdatasampler R package and the data are about fiddler crabs (Minuca pugnax) at the Plum Island Ecosystems (PIE) LTER site.\n\n# Load the lterdatasampler package\nlibrary(lterdatasampler)\n\n# Load the fiddler crab dataset\ndata(pie_crab)\n\n\nExploring the Data\nBefore beginning any code operations, it’s important to get a sense for the data. Characteristics like the dimensions of the dataset, the column names, and the type of information stored in each column are all crucial pre-requisites to knowing what tools can or should be used on the data.\nChecking the data structure is one way of getting a lot of this high-level information.\n\n# Check dataset structure\nstr(pie_crab)\n\ntibble [392 × 9] (S3: tbl_df/tbl/data.frame)\n $ date         : Date[1:392], format: \"2016-07-24\" \"2016-07-24\" ...\n $ latitude     : num [1:392] 30 30 30 30 30 30 30 30 30 30 ...\n $ site         : chr [1:392] \"GTM\" \"GTM\" \"GTM\" \"GTM\" ...\n $ size         : num [1:392] 12.4 14.2 14.5 12.9 12.4 ...\n $ air_temp     : num [1:392] 21.8 21.8 21.8 21.8 21.8 ...\n $ air_temp_sd  : num [1:392] 6.39 6.39 6.39 6.39 6.39 ...\n $ water_temp   : num [1:392] 24.5 24.5 24.5 24.5 24.5 ...\n $ water_temp_sd: num [1:392] 6.12 6.12 6.12 6.12 6.12 ...\n $ name         : chr [1:392] \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" ...\n\n\nFor data that are primarily numeric, you may find data summary functions to be valuable. Note that most functions of this type do not provide useful information on text columns so you’ll need to find that information elsewhere.\n\n# Get a simple summary of the data\nsummary(pie_crab)\n\n      date               latitude         site                size      \n Min.   :2016-07-24   Min.   :30.00   Length:392         Min.   : 6.64  \n 1st Qu.:2016-07-28   1st Qu.:34.00   Class :character   1st Qu.:12.02  \n Median :2016-08-01   Median :39.10   Mode  :character   Median :14.44  \n Mean   :2016-08-02   Mean   :37.69                      Mean   :14.66  \n 3rd Qu.:2016-08-09   3rd Qu.:41.60                      3rd Qu.:17.34  \n Max.   :2016-08-13   Max.   :42.70                      Max.   :23.43  \n    air_temp      air_temp_sd      water_temp    water_temp_sd  \n Min.   :10.29   Min.   :6.391   Min.   :13.98   Min.   :4.838  \n 1st Qu.:12.05   1st Qu.:8.110   1st Qu.:14.33   1st Qu.:6.567  \n Median :13.93   Median :8.410   Median :17.50   Median :6.998  \n Mean   :15.20   Mean   :8.654   Mean   :17.65   Mean   :7.252  \n 3rd Qu.:18.63   3rd Qu.:9.483   3rd Qu.:20.54   3rd Qu.:7.865  \n Max.   :21.79   Max.   :9.965   Max.   :24.50   Max.   :9.121  \n     name          \n Length:392        \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n\nFor text columns it can sometimes be useful to simply look at the unique entries in a given column and sort them alphabetically for ease of parsing.\n\n# Look at the sites included in the data\nsort(unique(pie_crab$site))\n\n [1] \"BC\"  \"CC\"  \"CT\"  \"DB\"  \"GTM\" \"JC\"  \"NB\"  \"NIB\" \"PIE\" \"RC\"  \"SI\"  \"VCR\"\n[13] \"ZI\" \n\n\nFor those of you who think more visually, a histogram can be a nice way of examining numeric data. There are simple histogram functions in the ‘base’ packages of most programming languages but it can sometimes be worth it to use those from special libraries because they can often convey additional detail.\n\n# Load the psych library\nlibrary(psych)\n\n# Get the histogram of crab \"size\" (carapace width in mm)\npsych::multi.hist(pie_crab$size)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussion: Data Exploration Tools\n\n\n\nWith a group of 4-5 others discuss the following questions:\n\nWhat tools do you use when exploring new data?\nDo you already use any of these tools to explore your data?\n\nIf you do, why do you use them?\nIf not, where do you think they might be valuable to include?\n\nWhat value–if any–do you see in including these exploratory efforts in your code workflow?\n\n\n\n\n\nQuality Control\nYou may have encountered the phrase “QA/QC” (Quality Assurance / Quality Control) in relation to data cleaning. Technically, quality assurance only encapsulates preventative measures for reducing errors. One example of QA would be using a template for field datasheets because using standard fields reduces the risk that data are recorded inconsistently and/or incompletely. Quality control on the other hand refers to all steps taken to resolve errors after data are collected. Any code that you write to fix typos or remove outliers from a dataset falls under the umbrella of QC.\nIn synthesis work, QA is only very rarely an option. You’ll be working with datasets that have already been collected and attempting to handle any issues post hoc which means the vast majority of data wrangling operations will be quality control methods. These QC efforts can be incredibly time-consuming so using a programming language (like  R or  Python) is a dramatic improvement over manually looking through the data using Microsoft Excel or other programs like it.\n\nNumber Checking\nWhen you read in a dataset and a column that should be numeric is instead read in as a character, it can be a sign that there are malformed numbers lurking in the background. Checking for and resolving these non-numbers is preferable to simply coercing the column into being numeric because the latter method typically changes those values to ‘NA’ where a human might be able to deduce the true number each value ‘should be.’\n\n# Load the supportR package\nlibrary(supportR)\n\n# Create an example dataset with non-numbers in ideally numeric columns\nfish_ct &lt;- data.frame(\"species\" = c(\"salmon\", \"bass\", \"halibut\", \"moray eel\"),\n                      \"count\" = c(12, \"14x\", \"_23\", 1))\n\n# Check for malformed numbers in column(s) that should be numeric\nbad_nums &lt;- supportR::num_check(data = fish_ct, col = \"count\")\n\nFor 'count', 2 non-numbers identified: '14x' | '_23'\n\n\nIn the above example, “14x” would be coerced to NA if you simply force the column without checking but you could drop the “x” with text replacing methods once you use tools like this one to flag it for your attention.\n\n\nText Replacement\nOne of the simpler ways of handling text issues is just to replace a string with another string. Most programming languages support this functionality.\n\n# Use pattern match/replace to simplify problem entries\nfish_ct$count &lt;- gsub(pattern = \"x|_\", replacement = \"\", x = fish_ct$count)\n\n# Check that they are fixed\nbad_nums &lt;- supportR::num_check(data = fish_ct, col = \"count\")\n\nFor 'count', no non-numeric values identified.\n\n\nThe vertical line in the gsub example above lets us search for (and replace) multiple patterns. Note however that while you can search for many patterns at once, only a single replacement value can be provided with this function.\n\n\nRegular Expressions\nYou may sometimes want to perform more generic string matching where you don’t necessarily know–or want to list–all possible strings to find and replace. For instance, you may want remove any letter in a numeric column or find and replace numbers with some sort of text note. “Regular expressions” are how programmers specify these generic matches and using them can be a nice way of streamlining code.\n\n# Make a test vector\nregex_vec &lt;- c(\"hello\", \"123\", \"goodbye\", \"456\")\n\n# Find all numbers and replace with the letter X\ngsub(pattern = \"[[:digit:]]\", replacement = \"x\", x = regex_vec)\n\n[1] \"hello\"   \"xxx\"     \"goodbye\" \"xxx\"    \n\n# Replace any number of letters with only a single 0\ngsub(pattern = \"[[:alpha:]]+\", replacement = \"0\", x = regex_vec)\n\n[1] \"0\"   \"123\" \"0\"   \"456\"\n\n\nThe stringr package cheatsheet has a really nice list of regular expression options that you may find valuable if you want to delve deeper on this topic. Scroll to the second page of the PDF to see the most relevant parts.\n\n\n\nConditionals\nRather than finding and replacing content, you may want to create a new column based on the contents of a different column. In plain language you might phrase this as ‘if column X has [some values] then column Y should have [other values]’. These operations are called conditionals and are an important part of data wrangling.\nIf you only want your conditional to support two outcomes (as in an either/or statement) there are useful functions that support this. Let’s return to our Plum Island Ecosystems crab dataset for an example.\n\n# Make a new colum with an either/or conditional\npie_crab_v2 &lt;- pie_crab %&gt;% \n1  dplyr::mutate(size_category = ifelse(test = (size &gt;= 15),\n                                       yes = \"big\",\n                                       no = \"small\"),\n                .after = size) \n\n# Count the number of crabs in each category\npie_crab_v2 %&gt;% \n  dplyr::group_by(size_category) %&gt;% \n  dplyr::summarize(crab_ct = dplyr::n())\n\n\n1\n\nmutate makes a new column, ifelse is actually doing the conditional\n\n\n\n\n# A tibble: 2 × 2\n  size_category crab_ct\n  &lt;chr&gt;           &lt;int&gt;\n1 big               179\n2 small             213\n\n\nIf you have multiple different conditions you can just stack these either/or conditionals together but this gets cumbersome quickly. It is preferable to instead use a function that supports as many alternates as you want!\n\n# Make a new column with several conditionals\npie_crab_v2 &lt;- pie_crab %&gt;% \n  dplyr::mutate(size_category = dplyr::case_when( \n1    size &lt;= 10 ~ \"tiny\",\n    size &gt; 10 & size &lt;= 15 ~ \"small\",\n    size &gt; 15 & size &lt;= 20 ~ \"big\",\n    size &gt; 20 ~ \"huge\",\n2    TRUE ~ \"uncategorized\"),\n                .after = size)\n\n# Count the number of crabs in each category\npie_crab_v2 %&gt;% \n  dplyr::group_by(size_category) %&gt;% \n  dplyr::summarize(crab_ct = dplyr::n())\n\n\n1\n\nSyntax is ‘test ~ what to do when true’\n\n2\n\nThis line is a catch-all for any rows that don’t meet previous conditions\n\n\n\n\n# A tibble: 4 × 2\n  size_category crab_ct\n  &lt;chr&gt;           &lt;int&gt;\n1 big               150\n2 huge               28\n3 small             178\n4 tiny               36\n\n\nNote that you can use functions like this one when you do have an either/or conditional if you prefer this format.\n\n\n\n\n\n\nActivity: Conditionals\n\n\n\nIn a script, attempt the following with the PIE crab data:\n\nCreate a column indicating when air temperature is above or below 13° Fahrenheit\nCreate a column indicating whether water temperature is lower than the first quartile, between the first quartile and the median water temp, between the median and the third quartile or greater than the third quartile\n\nHint: consult the summary function output!\n\n\n\n\n\n\nUniting / Separating Columns\nSometimes one column has multiple pieces of information that you’d like to consider separately. A date column is a common example of this because particular months are always in a given season regardless of the specific day or year. So, it can be useful to break a complete date (i.e., year/month/day) into its component bits to be better able to access those pieces of information.\n\n# Split date into each piece of temporal info\npie_crab_v3 &lt;- pie_crab_v2 %&gt;% \n  tidyr::separate_wider_delim(cols = date, \n1                              delim = \"-\",\n                              names = c(\"year\", \"month\", \"day\"),\n2                              cols_remove = TRUE)\n\n# Check that out\nstr(pie_crab_v3)\n\n\n1\n\n‘delim’ is short for “delimiter” which we covered in the Reproducibility module\n\n2\n\nThis argument specifies whether to remove the original column when making the new columns\n\n\n\n\ntibble [392 × 12] (S3: tbl_df/tbl/data.frame)\n $ year         : chr [1:392] \"2016\" \"2016\" \"2016\" \"2016\" ...\n $ month        : chr [1:392] \"07\" \"07\" \"07\" \"07\" ...\n $ day          : chr [1:392] \"24\" \"24\" \"24\" \"24\" ...\n $ latitude     : num [1:392] 30 30 30 30 30 30 30 30 30 30 ...\n $ site         : chr [1:392] \"GTM\" \"GTM\" \"GTM\" \"GTM\" ...\n $ size         : num [1:392] 12.4 14.2 14.5 12.9 12.4 ...\n $ size_category: chr [1:392] \"small\" \"small\" \"small\" \"small\" ...\n $ air_temp     : num [1:392] 21.8 21.8 21.8 21.8 21.8 ...\n $ air_temp_sd  : num [1:392] 6.39 6.39 6.39 6.39 6.39 ...\n $ water_temp   : num [1:392] 24.5 24.5 24.5 24.5 24.5 ...\n $ water_temp_sd: num [1:392] 6.12 6.12 6.12 6.12 6.12 ...\n $ name         : chr [1:392] \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" ...\n\n\nWhile breaking apart a column’s contents can be useful, it can also be helpful to combine the contents of several columns!\n\n# Re-combine data information back into date\npie_crab_v4 &lt;- pie_crab_v3 %&gt;% \n  tidyr::unite(col = \"date\",\n1               sep = \"/\",\n               year:day, \n2               remove = FALSE)\n\n# Structure check\nstr(pie_crab_v4)\n\n\n1\n\nThis is equivalent to the ‘delim’ argument in the previous function\n\n2\n\nComparable to the ‘cols_remove’ argument in the previous function\n\n\n\n\ntibble [392 × 13] (S3: tbl_df/tbl/data.frame)\n $ date         : chr [1:392] \"2016/07/24\" \"2016/07/24\" \"2016/07/24\" \"2016/07/24\" ...\n $ year         : chr [1:392] \"2016\" \"2016\" \"2016\" \"2016\" ...\n $ month        : chr [1:392] \"07\" \"07\" \"07\" \"07\" ...\n $ day          : chr [1:392] \"24\" \"24\" \"24\" \"24\" ...\n $ latitude     : num [1:392] 30 30 30 30 30 30 30 30 30 30 ...\n $ site         : chr [1:392] \"GTM\" \"GTM\" \"GTM\" \"GTM\" ...\n $ size         : num [1:392] 12.4 14.2 14.5 12.9 12.4 ...\n $ size_category: chr [1:392] \"small\" \"small\" \"small\" \"small\" ...\n $ air_temp     : num [1:392] 21.8 21.8 21.8 21.8 21.8 ...\n $ air_temp_sd  : num [1:392] 6.39 6.39 6.39 6.39 6.39 ...\n $ water_temp   : num [1:392] 24.5 24.5 24.5 24.5 24.5 ...\n $ water_temp_sd: num [1:392] 6.12 6.12 6.12 6.12 6.12 ...\n $ name         : chr [1:392] \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" ...\n\n\nNote in this output how despite re-combining data information the column is listed as a character column! Simply combining or separating data is not always enough so you need to really lean into frequent data structure checks to be sure that your data are structured in the way that you want.\n\n\nJoining Data\nOften the early steps of a synthesis project involve combining the data tables horizontally. You might imagine that you have two groups’ data on sea star abundance and–once you’ve synonymized the column names–you can simply ‘stack’ the tables on top of one another. Slightly trickier but no less common is combining tables by the contents of a shared column (or columns). Cases like this include wanting to combine your sea star table with ocean temperature data from the region of each group’s research. You can’t simply attach the columns because that assumes that the row order is identical between the two data tables (and indeed, that there are the same number of rows in both to begin with!). In this case, if both data tables shared some columns (perhaps “site” and coordinate columns) you can use joins to let your computer match these key columns and make sure that only appropriate rows are combined.\nBecause joins are completely dependent upon the value in both columns being an exact match, it is a good idea to carefully check the contents of those columns before attempting a join to make sure that the join will be successful.\n\n# Create a fish taxonomy dataframe that corresponds with the earlier fish dataframe\nfish_tax &lt;- data.frame(\"species\" = c(\"salmon\", \"bass\", \"halibut\", \"eel\"),\n                       \"family\" = c(\"Salmonidae\", \"Serranidae\", \"Pleuronectidae\", \"Muraenidae\"))\n\n# Check to make sure that the 'species' column matches between both tables\nsupportR::diff_check(old = fish_ct$species, new = fish_tax$species) \n\nFollowing element(s) found in old object but not new: \n\n\n[1] \"moray eel\"\n\n\nFollowing element(s) found in new object but not old: \n\n\n[1] \"eel\"\n\n\n\n# Use text replacement methods to fix that mistake in one table\nfish_tax_v2 &lt;- fish_tax %&gt;% \n1  dplyr::mutate(species = gsub(pattern = \"^eel$\",\n                               replacement = \"moray eel\", \n                               x = species))\n\n# Re-check to make sure that fixed it\nsupportR::diff_check(old = fish_ct$species, new = fish_tax_v2$species)\n\n\n1\n\nThe symbols around “eel” mean that we’re only finding/replacing exact matches. It doesn’t matter in this context but often replacing a partial match would result in more problems. For example, replacing “eel” with “moray eel” could make “electric eel” into “electric moray eel”.\n\n\n\n\nAll elements of old object found in new\n\n\nAll elements of new object found in old\n\n\nNow that the shared column matches between the two two dataframes we can use a join to combine them! There are four types of join:\n\nleft/right join\nfull join (a.k.a. outer join)\ninner join\nanti join\n\nYou can learn more about the types of join here or here but the quick explanation is that the name of the join indicates whether the rows of the “left” and/or the “right” table are retained in the combined table. In synthesis work a left join or full join is most common (where you have your primary data in the left position and some ancillary/supplementary dataset in the right position).\n\n# Let's combine the fish count and fish taxonomy information\nfish_df &lt;- fish_ct %&gt;% \n  # Actual join step\n1  dplyr::left_join(y = fish_tax_v2, by = \"species\") %&gt;%\n  # Move 'family' column to the left of all other columns\n  dplyr::relocate(family, .before = dplyr::everything())\n\n# Look at the result of that\nfish_df\n\n\n1\n\nThe ‘by’ argument accepts a vector of column names found in both data tables\n\n\n\n\n          family   species count\n1     Salmonidae    salmon    12\n2     Serranidae      bass    14\n3 Pleuronectidae   halibut    23\n4     Muraenidae moray eel     1\n\n\n\n\n\n\n\n\nActivity: Separating Columns & Joining Data\n\n\n\nIn a script, attempt the following with the PIE crab data:\n\nCreate a data frame where you bin months into seasons (i.e., winter, spring, summer, fall)\n\nUse your judgement on which month(s) should fall into each given PIE’s latitude/location\n\nJoin your season table to the PIE crab data based on month\n\nHint: you may need to modify the PIE dataset to ensure both data tables share at least one column upon which they can be joined\n\nCalculate the average size of crabs in each season in order to identify which season correlates with the largest crabs\n\n\n\n\n\nLeveraging Data Shape\nYou may already be familiar with data shape but fewer people recognize how playing with the shape of data can make certain operations dramatically more efficient. If you haven’t encountered it before, any data table can be said to have one of two ‘shapes’: either long or wide. Wide data have all measured variables from a single observation in one row (typically resulting in more columns than rows or “wider” data tables). Long data usually have one observation split into many rows (typically resulting in more rows than columns or “longer” data tables).\nData shape is often important for statistical analysis or visualization but it has an under-appreciated role to play in quality control efforts as well. If many columns have the shared criteria for what constitutes “tidy”, you can reshape the data to get all of those values into a single column (i.e., reshape longer), perform any needed wrangling, then–when you’re finished–reshape back into the original data shape (i.e., reshape wider). As opposed to applying the same operations repeatedly to each column individually.\nLet’s consider an example to help clarify this. We’ll simulate a butterfly dataset where both the number of different species and their sex were recorded in the same column. This makes the column not technically numeric and therefore unusable in analysis or visualization.\n\n# Generate a butterfly dataframe\nbfly_v1 &lt;- data.frame(\"pasture\" = c(\"PNW\", \"PNW\", \"RCS\", \"RCS\"),\n                      \"monarch\" = c(\"14m\", \"10f\", \"7m\", \"16f\"),\n                      \"melissa_blue\" = c(\"32m\", \"2f\", \"6m\", \"0f\"),\n                      \"swallowtail\" = c(\"1m\", \"3f\", \"0m\", \"5f\"))\n\n# First we'll reshape this into long format\nbfly_v2 &lt;- bfly_v1 %&gt;% \n  tidyr::pivot_longer(cols = -pasture, \n                      names_to = \"butterfly_sp\", \n                      values_to = \"count_sex\")\n\n# Check what that leaves us with\nhead(bfly_v2, n = 4)\n\n# A tibble: 4 × 3\n  pasture butterfly_sp count_sex\n  &lt;chr&gt;   &lt;chr&gt;        &lt;chr&gt;    \n1 PNW     monarch      14m      \n2 PNW     melissa_blue 32m      \n3 PNW     swallowtail  1m       \n4 PNW     monarch      10f      \n\n# Let's separate count from sex to be more usable later\nbfly_v3 &lt;- bfly_v2 %&gt;% \n  tidyr::separate_wider_regex(cols = count_sex, \n                              c(count = \"[[:digit:]]+\", sex = \"[[:alpha:]]\")) %&gt;% \n  # Make the 'count' column a real number now\n  dplyr::mutate(count = as.numeric(count))\n\n# Re-check output\nhead(bfly_v3, n = 4)\n\n# A tibble: 4 × 4\n  pasture butterfly_sp count sex  \n  &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;\n1 PNW     monarch         14 m    \n2 PNW     melissa_blue    32 m    \n3 PNW     swallowtail      1 m    \n4 PNW     monarch         10 f    \n\n# Reshape back into wide-ish format\nbfly_v4 &lt;- bfly_v3 %&gt;% \n  tidyr::pivot_wider(names_from = \"butterfly_sp\", values_from = count)\n\n# Re-re-check output\nhead(bfly_v4)\n\n# A tibble: 4 × 5\n  pasture sex   monarch melissa_blue swallowtail\n  &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n1 PNW     m          14           32           1\n2 PNW     f          10            2           3\n3 RCS     m           7            6           0\n4 RCS     f          16            0           5\n\n\nWhile we absolutely could have used the same function to break apart count and butterfly sex data it would have involved copy/pasting the same information repeatedly. By pivoting to long format first, we can greatly streamline our code. This can also be advantageous for unit conversions, applying data transformations, or checking text column contents among many other possible applications.\n\n\nLoops\nAnother way of simplfying repetitive operations is to use a “for loop” (often called simply “loops”). Loops allow you to iterate across a piece of code for a set number of times. Loops require you to define an “index” object that will change itself at the end of each iteration of the loop before beginning the next iteration. This index object’s identity will be determined by whatever set of values you define at the top of the loop.\nHere’s a very bare bones example to demonstrate the fundamentals.\n\n# Loop across each number between 2 and 4\n1for(k in 2:4){\n  \n  # Square the number\n  result &lt;- k^2\n  \n  # Message that outside of the loop\n  message(k, \" squared is \", result)\n2}\n\n\n1\n\n‘k’ is our index object in this loop\n\n2\n\nNote that the operations to iterate across are wrapped in curly braces ({...})\n\n\n\n\n2 squared is 4\n\n\n3 squared is 9\n\n\n4 squared is 16\n\n\nOnce you get the hang of loops, they can be a nice way of simplifying your code in a relatively human-readable way! Let’s return to our Plum Island Ecosystems crab dataset for a more nuanced example.\n\n# Create an empty list\ncrab_list &lt;- list()\n\n# Let's loop across size categories of crab\n1for(focal_size in unique(pie_crab_v4$size_category)){\n  \n  # Subset the data to just this size category\n  focal_df &lt;- pie_crab_v4 %&gt;% \n    dplyr::filter(size_category == focal_size)\n  \n  # Calculate average and standard deviation of size within this category\n  size_avg &lt;- mean(focal_df$size, na.rm = T) \n  size_dev &lt;- sd(focal_df$size, na.rm = T) \n  \n  # Assemble this into a data table and add to our list\n  crab_list[[focal_size]] &lt;- data.frame(\"size_category\" = focal_size,\n                                        \"size_mean\" = size_avg,\n                                        \"size_sd\" = size_dev)\n} # Close loop\n\n# Unlist the outputs into a dataframe\n2crab_df &lt;- purrr::list_rbind(x = crab_list)\n\n# Check out the resulting data table\ncrab_df\n\n\n1\n\nNote that this is not the most efficient way of doing group-wise summarization but is–hopefully–a nice demonstration of loops!\n\n2\n\nWhen all elements of your list have the same column names, list_rbind efficiently stacks those elements into one longer data table.\n\n\n\n\n  size_category size_mean   size_sd\n1         small 12.624270 1.3827471\n2          tiny  8.876944 0.9112686\n3           big 17.238267 1.3650173\n4          huge 21.196786 0.8276744\n\n\n\n\nCustom Functions\nFinally, writing your own, customized functions can be really useful particularly when doing synthesis work. Custom functions are generally useful for reducing duplication and increasing ease of maintenance (see the note on custom functions in the SSECR Reproducibility module) and also can be useful end products of synthesis work in and of themselves.\nIf one of your group’s outputs is a new standard data format or analytical workflow, the functions that you develop to aid yourself become valuable to anyone who adopts your synthesis project’s findings into their own workflows. If you get enough functions you can even release a package that others can install and use on their own computers. Such packages are a valuable product of synthesis efforts and can be a nice addition to a robust scientific resume/CV.\n\n# Define custom function\ncrab_hist &lt;- function(df, size_cat){\n  \n  # Subset data to the desired category\n  data_sub &lt;- dplyr::filter(.data = df, size_category == size_cat)\n  \n  # Create a histogram\n  p &lt;- psych::multi.hist(x = data_sub$size)\n}\n\n# Invoke function\ncrab_hist(df = pie_crab_v4, size_cat = \"tiny\")\n\n\n\n\n\n\n\n\nWhen writing your own functions it can also be useful to program defensively. This involves anticipating likely errors and writing your own error messages that are more informative to the user than whatever machine-generated error would otherwise get generated\n\n# Define custom function\n1crab_hist &lt;- function(df, size_cat = \"small\"){\n  \n  # Error out if 'df' isn't the right format\n2  if(is.data.frame(df) != TRUE)\n    stop(\"'df' must be provided as a data frame\")\n  \n  # Error out if the data doesn't have the right columns\n3  if(all(c(\"size_category\", \"size\") %in% names(df)) != TRUE)\n    stop(\"'df' must include a 'size' and 'size_category' column\")\n  \n  # Error out for unsupported size category values\n  if(size_cat %in% unique(df$size_category) != TRUE)\n    stop(\"Specified 'size_cat' not found in provided data\")\n  \n  # Subset data to the desired category\n  data_sub &lt;- dplyr::filter(.data = df, size_category == size_cat)\n  \n  # Create a histogram\n  p &lt;- psych::multi.hist(x = data_sub$size)\n}\n\n# Invoke new-and-improved function\n4crab_hist(df = pie_crab_v4)\n\n\n1\n\nThe default category is now set to “small”\n\n2\n\nWe recommend phrasing your error checks with this format (i.e., ’if &lt;some condition&gt; is not true, then &lt;informative error/warning message&gt;)\n\n3\n\nThe %in% operator lets you check whether one value matches any element of a set of accepted values. Very useful in contexts like this because the alternative would be a lot of separate “or” conditionals\n\n4\n\nWe don’t need to specify the ‘size_cat’ argument because we can rely on the default\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nActivity: Custom Functions\n\n\n\nIn a script, attempt the following on the PIE crab data:\n\nWrite a function that:\n\n\ncalculates the median of the user-supplied column\n\n\ndetermines whether each value is above, equal to, or below the median\n\n\nmakes a column indicating the results of step B\n\n\nUse the function on the standard deviation of water temperature\nUse it again on the standard deviation of air temperature\nRevisit your function and identify 2-3 likely errors\nWrite custom checks (and error messages) for the set of likely issues you just identified",
    "crumbs": [
      "Phase III -- Execute",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "mod_wrangle.html#additional-resources",
    "href": "mod_wrangle.html#additional-resources",
    "title": "Data Harmonization & Wrangling",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\nThe Ultimate Guide to Data Cleaning. Elgarby, O. 2019. Medium\nSome Simple Guidelines for Effective Data Management. Borer et al., 2009. Ecological Society of America Bulletin\n\n\n\nWorkshops & Courses\n\nData Analysis and Visualization in R for Ecologists, Episode 4: Manipulating, Analyzing, and Exporting Data with tidyverse. The Carpentries\nCoding in the Tidyverse. NCEAS Scientific Computing Team, 2023.\nNCEAS Learning Hub’s coreR Course, Chapter 8: Cleaning & Wrangling Data. NCEAS Learning Hub, 2023.\nNCEAS Learning Hub’s coreR Course, Chapter 16: Writing Functions & Packages. NCEAS Learning Hub, 2023.\nOpen Science Synthesis for the Delta Science Program’s Data Munging / QA / QC / Cleaning\n\n\n\nWebsites\n\nTen Commandments for Good Data Management",
    "crumbs": [
      "Phase III -- Execute",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Course Overview",
    "section": "",
    "text": "Synthesis Skills for Early Career Researchers (SSECR; [SEE-ker]) is a newly-designed course organized by the Long Term Ecological Research (LTER) Network. This course aims to address the need for more comprehensive interpersonal and data skills in ecology. You can find more information on SSECR at the LTER Network page for the course.\nIf you’re interested in joining the course as a student you can see the application here. If instead you’re interested in joining as a team project mentor you can find more information–and apply–here.",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#course-priorities",
    "href": "index.html#course-priorities",
    "title": "Course Overview",
    "section": "Course Priorities",
    "text": "Course Priorities\n\nSurface and test new synthesis ideas for feasibility\nPrepare more graduate students to be effective participants in/leaders of the synthesis projects\nConnect LTER graduate students across sites\nDevelop intergenerational linkages around synthesis research",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Course Overview",
    "section": "Course Description",
    "text": "Course Description\nThe course is structured around small group synthesis projects, so that lessons are immediately applied to a synthesis problem that is relevant to learners and will likely result in a publication. The course starts with an in-person launch to establish cohort cohesion and ensure that any setup issues are resolved. Participants will pitch projects to the group and assemble a team of collaborators.The ideal configuration would be 6 project teams of 4-5 students each. Prior to the start of the course, the Network Office will recruit a corps of potential project mentors, who will be matched with participant projects and who agree to meet with students approximately 4-5 times per year.\nApplicants will propose modest or exploratory synthesis projects as part of the application process. In addition to ideas stemming from participants’ own work, course mentors will make available a small library of synthesis ideas in search of execution. The in-person kickoff week will focus on cohort-building, pitching projects and assembling project teams, identifying relevant data, and getting set up on servers and collaboration tools.",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#course-scheduling",
    "href": "index.html#course-scheduling",
    "title": "Course Overview",
    "section": "Course Scheduling",
    "text": "Course Scheduling\nCourse participants meet for three hours, weekly, at the same time each week. Sessions alternate between hands-on instruction in technical and soft skills that are relevant for inclusive synthesis and team work on the chosen group projects.\nEach three-hour session will include 2 components:\n\nInspiration (~60 minutes): Presentation by an experienced synthesis scientist, describing why and how they conduct synthesis. This diverse group of researchers will be recruited from across the field and course participants will have ample time for discussion with each presenter.\nInstruction (~120 minutes): Each session will focus on a specific instructional topic, with technical skills, team-science skills, and communication topics interspersed throughout the year. The discussion will be limited to official course participants, but instructional materials for each topic will be available online, allowing individuals or site- or topic-based groups to follow along independently.\n\nTechnical skills will build on earlier lessons and are not intended to be stand-alone modules. The course will include social and leadership skills required to bring a synthesis project from idea to completion (or, for larger projects to completed proposal) and will include techniques for ensuring that multiple thinking and learning styles are respected and valued.\n\n\nProject groups will also meet at a time of their own choosing to work on projects. Project mentors are encouraged to participate in work sessions at least 4 times throughout the year.",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#course-policies",
    "href": "index.html#course-policies",
    "title": "Course Overview",
    "section": "Course Policies",
    "text": "Course Policies\n\nArtificial Intelligence Tools\nArtificial intelligence (AI) tools are increasingly well-known and widely discussed in the context of data science. AI products can increase the efficiency of code writing and are becoming a common part of the data science landscape. For the purposes of this course, we strongly recommend that you do not use AI tools. There is an under-discussed ethical consideration to the use and training of these tools in addition to their known practical limitations. However, the main reason we suggest you not use them for this class though is that leaning too heavily upon AI tools is likely to negatively impact your learning and skill acquisition.\nYou may have prior experience with some of the quantitative skills this course aims to teach but others are likely new to you. During the first steps of learning any new skill, it can be really helpful to struggle a bit in solving problems. Your efforts now will help refine your troubleshooting skills and will likely make it easier to remember how you solved a given problem the next time it arises. Over-use of AI tools can short circuit this pathway to mastery. Once you have become a proficient coder, you will be better able to identify and avoid any distortions or assumptions introduced by relying on AI.\nAI Resources\n\nPratim Ray, P. ChatGPT: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope. 2023. Internet of Things and Cyber-Physical Systems\nTrust, T. ChatGPT & Education Slide Deck. 2023. National Teaching Repository\nCsik, S. Teach Me How to Google. University of California, Santa Barbara (UCSB) Master of Environmental Data Science (MEDS) Program.",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "mod_reports.html",
    "href": "mod_reports.html",
    "title": "Reproducible Reports",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase V -- Share",
      "Reproducible Reports"
    ]
  },
  {
    "objectID": "mod_reports.html#overview",
    "href": "mod_reports.html#overview",
    "title": "Reproducible Reports",
    "section": "",
    "text": "Under Construction, stay tuned!",
    "crumbs": [
      "Phase V -- Share",
      "Reproducible Reports"
    ]
  },
  {
    "objectID": "mod_reports.html#learning-objectives",
    "href": "mod_reports.html#learning-objectives",
    "title": "Reproducible Reports",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter completing this module you will be able to:\n\nIdentify the three fundamental elements of R Markdown / Quarto documents\nUse Markdown syntax to accomplish text styling\nCreate reports that use a blend of plain text and embedded code to effectively communicate rationale, methodologies, and primary findings\nMake a (small) Quarto website\nDeploy a website and/or report through GitHub Pages",
    "crumbs": [
      "Phase V -- Share",
      "Reproducible Reports"
    ]
  },
  {
    "objectID": "mod_reports.html#module-content",
    "href": "mod_reports.html#module-content",
    "title": "Reproducible Reports",
    "section": "Module Content",
    "text": "Module Content",
    "crumbs": [
      "Phase V -- Share",
      "Reproducible Reports"
    ]
  },
  {
    "objectID": "mod_reports.html#additional-resources",
    "href": "mod_reports.html#additional-resources",
    "title": "Reproducible Reports",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPapers & Documents\n\n\n\n\n\nWorkshops & Courses\n\nNCEAS coreR Literature Analysis with Quarto session\nOSS Reproducible Papers with R Markdown\nUCSB’s Master of Environmental Data Science (MEDS) Creating your Personal Website using Quarto lesson\n\n\n\nWebsites\n\nPosit’s Welcome to Quarto",
    "crumbs": [
      "Phase V -- Share",
      "Reproducible Reports"
    ]
  }
]