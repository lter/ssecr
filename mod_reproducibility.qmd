---
title: "Reproducibility Best Practices"
---

## Overview

As we set out to engage with the synthesis skills this course aims to offer, it will be helpful to begin with a careful consideration of "reproducibility." Because synthesis projects draw data from many sources and typically involve many researchers working in concert, reproducibility is particularly important. In individual projects, adhering to reproducibility best practices is certainly a good goal but failing to do so for synthesis projects can severely limit the work in a more significant way than for those individual projects. "Reproducibility" is a wide sphere encompassing many different--albeit related--topics so it can be challenging to feel well-equipped to evaluate how well we are following these guidelines in our own work. In this module, we will cover a few fundamental facets of reproducibility and point to some considerations that may encourage you to push yourself to elevate your practices to the next level.

## Learning Objectives

After completing this module you will be able to: 

- <u>Identify</u> core tenets of reproducibility best practices
- <u>Create</u> robust workflow documentation
- <u>Implement</u> reproducible project organization strategies
- <u>Discuss</u> methods for improving the reproducibility of your code products
- <u>Summarize</u> FAIR and CARE data principles
- <u>Evaluate</u> the FAIR/CAREness of your work

## Lego Activity

Before we dive into the world of reproducibility for synthesis projects, we thought it would be fun (and informative!) to begin with an activity that is a useful analogy for the importance of some of the concepts we'll cover today. The LEGO {{< fa registered >}} activity was designed by [Mary Donaldson](https://orcid.org/0000-0002-1936-3499) and [Matt Mahon](https://orcid.org/0000-0001-8950-8422) at the University of Glasgow. The full materials can be accessed [here](https://eprints.gla.ac.uk/196477/).

## Project Organization & Documentation

Much of the popular conversation around reproducibility centers on reproducibility as it pertains to code. That is definitely an important facet but before we write even a single line it is critically important that we first discuss what factors go into project-wide reproducibility. "Perfect" code in a project that isn't structured thoughtfully can still result in a synthesis project that is not reproducible while even "bad" code can be made more intelligible when it is placed in a well-documented/organized project!

### Folder Structure

The simplest and often most effective way of beginning a reproducible project is adopting (and sticking to) a good file organization system. There is no single "best" way of organizing your projects' files as long as you are consistent. Consistency allows those navigating your system to _deduce_ where particular files are likely to be without having in-depth knowledge of the entire suite of materials. 

To begin, it is best to have a single folder for each project. This makes it simple to find the project's inputs and outputs and also makes collaboration and documentation much cleaner. Later in your project's life cycle, this 'one folder' approach will also make it easier to share your project with external reviewers or new team members. For researchers used to working alone there can be a temptation to think about your leadership of a project as the fundamental unit rather than the individual projects' scopes. This method works fine when working alone but greatly increases the difficulty of communication and co-working in projects led by teams. RStudio (the primary <u>I</u>ntegrated <u>D</u>eveloper <u>E</u>nvironment for R) and most version control systems assume that each project's materials will be placed in a single folder and either of these systems can confer significant benefits to your work (well worth any potential reorganization difficulty).

Within your project folder, it is valuable to structure your folders and files hierarchically. Having a folder with dozens of mixed file types of various purposes that may be either inputs or outputs is cumbersome to document and difficult to navigate. If instead you adopt a system of sub-folders that group files based on purpose and/or source engagement becomes much simpler. You need not use an intricate web of sub-folders either; often just a single layer of these sub-folders provides sufficient structure to meet your project's organizational needs.

:::{.callout-warning icon="false"}
#### Discussion: Folder Structure

With a partner discuss (some of) the following questions:

- How do you typically organize your projects' files?
- What benefits do you see of your current approach?
- What--if any--limitations to your system have you experienced?
- Do you think your structure would work well in a team environment?
    - If not, what changes might you make to better fit that context?
:::

### File Names

Beyond the structure and degree of nestedness you adopt for your folders, your files can (and should) include a lot of helpful contextual information about themselves. An ideal file name should be very informative about that file's contents, purpose, and relation to other project files. Some or all of that information may be reinforced by the folder(s) in which the file is placed, but the file name itself should _also_ confer that information. This may feel redundant but if late in your project's lifecycle you decide a different folder system is needed, information-dense file names will allow you to change file locations without excessive difficulty.

You should also consider how 'machine readable' your file names are. One fundamental way in which this changes user's experience is how file management applications (e.g., Apple's Finder) visually display files. By default files are typically sorted alphabetically and numerically. So, even if the script "wrangle.R" should be run _first_ in your workflow, most file explorers would put that script last or at the bottom. If instead you changed it's name to "01_wrangle.R" now it would likely be sorted towards the top and encountered earlier by those interested in your workflow. Notice too in that example that we have "zero padded" the script so that if we eventually had a tenth script file explorers would correctly sort it ("10..." would be before "1..." in most file sorting systems).

You should also avoid spaces and accented characters (e.g., é, ü, etc.) as some computers will not be able to recognize these characters. Windows operating systems in particular have a very difficult time parsing folder names with spaces (e.g., "raw data" versus "raw_data"). Using a mix of upper and lowercase letters can be effective when done carefully but also requires a lot of attention to detail on the part of those creating new files. It may be simplest to stick with all lowercase or all uppercase for your file names.

Be consistent with any delimiters you use in file names! Two common ones are the hyphen (-) and underscore (_). If you use one instead of spaces, be sure to _only_ use that one for that use-case rather than using them interchangeably. You may find it useful to use one delimiter to separate a type of information and then the other in lieu of spaces. For example, "fxn_calc-diversity.R" uses the prefix "fxn_" to indicate that the script contains a function while the words to the right of the underscore briefly describe the purpose of that function.

In that same vein, you may want to consider using "slugs" in your file names. Slugs are human-readable, unique pieces of file names that are shared between files and the outputs that they create. For example, the files created by "01_wrangle.R" could all begin with "01_" (the slug in this case). The benefit of this approach is that diagnosing strange outputs--or simply finding the source of a given file or graph--is a straightforward matter of looking for the matching slug.

### Documentation

Documenting a project can feel like a Sisyphean task but it is often not as hard as one might imagine and well worth the effort! One simple practice you can adopt to dramatically improve the reproducibility of your project is to create a "README" file in the top-level of your project's folder system. This file can be formatted however you'd like but generally READMEs should include (1) a project overview written in plain language, (2) a basic table of contents for the primary folders in your project folder, and (3) a brief description of the file naming scheme you've adopted for this project.

Your project's README becomes the 'landing page' for those navigating your repository and makes it easy for team members to know where documentation should go (in the README!). You may also choose to create a README file for some of the sub-folders of your project. This can be particularly valuable for your "data" folder(s) as it is an easy place to store data source/provenance information that might be overwhelming to include in the project-level README file.

Finally, you should choose a place to keep track of ideas, conversations, and decisions about the project. While you can take notes on these topics on a piece of paper, adopting a digital equivalent is often helpful because you can much more easily search a lengthy document when it is machine readable. We will discuss GitHub during the [Version Control module](https://lter.github.io/ssecr/mod_version-control.html) but GitHub offers something called [Issues](https://nceas.github.io/scicomp-workshop-collaborative-coding/issues.html) that can be a really effective place to record some of this information.

### Best Practices / Recommendations

If you integrate any of the concepts we've covered above you will find the reproducibility and transparency of your project will greatly increase. However, if you'd like additional recommendations we've assembled a non-exhaustive set of _additional_ "best practices" that you may find helpful.

#### Never Edit Raw Data

First and foremost, it is critical that you <u>**_never_**</u> edit the raw data directly. If you do need to edit the raw data, use a script to make all needed edits and save the output of that script as a _separate_ file. Editing the raw data directly without a script or using a script but overwriting the raw data are both incredibly risky operations because your create a file that "looks" like the raw data (and is likely documented as such) but differs from what others would have if they downloaded the 'real' raw data personally.

#### Separate Raw and Processed Data

In the same vein as the previous best practice, we recommend that you separate the raw and processed data into separate folders. This will make it easier to avoid accidental edits to the raw data and will make it clear what data are created by your project's scripts; even if you choose not to adopt a file naming convention that would make this clear.

#### Quarantine External Outputs

This can sound harsh, but it is often a good idea to "quarantine" outputs received from others until they can be carefully vetted. This is not at all to suggest that such contributions might be malicious! As you embrace more of the project organization recommendations we've described above outputs from others have more and more opportunities to diverge from the framework you establish. Quarantining inputs from others gives you a chance to rename files to be consistent with the rest of your project as well as make sure that the style and content of the code also match (e.g., use or exclusion of particular packages, comment frequency and content, etc.)

## Reproducible Coding

- Scripted approaches are more reproducible than unscripted ones (e.g., Excel, Google Sheets, etc.) but still often do not use reproducibility best practices
- Principles
    - All changes between raw data and the final data should be done with scripts
    - Workflow should be divided into logical, modular scripts (e.g., wrangle vs. analyze vs. graph)
    - If an operation is duplicated more than 3 times within a project, write a custom function to centralize the work
    - If an operation is duplicated across more than 3 _projects_, consider creating an R package
    - Code should be thoroughly documented with comments
    - Comments should focus on "why" of operations rather than "what" (assumes code is being read by a person who can interpret the 'what' by scanning the code)
- Code tells the computer what to do, comments tell the human what we're telling the computer to do
- Benefits of clear / reproducible code:
    - Returning to code months later is easier
    - Easier to share methods and facilitate external result validation
    - Encourages adoption of your methods/workflows
- Choose a logical coding style and _stick with it_
- Concise and descriptive object names (and variable names)
- Use spaces between operators and after commas
- Indentation should be consistent about tabs vs. spaces
- Low (ish) effort way to increase reproducibility is to use extensive/clear comments
- Portable code (i.e., transferable) uses relative file paths
- Custom functions should be written "defensively"
    - Anticipate/identify likely errors and code custom warning/error messages that clearly identify how to fix them
    - Key is to "fail fast" and ensure code throws an error _as soon as something unexpected happens_ rather than doing a bunch of processing and failing later on because of something that could be identified early on
- Carefully load packages and record package versions

## FAIR & CARE Data Principles

Data availability, data size, and demand for transparency by government and funding agencies are all steadily increasing. While ensuring that your project and code practices are reproducible is important, it is also important to consider how open and reproducible your data are as well. Synthesis projects are in a unique position here because synthesis projects use data that may have been previously published on and/or be added to a public data repository by the original data collectors. However, synthesis projects take data from these different sources and wrangle it such that the different data sources are comparable to one another. These 'synthesis data products' can be valuable to consider archiving in a public repository to save other researchers from needing to re-run your entire wrangling workflow in order to get the data product. In either primary or synthesis research contexts there are several valuable frameworks to consider as data structure and metadata are being decided. Among these are the FAIR and CARE data principles.

### FAIR

FAIR is an acronym for <u>F</u>indable <u>A</u>ccessible <u>I</u>nterpoerable and <u>R</u>eusable. Each element of the FAIR principles can be broken into a set of concrete actions that you can take _throughout the lifecycle of your project_ to ensure that your data are open and transparent. Perhaps most importantly, FAIR data are most easily used by other research teams in the future so the future impact of your work is--in some ways--dependent upon how thoroughly you consider these actions.

Consider the following list of actions you might take to make your data FAIR. Note that not all actions may be appropriate for all types of data (see our discussion of the CARE principles below), but these guidelines are still important to consider--even if you ultimately choose to reject some of them. Virtually all of the guidelines considered below apply to metadata (i.e., the formal documentation describing your data) and the 'actual' data but for ease of reference we will call both of these resources "data."

<img src="images/comic_fair-data.png" alt="Stick figure students point at large capital letters spelling out FAIR" width="50%" align="right">

**Findability**

- Ensure that data have a globally unique and _persistent_ identifier
- Completely fill out all metadata details
- Register/index data in a searchable resource

**Accessibility**

- Store data in a file format that is open, free, and universally implementable (e.g., CSV rather than MS Excel, etc.)
- Ensure that metadata will be available _even if the data they describe are not_

**Interoperability**

- Use formal, shared, and broadly applicable language for knowledge representation
    - This can mean using full species names rather than codes or shorthand that may not be widely known
- Use vocabularies that are broadly used and that themselves follow FAIR principles
- Include explicit references to other FAIR data

**Reusability**

- Describe your data with rich detail that covers a _plurality of relevant attributes_
- Attach a clear data usage license so that secondary data users know how they are allowed to use your data
- Include detailed provenance information about your data
- Ensure that your data meet _discipline-specific_ community standards

:::{.callout-warning icon="false"}
#### Discussion: Consider Data FAIRness

Consider the first data chapter of your thesis or dissertation. On a scale of 1-5, how FAIR do you think your data and metadata are? What actions could you take to make your data more FAIR compliant? If it helps, feel free to rate your (meta)data based on each FAIR criterion separately!

Feel free to use these questions to guide your thinking

- How are the data for that project stored?
- What metadata exists to document those data?
- How easy would it be for someone in your lab group to pick up and use your data?
- How easy would it be for someone <u>not</u> in your lab group?
:::

### CARE

While making data and code more FAIR is often a good ideal the philosophy behind those four criteria come from a perspective that emphasizes data sharing as a _good in and of itself_. This approach can ignore historical context and contemporary power differentials and thus be insufficient as the _sole_ tool to use in evaluating how data/code are shared and stored. The [Global Indigenous Data Alliance](https://www.gida-global.org/) (GIDA) created the CARE principles with these ethical considerations explicitly built into their tenets. **Before** making your data widely available and transparent (ideally before even beginning your research), it is crucial to consider this ethical dimension. 

CARE stands for <u>C</u>ollective Benefit, <u>A</u>uthority to Control, <u>R</u>esponsibility, and <u>E</u>thics. Ensuring that your data meet these criteria helps to advance Indigenous data sovereignty and respects those who have been--and continue to be--collecting knowledge about the world around us for millennia. The following actions are adapted from Jennings _et al._ 2023 (linked at the bottom of this page).

**Collective Benefit**

- Demonstrate how your research and its potential results are relevant and of value to the interests of the community at large and its individual members
- Include and value local community experts in the research team
- Use classifications and categories in ways defined by Indigenous communities and individuals
- Disaggregate large geographic scale data to increase relevance for place-specific Indigenous priorities
- Compensate community experts _throughout_ the research process (proposal development through to community review of _pre_-publication manuscripts)

**Authority to Control**

- Establish institutional principles or protocoles that explicitly recognize Indigenous Peoples' rights to and interests in their knowledges/data
- Ensure data collection and distribution are consistent with individual and community consent provisions and that consent is _ongoing_ (including the right to withdraw or refuse)
- Ensure Indigenous communities have access to the (meta)data in usable format

**Responsibility**

- Create and expand opportunities for community capacity
- Record the Traditional Knowledge and biocultural labels of the Local Contexts Hub in metadata
- Ensure review of draft publications _before_ publication
- Use the languages of Indigenous Peoples in the (meta)data

**Ethics**

- Access research using Indigenous ethical frameworks
- Use community-defined review processes with appropriate reviewers for activities delineated in data management plans
- Work to maximize benefits from the perspectives of Indigenous Peoples by clear and transparent dialogue with communities and individuals
- Engage with community guidelines for the use and reuse of data (including facilitating data removal and/or disposal requests from aggregated datasets)

<p align="center">
<img src="images/image_care-fair.png" alt="Patterned image reading 'Be FAIR and CARE' with the letters of both acronyms defined beneath each letter" width="70%">
</p>

## Additional Resources

### Papers & Documents

- [Applying the 'CARE Principles for Indigenous Data Governance' to Ecology and Biodiversity Research](https://www.nature.com/articles/s41559-023-02161-2). Jennings _et al._, 2023. **Nature Ecology & Evolution**
- [Guides to Better Science - Reproducible Code](https://www.britishecologicalsociety.org/publications/better-science/). The British Ecological Society, 2024.
- [FAIR Teaching Handbook](https://fairsfair.gitbook.io/fair-teaching-handbook/). Englehardt _et al._, 2024.
- [R Packages](https://r-pkgs.org/) (2nd ed.). Wickham & Bryan.

### Workshops & Courses

- Data Analysis and Visualization in R for Ecologists, [Episode 1: Before We Start](https://datacarpentry.org/R-ecology-lesson/00-before-we-start.html). The Carpentries
- Introduction to R for Geospatial Data, [Episode 2: Project Management with RStudio](https://datacarpentry.org/r-intro-geospatial/02-project-intro.html). The Carpentries
- coreR Course, [Chapter 5: FAIR and CARE Principles](https://learning.nceas.ucsb.edu/2023-10-coreR/session_05.html). National Center for Ecological Analysis and Synthesis (NCEAS) Learning Hub, 2023.
- coreR Course, [Chapter 18: Reproducibility & Provenance](https://learning.nceas.ucsb.edu/2023-10-coreR/session_18.html). NCEAS Learning Hub, 2023.

### Websites

- [Coding Tips](https://nceas.github.io/scicomp.github.io/best_practices.html). NCEAS Scientific Computing Team, 2024.
- [Team Coding: 5 Essentials](https://nceas.github.io/scicomp.github.io/onboard-scaffold_team_coding.html). NCEAS Scientific Computing Team, 2024.
- Advanced R (1st ed.) [Style Guide](http://adv-r.had.co.nz/Style.html). Wickham
- PEP 8 [Style Guide for Python Code](https://peps.python.org/pep-0008/). van Rossum _et al._ 2013.
- [Google Style Guides](https://google.github.io/styleguide/)
