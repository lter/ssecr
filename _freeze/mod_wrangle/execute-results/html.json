{
  "hash": "dee3934f3fe2cf18e7db8998d0fb0569",
  "result": {
    "markdown": "---\ntitle: \"Data Harmonization & Wrangling\"\n---\n\n\n## Overview\n\nNow that we have covered how to find data and use data visualization methods to explore it, we can move on to combining separate data files and preparing that combined data file for analysis. For the purposes of this module we're adopting a very narrow view of harmonization and a very broad view of wrangling but this distinction aligns well with two discrete philosophical/logistical arenas. To make those definitions explicit:\n\n- <u>\"Harmonization\" = process of combining separate primary data objects into one object</u>. This includes things like synonymizing columns, or changing data format to support combination. This _excludes_ quality control steps--even those that are undertaken before harmonization begins.\n\n- <u>\"Wrangling\" = all modifications to data meant to create an analysis-ready 'tidy' data object</u>. This includes quality control, unit conversions, and data 'shape' changes to name a few. Note that attaching ancillary data to your primary data object (e.g., attaching temperature data to a dataset on plant species composition) _also falls into this category!_\n\nIf you'd like to follow along with the code chunks included throughout this module, you'll need to install the following packages:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Note that these lines only need to be run once per computer\n## So you can skip this step if you've *ever* installed these before\ninstall.packages(\"ltertools\")\ninstall.packages(\"lterdatasampler\")\ninstall.packages(\"psych\")\n```\n:::\n\n\n## Learning Objectives\n\nAfter completing this module you will be able to: \n\n- <u>Identify</u> typical steps in data harmonization and wrangling workflows\n- <u>Create</u> a harmonization workflow\n- <u>Define</u> quality control\n- <u>Summarize</u> typical operations in a quality control workflow\n- <u>Use</u> regular expressions to perform flexible text operations\n- <u>Write</u> custom functions to reduce code duplication\n- <u>Identify</u> value of and typical obstacles to data 'joining'\n- <u>Explain</u> benefits and drawbacks of using data shape to streamline code\n- <u>Design</u> a complete data wrangling workflow\n\n## Harmonizing Data\n\nData harmonization is an interesting topic in that it is _vital_ for synthesis projects but only very rarely relevant for primary research. Synthesis projects must reckon with the data choices made by each team of original data collectors. These collectors may or may not have recorded their judgement calls (or indeed, any metadata) but before synthesis work can be meaningfully done these independent datasets must be made comparable to one another and combined.\n\nFor tabular data, we recommend using the [`ltertools` R package](https://lter.github.io/ltertools/) to perform any needed harmonization. This package relies on a \"column key\" to translate the original column names into equivalents that apply across all datasets. Users can generate this column key however they would like but Google Sheets is a strong option as it allows multiple synthesis team members to simultaneously work on filling in the needed bits of the key.\n\nThe column key requires three columns:\n\n1. \"source\" -- Name of the raw file\n2. \"raw_name\" -- Name of all raw columns in that file to be synonymized\n3. \"tidy_name\" -- New name for each raw column that should be carried to the harmonized data\n\nNote that any raw names either not included in the column key or that lack a tidy name equivalent will be excluded from the final data object. For more information, consult the [`ltertools` package vignette](https://lter.github.io/ltertools/articles/ltertools.html). For convenience, we're attaching the visual diagram of this method of harmonization included in the `ltertools` vignette below.\n\n<p align=\"center\">\n<img src=\"images/image_harmonize-workflow.png\" alt=\"Four color-coded tables are in a soft rectangle. One is pulled out and its column names are replaced based on their respective 'tidy names' in the column key table. This is done for each of the other tables then the four tables--with fixed column names--are combined into a single data table\" width=\"90%\">\n</p>\n\n## Wrangling Data\n\nData wrangling is a _huge_ subject that covers a wide range of topics. In this part of the module, we'll attempt to touch on a wide range of tools that may prove valuable to your data wrangling efforts. This is certainly non-exhaustive and you'll likely find new tools that fit your coding style and professional intuition better. However, hopefully the topics covered below provide a nice 'jumping off' point to reproducibly prepare your data for analysis and visualization work later in the lifecycle of the project.\n\nThis module will use example data to demonstrate these tools but as we work through these topics you should <u>feel free to substitute a dataset of your choosing</u>! If you don't have one in mind, you can use the example dataset shown in the code chunks throughout this module.\n\nThis dataset comes from the [`lterdatasampler` R package](https://lter.github.io/lterdatasampler/) and the data are about fiddler crabs (_Minuca pugnax_) at the [Plum Island Ecosystems LTER](https://pie-lter.ecosystems.mbl.edu/welcome-plum-island-ecosystems-lter) site.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the lterdatasampler package\nlibrary(lterdatasampler)\n\n# Load the fiddler crab dataset\ndata(pie_crab)\n```\n:::\n\n\n### Exploring the Data\n\nBefore beginning any code operations, it's important to get a sense for the data. Characteristics like the dimensions of the dataset, the column names, and the type of information stored in each column are all crucial pre-requisites to knowing what tools can or should be used on the data.\n\nChecking the data structure is one way of getting a lot of this high-level information.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check dataset structure\nstr(pie_crab)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nClasses 'tbl_df', 'tbl' and 'data.frame':\t392 obs. of  9 variables:\n $ date         : Date, format: \"2016-07-24\" \"2016-07-24\" ...\n $ latitude     : num  30 30 30 30 30 30 30 30 30 30 ...\n $ site         : chr  \"GTM\" \"GTM\" \"GTM\" \"GTM\" ...\n $ size         : num  12.4 14.2 14.5 12.9 12.4 ...\n $ air_temp     : num  21.8 21.8 21.8 21.8 21.8 ...\n $ air_temp_sd  : num  6.39 6.39 6.39 6.39 6.39 ...\n $ water_temp   : num  24.5 24.5 24.5 24.5 24.5 ...\n $ water_temp_sd: num  6.12 6.12 6.12 6.12 6.12 ...\n $ name         : chr  \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" ...\n```\n:::\n:::\n\n\nFor data that are primarily numeric, you may find data summary functions to be valuable. Note that most functions of this type do not provide useful information on text columns so you'll need to find that information elsewhere.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get a simple summary of the data\nsummary(pie_crab)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      date               latitude         site                size      \n Min.   :2016-07-24   Min.   :30.00   Length:392         Min.   : 6.64  \n 1st Qu.:2016-07-28   1st Qu.:34.00   Class :character   1st Qu.:12.02  \n Median :2016-08-01   Median :39.10   Mode  :character   Median :14.44  \n Mean   :2016-08-02   Mean   :37.69                      Mean   :14.66  \n 3rd Qu.:2016-08-09   3rd Qu.:41.60                      3rd Qu.:17.34  \n Max.   :2016-08-13   Max.   :42.70                      Max.   :23.43  \n    air_temp      air_temp_sd      water_temp    water_temp_sd  \n Min.   :10.29   Min.   :6.391   Min.   :13.98   Min.   :4.838  \n 1st Qu.:12.05   1st Qu.:8.110   1st Qu.:14.33   1st Qu.:6.567  \n Median :13.93   Median :8.410   Median :17.50   Median :6.998  \n Mean   :15.20   Mean   :8.654   Mean   :17.65   Mean   :7.252  \n 3rd Qu.:18.63   3rd Qu.:9.483   3rd Qu.:20.54   3rd Qu.:7.865  \n Max.   :21.79   Max.   :9.965   Max.   :24.50   Max.   :9.121  \n     name          \n Length:392        \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n```\n:::\n:::\n\n\nFor text columns it can sometimes be useful to simply look at the unique entries in a given column and sort them alphabetically for ease of parsing.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Look at the sites included in the data\nsort(unique(pie_crab$site))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"BC\"  \"CC\"  \"CT\"  \"DB\"  \"GTM\" \"JC\"  \"NB\"  \"NIB\" \"PIE\" \"RC\"  \"SI\"  \"VCR\"\n[13] \"ZI\" \n```\n:::\n:::\n\n\nFor those of you who think more visually, a histogram can be a nice way of examining numeric data. There are simple histogram functions in the 'base' packages of most programming languages but it can sometimes be worth it to use those from special libraries because they can often convey additional detail.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load the psych library\nlibrary(psych)\n\n# Get the histogram of crab \"size\" (carapace width in mm)\npsych::multi.hist(pie_crab$size)\n```\n\n::: {.cell-output-display}\n![](mod_wrangle_files/figure-html/multi-hist-1.png){fig-align='center' width=384}\n:::\n:::\n\n\n### Quality Control\n\nYou may have encountered the phrase \"QA/QC\" (<u>Q</u>uality <u>A</u>ssurance / <u>Q</u>uality <u>C</u>ontrol) in relation to data cleaning. Technically, quality assurance only encapsulates _preventative_ measures for reducing errors. One example of QA would be using a template for field datasheets because using standard fields reduces the risk that data are recorded inconsistently and/or incompletely. Quality control on the other hand refers to all steps taken to resolve errors _after_ data are collected. Any code that you write to fix typos or remove outliers from a dataset falls under the umbrella of QC.\n\nIn synthesis work, QA is only very rarely an option. You'll be working with datasets that have already been collected and attempting to handle any issues _post hoc_ which means the vast majority of data wrangling operations will be quality control methods. These QC efforts can be **incredibly** time-consuming so using a programming language (like {{< fa brands r-project >}} R or {{< fa brands python >}} Python) is a dramatic improvement over manually looking through the data using Microsoft Excel or other programs like it.\n\n\n`suppportR::num_check`\n\n#### Regular Expressions & Text Methods\n\n`gsub`\n\n`stringr::str_sub`\n\n\n\n### Custom Functions\n\n\n\n\n### Uniting / Separating Columns\n\n\n`tidyr::separate_wider_delim`\n\n### Joining Data\n\na.k.a. attaching data by columns\n\n`dplyr::left_join`\n\n`supportR::diff_check`\n\n\n### Leveraging Data Shape\n\n1. `tidyr::pivot_longer`\n2. operations on consolidated columns\n3. `tidyr::pivot_wider`\n\n\n\n\n\n\n\n\n\n\n\n\n## Additional Resources\n\n### Papers & Documents\n\n- \n\n### Workshops & Courses\n\n- Data Analysis and Visualization in R for Ecologists, [Episode 4: Manipulating, Analyzing, and Exporting Data with `tidyverse`](https://datacarpentry.org/R-ecology-lesson/03-dplyr.html). The Carpentries\n- [Coding in the Tidyverse](https://nceas.github.io/scicomp-workshop-tidyverse/). NCEAS Scientific Computing Team, 2023.\n- coreR Course, [Chapter 8: Cleaning & Wrangling Data](https://learning.nceas.ucsb.edu/2023-10-coreR/session_08.html). NCEAS Learning Hub, 2023.\n- coreR Course, [Chapter 16: Writing Functions & Packages](https://learning.nceas.ucsb.edu/2023-10-coreR/session_16.html). NCEAS Learning Hub, 2023.\n\n### Websites\n\n- \n",
    "supporting": [
      "mod_wrangle_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}